{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd8abfc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:21:59.007476Z",
     "iopub.status.busy": "2024-09-15T10:21:59.007065Z",
     "iopub.status.idle": "2024-09-15T10:22:01.158781Z",
     "shell.execute_reply": "2024-09-15T10:22:01.158032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.9/250.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "649e7b19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:01.162164Z",
     "iopub.status.busy": "2024-09-15T10:22:01.161839Z",
     "iopub.status.idle": "2024-09-15T10:22:02.562471Z",
     "shell.execute_reply": "2024-09-15T10:22:02.561841Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import glob\n",
    "# **IMPORTANT**: Have to do this line *before* importing tensorflow\n",
    "os.environ['PYTHONHASHSEED'] = str(2)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import random\n",
    "from config_module import Config\n",
    "config = Config()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Import modules\n",
    "import itertools\n",
    "import math\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import ast\n",
    "import seaborn as sn\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcd6dc89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:02.565784Z",
     "iopub.status.busy": "2024-09-15T10:22:02.565370Z",
     "iopub.status.idle": "2024-09-15T10:22:02.655228Z",
     "shell.execute_reply": "2024-09-15T10:22:02.654558Z"
    }
   },
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem()\n",
    "bucket = config.get_value('bucket')\n",
    "# prefix = config.get_value('prefix')\n",
    "prefix_inp = config.get_value('prefix_inp')\n",
    "#prefix_out = config.get_value('prefix_out')\n",
    "inp_path = f\"{config.get_value('primary_path')}/{config.get_value('tpcap_model')}/{config.get_value('model_training_date')}/{config.get_value('model_output_folder_name')}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3227677",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:02.658331Z",
     "iopub.status.busy": "2024-09-15T10:22:02.657994Z",
     "iopub.status.idle": "2024-09-15T10:22:02.661540Z",
     "shell.execute_reply": "2024-09-15T10:22:02.660969Z"
    }
   },
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9ca4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2024, 10, 22, 9, 20, 36, 599722)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c261829f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:02.664184Z",
     "iopub.status.busy": "2024-09-15T10:22:02.663742Z",
     "iopub.status.idle": "2024-09-15T10:22:02.667005Z",
     "shell.execute_reply": "2024-09-15T10:22:02.666351Z"
    }
   },
   "outputs": [],
   "source": [
    "# part_master_path_1 = f\"s3://s3-dx-tdem-ie-nonprod-persist/persist-bo/persist-bo-demand-forecast/to-be-removed/test_data/TPCAP Part Master New.xlsx\"\n",
    "# data_gt_master_path_1=f\"s3://s3-dx-tdem-ie-nonprod-persist/persist-bo/persist-bo-demand-forecast/to-be-removed/master_data/job3/etljob3input/GT_part_master.csv\"\n",
    "# data_pic_master_path_1=f\"s3://s3-dx-tdem-ie-nonprod-persist/persist-bo/persist-bo-demand-forecast/to-be-removed/master_data/job3/etljob3input/PIC_master.csv\"\n",
    "# data_supp_master_path_1=f\"s3://s3-dx-tdem-ie-nonprod-persist/persist-bo/persist-bo-demand-forecast/to-be-removed/master_data/job3/etljob3input/Supplier_master.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e980f05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:02.669584Z",
     "iopub.status.busy": "2024-09-15T10:22:02.669308Z",
     "iopub.status.idle": "2024-09-15T10:22:02.673627Z",
     "shell.execute_reply": "2024-09-15T10:22:02.673007Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    if file_extension == '.xlsx':\n",
    "        # Read Excel file\n",
    "        data = pd.read_excel(file_path)\n",
    "    elif file_extension == '.csv':\n",
    "        # Read CSV file\n",
    "        data = pd.read_csv(file_path)\n",
    "    elif file_extension == '.txt':\n",
    "        # Read TXT file\n",
    "        data = pd.read_csv(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format: {}\".format(file_extension))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c8884be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:02.676262Z",
     "iopub.status.busy": "2024-09-15T10:22:02.675985Z",
     "iopub.status.idle": "2024-09-15T10:22:02.682462Z",
     "shell.execute_reply": "2024-09-15T10:22:02.681872Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_latest_s3_file(bucket_name, prefix, file_prefix):\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # List all objects under the given prefix\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    \n",
    "    if 'Contents' not in response:\n",
    "        print(\"No files found.\")\n",
    "        return None\n",
    "    \n",
    "    # Filter objects that start with the specific file prefix\n",
    "    filtered_files = [\n",
    "        obj for obj in response['Contents'] \n",
    "        if obj['Key'].startswith(f\"{prefix}{file_prefix}\")\n",
    "    ]\n",
    "    \n",
    "    if not filtered_files:\n",
    "        print(\"No files found with the specific prefix.\")\n",
    "        return None\n",
    "    \n",
    "    # Extract the date from each file name and store along with the key\n",
    "    files_with_dates = []\n",
    "    for obj in filtered_files:\n",
    "        #print(obj)\n",
    "        key = obj['Key']\n",
    "        # Assuming the date is at the end of the filename after the last underscore\n",
    "        try:\n",
    "            # Get the base name without directory path\n",
    "            base_name = os.path.basename(key)\n",
    "            name_without_extension = os.path.splitext(base_name)[0]\n",
    "            date_str = name_without_extension.split('_')[-1]  # Get the date string part\n",
    "            file_date = datetime.strptime(date_str, \"%Y-%m-%d\")  # Parse date\n",
    "            files_with_dates.append((key, file_date))\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing date from file name {key}: {e}\")\n",
    "\n",
    "    if not files_with_dates:\n",
    "        print(\"No valid files with dates found.\")\n",
    "        return None\n",
    "\n",
    "    # Find the file with the latest date\n",
    "    latest_file_key, latest_date = max(files_with_dates, key=lambda x: x[1])\n",
    "    \n",
    "    return latest_file_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e6a8c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'persist-bo/persist-bo-demand-forecast/tpcap-models/20240913/model-training-output/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c212239-f952-430f-81f3-f8d390c495c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:02.685104Z",
     "iopub.status.busy": "2024-09-15T10:22:02.684803Z",
     "iopub.status.idle": "2024-09-15T10:22:03.168751Z",
     "shell.execute_reply": "2024-09-15T10:22:03.168075Z"
    }
   },
   "outputs": [],
   "source": [
    "#prefix_output_file = config.get_value('prefix_output_file')\n",
    "prefix_output = f\"{config.get_value('primary_path')}/{config.get_value('tpcap_model')}/{config.get_value('model_training_date')}/{config.get_value('model_output_folder_name')}\"\n",
    "\n",
    "bestfit_file_prophet = os.path.basename(get_latest_s3_file(bucket, inp_path,'train_prophet_forecast_'))\n",
    "bestfit_file_lstm = os.path.basename(get_latest_s3_file(bucket, inp_path,'train_lstm_forecast_'))\n",
    "bestfit_file_ridge = os.path.basename(get_latest_s3_file(bucket, inp_path,'train_ridge_forecast_'))\n",
    "bestfit_file_lasso = os.path.basename(get_latest_s3_file(bucket, inp_path,'train_lasso_forecast_'))\n",
    "bestfit_file_xgboost = os.path.basename(get_latest_s3_file(bucket, inp_path,'train_xgboost_forecast_'))\n",
    "prophet_config_filename = config.get_value('prophet_config_filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12fa928a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:03.171793Z",
     "iopub.status.busy": "2024-09-15T10:22:03.171464Z",
     "iopub.status.idle": "2024-09-15T10:22:03.175558Z",
     "shell.execute_reply": "2024-09-15T10:22:03.174981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_prophet_forecast_2024-09-13.xlsx\n",
      "train_lstm_forecast_1_2024-09-13.xlsx\n",
      "train_lasso_forecast_2024-09-13.xlsx\n",
      "train_ridge_forecast_2024-09-13.xlsx\n",
      "train_xgboost_forecast_2024-09-13.xlsx\n"
     ]
    }
   ],
   "source": [
    "print(bestfit_file_prophet)\n",
    "print(bestfit_file_lstm)\n",
    "print(bestfit_file_lasso)\n",
    "print(bestfit_file_ridge)\n",
    "print(bestfit_file_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fff86477",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:03.178185Z",
     "iopub.status.busy": "2024-09-15T10:22:03.177881Z",
     "iopub.status.idle": "2024-09-15T10:22:03.181651Z",
     "shell.execute_reply": "2024-09-15T10:22:03.181090Z"
    }
   },
   "outputs": [],
   "source": [
    "training_date = pd.to_datetime(config.get_value('model_training_date'))\n",
    "file_date_training = training_date.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dec2a613-51d2-4e19-806b-50001d254229",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:03.184172Z",
     "iopub.status.busy": "2024-09-15T10:22:03.183885Z",
     "iopub.status.idle": "2024-09-15T10:22:03.188357Z",
     "shell.execute_reply": "2024-09-15T10:22:03.187794Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_mape(x, y):\n",
    "    \"\"\"\n",
    "    Function to calculate the Mean Absolute Percentage Error (MAPE) between two values x and y.\n",
    "    \n",
    "    Parameters:\n",
    "    x: The true value.\n",
    "    y: The predicted value.\n",
    "    \n",
    "    Returns:\n",
    "    The MAPE value as a float.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if x is NaN\n",
    "    if math.isnan(x):\n",
    "        return 0\n",
    "    \n",
    "    # Check if x and y are equal\n",
    "    if x == y:\n",
    "        return 1\n",
    "    \n",
    "    # Check if x is zero\n",
    "    elif x == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    else:\n",
    "        mape = 1 - (abs(x - y) / x)\n",
    "        return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9efb28c4-631b-4574-8bb8-03d6eda36d33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:03.190921Z",
     "iopub.status.busy": "2024-09-15T10:22:03.190628Z",
     "iopub.status.idle": "2024-09-15T10:22:03.194579Z",
     "shell.execute_reply": "2024-09-15T10:22:03.194047Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_consecutive_months(start_date, n_months=3):\n",
    "    \"\"\"\n",
    "    Generate a list of consecutive month start dates.\n",
    "\n",
    "    Args:\n",
    "    - start_date (str): Starting date in 'YYYY-MM-DD' format.\n",
    "    - n_months (int): Number of consecutive months to generate.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of consecutive month start dates in 'YYYY-MM-DD' format.\n",
    "    \"\"\"\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    month_list = [(start_date + pd.DateOffset(months=i)).strftime('%Y-%m-%d') for i in range(1, n_months + 1)]\n",
    "    return month_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eda0c100-da71-499b-abc2-9b43c7218fc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:03.197040Z",
     "iopub.status.busy": "2024-09-15T10:22:03.196745Z",
     "iopub.status.idle": "2024-09-15T10:22:03.201934Z",
     "shell.execute_reply": "2024-09-15T10:22:03.201408Z"
    }
   },
   "outputs": [],
   "source": [
    "# def prophet_best_fit_lag1(bucket_name, prophet_training_prefix, prophet_filename, model_config_filename, training_end_date):\n",
    "def prophet_best_fit_lag1(bucket_name, prophet_training_prefix, prophet_filename, training_end_date):\n",
    "\n",
    "    \"\"\"\n",
    "    Combine Prophet forecasts from three different lags into a single formatted dataframe for lag 1.\n",
    "\n",
    "    Args:\n",
    "    - prophet_lag1_url (str): File path or URL to Prophet forecasts\n",
    "    - training_end_date: end date for training data\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Combined and formatted dataframe containing Prophet forecasts\n",
    "    \"\"\"\n",
    "    # Read Prophet forecasts from Excel files or other sources\n",
    "    prophet_lag1_url = f's3://{bucket_name}/{prophet_training_prefix}/{prophet_filename}'\n",
    "    all_parts_prophet_lag1 = pd.read_excel(prophet_lag1_url)\n",
    "    \n",
    "\n",
    "    # Initialize an empty dataframe to store combined lag 1 data\n",
    "    all_lag1_data_prophet = pd.DataFrame()\n",
    "\n",
    "    all_parts_prophet_lag1 = all_parts_prophet_lag1.rename(columns={'forecasted_values':'Prophet',\n",
    "                                                                    'actual':'Total_qty_per_month'}).drop(['model_name', \n",
    "                                                                                                                   'accuracy_pred'], axis=1)\n",
    "\n",
    "    all_parts_prophet_lag1 = all_parts_prophet_lag1[[\"period\", \"Prophet\", \"part_number\", \"Total_qty_per_month\", \"type\"]]\n",
    "    test_date_range = get_consecutive_months(training_end_date)\n",
    "    all_parts_prophet_lag1 = all_parts_prophet_lag1[all_parts_prophet_lag1['period'].isin(test_date_range)].drop('type', axis=1)\n",
    "    all_parts_prophet_lag1 = all_parts_prophet_lag1[['period', 'part_number', 'Total_qty_per_month', 'Prophet']]\n",
    "    return all_parts_prophet_lag1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "981eaa8b-6ad7-47cf-a2db-709f1714e3cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:03.204342Z",
     "iopub.status.busy": "2024-09-15T10:22:03.204080Z",
     "iopub.status.idle": "2024-09-15T10:22:03.208994Z",
     "shell.execute_reply": "2024-09-15T10:22:03.208442Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_best_fit_lag1(bucket_name, lstm_training_prefix, lstm_filename, training_end_date):\n",
    "    \"\"\"\n",
    "    Combine LSTM forecasts from three different lags into a single formatted dataframe for lag 1.\n",
    "\n",
    "    Args:\n",
    "    - lstm_lag1_url (str): File path or URL to LSTM forecasts\n",
    "    - training_end_date: end date for training data\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Combined and formatted dataframe containing LSTM forecasts for lag 1.\n",
    "    \"\"\"\n",
    "    # Read LSTM forecasts from Excel files or other sources\n",
    "    lstm_lag1_url = f's3://{bucket_name}/{lstm_training_prefix}/{lstm_filename}'\n",
    "    print(lstm_lag1_url)\n",
    "    lstm_data_lag1 = pd.read_excel(lstm_lag1_url)\n",
    "    \n",
    "    # Convert 'period' column to string format for filtering\n",
    "    lstm_data_lag1['period'] = pd.to_datetime(lstm_data_lag1['period']).astype(str)\n",
    "    test_date_range = get_consecutive_months(training_end_date)\n",
    "\n",
    "    # Filter data for specific periods (assumed to be the last three months of the forecast)\n",
    "    lstm_data_lag1 = lstm_data_lag1[lstm_data_lag1['period'].isin(test_date_range)].drop('last_history', \n",
    "                                                                                         axis=1).reset_index(drop=True).rename(columns={'Forecast':'LSTM single model', \n",
    "                                                                                                                                        'Actual':'Total_qty_per_month'})\n",
    "    \n",
    "    \n",
    "    return lstm_data_lag1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "023dba2e-7d6d-469c-b021-742adb2b82ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:03.211486Z",
     "iopub.status.busy": "2024-09-15T10:22:03.211206Z",
     "iopub.status.idle": "2024-09-15T10:22:03.215890Z",
     "shell.execute_reply": "2024-09-15T10:22:03.215345Z"
    }
   },
   "outputs": [],
   "source": [
    "def lasso_best_fit_lag1(bucket_name, lasso_training_prefix, lasso_filename, training_end_date):\n",
    "    \"\"\"\n",
    "    Combine and format Lasso regression forecasts from three different lags into a single dataframe for lag 1.\n",
    "\n",
    "    Args:\n",
    "    - lasso_lag1_url (str): File path or URL to Lasso regression forecasts\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Combined and formatted dataframe containing Lasso regression forecasts for lag 1.\n",
    "    \"\"\"\n",
    "    # Read Lasso regression forecasts from Excel files or other sources\n",
    "    lasso_lag1_url = f's3://{bucket_name}/{lasso_training_prefix}/{lasso_filename}'\n",
    "    lasso_data_lag1 = pd.read_excel(lasso_lag1_url)\n",
    "    test_date_range = get_consecutive_months(training_end_date)\n",
    "    lasso_data_lag1['period'] = pd.to_datetime(lasso_data_lag1['period']).astype(str)\n",
    "    \n",
    "    # Filter data for specific periods (assumed to be the last three months of the forecast)\n",
    "    lasso_lag1_subset = lasso_data_lag1[lasso_data_lag1['period'].isin(test_date_range)].reset_index(drop=True).rename(columns={'Predicted_values_lag1':'Lasso',\n",
    "                                                                                                                               'Actual_values':'Total_qty_per_month'})\n",
    "    \n",
    "    return lasso_lag1_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72fc501f-d06b-4932-9cda-22209340526b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:03.218437Z",
     "iopub.status.busy": "2024-09-15T10:22:03.218140Z",
     "iopub.status.idle": "2024-09-15T10:22:03.222945Z",
     "shell.execute_reply": "2024-09-15T10:22:03.222367Z"
    }
   },
   "outputs": [],
   "source": [
    "def ridge_best_fit_lag1(bucket_name, ridge_training_prefix, ridge_filename, training_end_date):\n",
    "    \"\"\"\n",
    "    Combine and format Ridge regression forecasts from three different lags into a single dataframe for lag 1.\n",
    "\n",
    "    Args:\n",
    "    - ridge_lag1_url (str): File path or URL to Ridge regression forecasts for lag 1.\n",
    "    - training_end_date: end date for training data\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Combined and formatted dataframe containing Ridge regression forecasts for lag 1.\n",
    "    \"\"\"\n",
    "    # Read Ridge regression forecasts from Excel files or other sources\n",
    "    ridge_lag1_url = f's3://{bucket_name}/{ridge_training_prefix}/{ridge_filename}'\n",
    "    ridge_data_lag1 = pd.read_excel(ridge_lag1_url)\n",
    "    test_date_range = get_consecutive_months(training_end_date)\n",
    "    ridge_data_lag1['period'] = pd.to_datetime(ridge_data_lag1['period']).astype(str)\n",
    "    \n",
    "    # Filter data for specific periods (assumed to be the last three months of the forecast)\n",
    "    ridge_lag1_subset = ridge_data_lag1[ridge_data_lag1['period'].isin(test_date_range)].reset_index(drop=True).rename(columns={'Predicted_values_lag1':'Ridge',\n",
    "                                                                                                                               'Actual_values':'Total_qty_per_month'})\n",
    "    \n",
    "    return ridge_lag1_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ffc77cb-cbcf-4a01-be63-5bc6639d06b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:03.225588Z",
     "iopub.status.busy": "2024-09-15T10:22:03.225301Z",
     "iopub.status.idle": "2024-09-15T10:22:03.229926Z",
     "shell.execute_reply": "2024-09-15T10:22:03.229377Z"
    }
   },
   "outputs": [],
   "source": [
    "def xgboost_best_fit_lag1(bucket_name, xgboost_training_prefix, xgboost_filename, training_end_date):\n",
    "    \"\"\"\n",
    "    Combine and format XGBoost regression forecasts\n",
    "\n",
    "    Args:\n",
    "    - xgboost_lag1_url (str): File path or URL to XGBoost regression forecasts for lag 1.\n",
    "    - training_end_date: end date for training data\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Combined and formatted dataframe containing XGBoost regression forecasts for lag 1.\n",
    "    \"\"\"\n",
    "    # Read XGBoost regression forecasts from Excel files or other sources\n",
    "    xgboost_lag1_url = f's3://{bucket_name}/{xgboost_training_prefix}/{xgboost_filename}'\n",
    "    xgb_data_lag1 = pd.read_excel(xgboost_lag1_url)\n",
    "    test_date_range = get_consecutive_months(training_end_date)\n",
    "    xgb_data_lag1['period'] = pd.to_datetime(xgb_data_lag1['period']).astype(str)\n",
    "    \n",
    "    # Filter data for specific periods (assumed to be the last three months of the forecast)\n",
    "    xgb_lag1_subset = xgb_data_lag1[xgb_data_lag1['period'].isin(test_date_range)].reset_index(drop=True).rename(columns={'Predicted_values_lag1':'XGBoost',\n",
    "                                                                                                                               'Actual_values':'Total_qty_per_month'})   \n",
    "    return xgb_lag1_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed61dc76-15d5-483a-8d96-160902b78bd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:03.232617Z",
     "iopub.status.busy": "2024-09-15T10:22:03.232156Z",
     "iopub.status.idle": "2024-09-15T10:22:03.236440Z",
     "shell.execute_reply": "2024-09-15T10:22:03.235901Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    if file_extension == '.xlsx':\n",
    "        # Read Excel file\n",
    "        data = pd.read_excel(file_path)\n",
    "    elif file_extension == '.csv':\n",
    "        # Read CSV file\n",
    "        data = pd.read_csv(file_path)\n",
    "    elif file_extension == '.txt':\n",
    "        # Read TXT file\n",
    "        data = pd.read_csv(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format: {}\".format(file_extension))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b045f13-6ced-4e2d-9b70-0ef72b34615a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:03.238959Z",
     "iopub.status.busy": "2024-09-15T10:22:03.238672Z",
     "iopub.status.idle": "2024-09-15T10:22:03.243982Z",
     "shell.execute_reply": "2024-09-15T10:22:03.243415Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_latest_file(folder_path, file_prefix):\n",
    "    # Construct the search pattern for the CSV files with the given prefix\n",
    "    search_pattern = os.path.join(folder_path, f\"{file_prefix}*\")\n",
    "    \n",
    "    \n",
    "    # Get the list of matching files\n",
    "    csv_files = glob.glob(search_pattern)\n",
    "    \n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found.\")\n",
    "        return None\n",
    "    \n",
    "    # Extract the datetime part and convert it to a datetime object\n",
    "    csv_files_with_dates = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            # Extract the datetime stamp from the file name\n",
    "            datetime_stamp = file[len(folder_path) + len(file_prefix) + 1:-5]\n",
    "            file_datetime = datetime.strptime(datetime_stamp, \"%y%m%d_%H%M%S\")\n",
    "            csv_files_with_dates.append((file, file_datetime))\n",
    "        except ValueError:\n",
    "            print(f\"Could not parse date from file name: {file}\")\n",
    "    \n",
    "    if not csv_files_with_dates:\n",
    "        print(\"No valid CSV files found.\")\n",
    "        return None\n",
    "    \n",
    "    # Sort the files by datetime in descending order and get the latest one\n",
    "    latest_file = max(csv_files_with_dates, key=lambda x: x[1])[0]\n",
    "    \n",
    "    return latest_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3efdb00a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:03.246556Z",
     "iopub.status.busy": "2024-09-15T10:22:03.246270Z",
     "iopub.status.idle": "2024-09-15T10:22:06.093853Z",
     "shell.execute_reply": "2024-09-15T10:22:06.093219Z"
    }
   },
   "outputs": [],
   "source": [
    "prophet_data = pd.read_excel(f\"s3://{bucket}/{prefix_output}/{bestfit_file_prophet}\")\n",
    "training_end_date = (pd.to_datetime(prophet_data['period']).max() + relativedelta(months=-3)).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34ae79a7-bc1c-4a0a-aa09-4b11b5caa466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:06.096986Z",
     "iopub.status.busy": "2024-09-15T10:22:06.096657Z",
     "iopub.status.idle": "2024-09-15T10:22:08.761091Z",
     "shell.execute_reply": "2024-09-15T10:22:08.760439Z"
    }
   },
   "outputs": [],
   "source": [
    "prophet_lag1_combined = prophet_best_fit_lag1(bucket, \n",
    "                                              prefix_output, \n",
    "                                              bestfit_file_prophet,\n",
    "                                              training_end_date\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f4fdcab-2c1d-450d-93f0-ee0e05e5c482",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:08.764154Z",
     "iopub.status.busy": "2024-09-15T10:22:08.763847Z",
     "iopub.status.idle": "2024-09-15T10:22:12.731475Z",
     "shell.execute_reply": "2024-09-15T10:22:12.730830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://s3-dx-tdem-ie-nonprod-persist/persist-bo/persist-bo-demand-forecast/tpcap-models/20240913/model-training-output/train_lstm_forecast_1_2024-09-13.xlsx\n"
     ]
    }
   ],
   "source": [
    "lstm_lag1_combined = lstm_best_fit_lag1(bucket, \n",
    "                                        prefix_output, \n",
    "                                        bestfit_file_lstm,\n",
    "                                        training_end_date\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "119bed08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:12.734514Z",
     "iopub.status.busy": "2024-09-15T10:22:12.734202Z",
     "iopub.status.idle": "2024-09-15T10:22:12.737191Z",
     "shell.execute_reply": "2024-09-15T10:22:12.736660Z"
    }
   },
   "outputs": [],
   "source": [
    "# bestfit_file_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f0f2908-34f2-477b-841e-933ad728378d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:12.739688Z",
     "iopub.status.busy": "2024-09-15T10:22:12.739415Z",
     "iopub.status.idle": "2024-09-15T10:22:33.323258Z",
     "shell.execute_reply": "2024-09-15T10:22:33.322640Z"
    }
   },
   "outputs": [],
   "source": [
    "lasso_lag1_combined = lasso_best_fit_lag1(bucket, \n",
    "                                          prefix_output, \n",
    "                                          bestfit_file_lasso,\n",
    "                                          training_end_date\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac0cc7f0-2f05-463d-8525-ab53e77e83fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:33.326251Z",
     "iopub.status.busy": "2024-09-15T10:22:33.325968Z",
     "iopub.status.idle": "2024-09-15T10:22:54.121389Z",
     "shell.execute_reply": "2024-09-15T10:22:54.120670Z"
    }
   },
   "outputs": [],
   "source": [
    "ridge_lag1_combined = ridge_best_fit_lag1(bucket, \n",
    "                                          prefix_output, \n",
    "                                          bestfit_file_ridge,\n",
    "                                          training_end_date\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33962c35-3a5f-4b55-9c07-a01cfe9c4691",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:22:54.124377Z",
     "iopub.status.busy": "2024-09-15T10:22:54.124068Z",
     "iopub.status.idle": "2024-09-15T10:23:14.725964Z",
     "shell.execute_reply": "2024-09-15T10:23:14.725326Z"
    }
   },
   "outputs": [],
   "source": [
    "xgboost_lag1_combined = xgboost_best_fit_lag1(bucket, \n",
    "                                              prefix_output, \n",
    "                                              bestfit_file_xgboost,\n",
    "                                              training_end_date\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a10dfb90-62cb-4317-9f44-663779fc3e2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:23:14.729108Z",
     "iopub.status.busy": "2024-09-15T10:23:14.728805Z",
     "iopub.status.idle": "2024-09-15T10:23:14.741969Z",
     "shell.execute_reply": "2024-09-15T10:23:14.741440Z"
    }
   },
   "outputs": [],
   "source": [
    "def combined_data_lag1(prophet_lag, lstm_lag, lasso_lag, ridge_lag, xgboost_lag):\n",
    "    \"\"\"\n",
    "    Combine forecasts from Prophet, LSTM, Lasso, Ridge, and XGBoost models for lag 1,\n",
    "    calculate MAPE for each model, identify the best-fitting model for each part number,\n",
    "    and structure the results in a pivot table format.\n",
    "\n",
    "    Args:\n",
    "    - prophet_lag (DataFrame): Dataframe containing Prophet model forecasts for lag 1.\n",
    "    - lstm_lag (DataFrame): Dataframe containing LSTM model forecasts for lag 1.\n",
    "    - lasso_lag (DataFrame): Dataframe containing Lasso model forecasts for lag 1.\n",
    "    - ridge_lag (DataFrame): Dataframe containing Ridge model forecasts for lag 1.\n",
    "    - xgboost_lag (DataFrame): Dataframe containing XGBoost model forecasts for lag 1.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Pivot table containing part numbers, best-fitting model, actual values,\n",
    "                forecasted values, and accuracy predictions for each period.\n",
    "    \"\"\"\n",
    "    # Transform all part number into string format\n",
    "    prophet_lag['part_number'] = prophet_lag['part_number'].astype(str)\n",
    "    lstm_lag['part_number'] = lstm_lag['part_number'].astype(str)\n",
    "    lasso_lag['part_number'] = lasso_lag['part_number'].astype(str)\n",
    "    ridge_lag['part_number'] = ridge_lag['part_number'].astype(str)\n",
    "    xgboost_lag['part_number'] = xgboost_lag['part_number'].astype(str)\n",
    "\n",
    "    prophet_lag['period'] = pd.to_datetime(prophet_lag['period']).astype(str)\n",
    "    xgboost_lag['period'] = pd.to_datetime(xgboost_lag['period']).astype(str)\n",
    "    lasso_lag['period'] = pd.to_datetime(lasso_lag['period']).astype(str)\n",
    "    ridge_lag['period'] = pd.to_datetime(ridge_lag['period']).astype(str)\n",
    "    lstm_lag['period'] = pd.to_datetime(lstm_lag['period']).astype(str)\n",
    "    \n",
    "    # Merge all forecasts into a single dataframe\n",
    "    fin_data = pd.merge(prophet_lag1_combined, \n",
    "                         pd.merge(xgboost_lag1_combined.drop('Total_qty_per_month', axis=1),\n",
    "                                  pd.merge(lasso_lag1_combined.drop('Total_qty_per_month', axis=1),\n",
    "                                           pd.merge(ridge_lag1_combined.drop('Total_qty_per_month', axis=1),\n",
    "                                                    lstm_lag1_combined.drop('Total_qty_per_month', axis=1),\n",
    "                                                    on=['period', 'part_number'], how='left'),\n",
    "                                           on=['period', 'part_number'], how='left'),\n",
    "                                  on=['period', 'part_number'], how='left'),\n",
    "                         on=['period', 'part_number'], how='left')\n",
    "\n",
    "    # Calculate MAPE for each model\n",
    "    all_lag1_data_sp3_all_models_mape = fin_data.iloc[:,:3]\n",
    "    for cols in fin_data.iloc[:,3:].columns:\n",
    "        all_lag1_data_sp3_all_models_mape[cols] = fin_data.apply(lambda row: calculate_mape(row['Total_qty_per_month'], row[cols]), axis=1)\n",
    "    \n",
    "    # Prepare dataframe for storing MAPE values\n",
    "    all_forecasted_data_mape_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over model columns to calculate mean MAPE per part number\n",
    "    for model_name in all_lag1_data_sp3_all_models_mape.columns[3:].tolist():\n",
    "        model_mape_df = all_lag1_data_sp3_all_models_mape.groupby(['part_number'])[model_name].mean().reset_index()\n",
    "        model_mape_df = model_mape_df.melt(id_vars='part_number', \n",
    "                                           var_name=model_name, \n",
    "                                           value_vars=model_name)\n",
    "        model_mape_df.columns = ['part_number', 'model', 'value']\n",
    "        all_forecasted_data_mape_df = pd.concat([all_forecasted_data_mape_df, model_mape_df], ignore_index=True)\n",
    "\n",
    "    # Select the rows with maximum MAPE for each part number\n",
    "    all_forecasted_data_mape_df_max = all_forecasted_data_mape_df.loc[all_forecasted_data_mape_df.groupby('part_number')['value'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "    # Create final dataframe to store best-fit forecasts and accuracy predictions\n",
    "    final_df_best_fit = pd.DataFrame()\n",
    "\n",
    "    # Iterate over part numbers to select best-fitting model and populate final dataframe\n",
    "    for pnt in all_forecasted_data_mape_df_max['part_number'].unique():\n",
    "        best_model_name = all_forecasted_data_mape_df_max[all_forecasted_data_mape_df_max['part_number']==pnt]['model'].values[0]\n",
    "        forecasted_data_subset_pnt = fin_data.loc[fin_data['part_number']==pnt, \n",
    "                                                                ['part_number', 'period', \n",
    "                                                                 'Total_qty_per_month', best_model_name]]\n",
    "        forecasted_data_subset_pnt['model_name'] = best_model_name\n",
    "        forecasted_data_subset_pnt.rename(columns={best_model_name:'forecasted_values'}, inplace=True)\n",
    "        final_df_best_fit = pd.concat([final_df_best_fit, forecasted_data_subset_pnt], ignore_index=True)\n",
    "\n",
    "    # Transform dates into string\n",
    "    final_df_best_fit['period'] = final_df_best_fit['period'].astype(str)\n",
    "\n",
    "    # Calculate accuracy prediction based on actual and forecasted values\n",
    "    final_df_best_fit = final_df_best_fit.assign(accuracy_pred = \n",
    "                                                 lambda x: np.where(x['Total_qty_per_month']!=0,\n",
    "                                                                    (1-(abs(x['forecasted_values'] - x['Total_qty_per_month'])/x['Total_qty_per_month'])),\n",
    "                                                                    0))\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    final_df_best_fit.rename(columns={'Total_qty_per_month': 'actual', 'forecasted_values': 'forecasted'}, inplace=True)\n",
    "\n",
    "    # Create pivot table to summarize results\n",
    "    pivot_df = final_df_best_fit.rename(columns={'Total_qty_per_month':'actual', \n",
    "                                                 'forecasted_values':'forecasted'}).pivot_table(index=['part_number', 'model_name'], \n",
    "                                                                                                columns='period', \n",
    "                                                                                                values=['actual', 'forecasted', 'accuracy_pred'], \n",
    "                                                                                                sort=False)\n",
    "    # Flatten the column names\n",
    "    pivot_df.columns = [f'{col[1]}_{col[0]}' for col in pivot_df.columns]\n",
    "    # Sort the columns\n",
    "    pivot_df = pivot_df.sort_index(axis=1).reset_index()\n",
    "\n",
    "    return pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f065861-a134-4a4a-9025-6aa94d69f9a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:23:14.744340Z",
     "iopub.status.busy": "2024-09-15T10:23:14.744096Z",
     "iopub.status.idle": "2024-09-15T10:23:41.831499Z",
     "shell.execute_reply": "2024-09-15T10:23:41.830884Z"
    }
   },
   "outputs": [],
   "source": [
    "final_data_compared_bestfit_1 = combined_data_lag1(prophet_lag1_combined, lstm_lag1_combined, lasso_lag1_combined, ridge_lag1_combined, xgboost_lag1_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df50c29c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:23:41.834458Z",
     "iopub.status.busy": "2024-09-15T10:23:41.834170Z",
     "iopub.status.idle": "2024-09-15T10:23:41.840320Z",
     "shell.execute_reply": "2024-09-15T10:23:41.839808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6525, 11)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_compared_bestfit_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8372207b-34f5-4729-8fd8-aa6e3008925d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:23:41.842863Z",
     "iopub.status.busy": "2024-09-15T10:23:41.842597Z",
     "iopub.status.idle": "2024-09-15T10:23:41.855377Z",
     "shell.execute_reply": "2024-09-15T10:23:41.854864Z"
    }
   },
   "outputs": [],
   "source": [
    "def final_data_bestfit_comparison(final_data_compared_bestfit):\n",
    "    # Extract the static columns and the dynamic date columns\n",
    "    static_columns = ['part_number', 'model_name']\n",
    "    date_columns = [col for col in final_data_compared_bestfit.columns if col not in static_columns]\n",
    "\n",
    "    # Function to extract date and type\n",
    "    def extract_date_and_type(col_name):\n",
    "        parts = col_name.split('_')\n",
    "        date = parts[0]\n",
    "        col_type = '_'.join(parts[1:])\n",
    "        return date, col_type\n",
    "\n",
    "    # Sort the date columns based on date and type\n",
    "    sorted_date_columns = sorted(date_columns, key=lambda x: (extract_date_and_type(x)[0], ['actual', 'forecasted', 'accuracy_pred'].index(extract_date_and_type(x)[1])))\n",
    "\n",
    "    # Combine static columns with sorted date columns\n",
    "    sorted_columns = static_columns + sorted_date_columns\n",
    "\n",
    "    # Reorder the DataFrame columns\n",
    "    final_data_compared_bestfit = final_data_compared_bestfit[sorted_columns]\n",
    "\n",
    "    actual_columns = [col for col in date_columns if 'actual' in col]\n",
    "    forecasted_columns = [col for col in date_columns if 'forecasted' in col]\n",
    "    date_columns_sorted = [col for col in final_data_compared_bestfit.columns if col not in static_columns]\n",
    "    final_data_compared_bestfit['Total_Actual_Demand'] = final_data_compared_bestfit[actual_columns].sum(axis=1)\n",
    "    final_data_compared_bestfit['Total_Forecasted_Demand'] = final_data_compared_bestfit[forecasted_columns].sum(axis=1)\n",
    "    final_data_compared_bestfit['Closeness_to_accuracy'] = final_data_compared_bestfit.apply(lambda row: \n",
    "                                                             calculate_mape(row['Total_Actual_Demand'], \n",
    "                                                                row['Total_Forecasted_Demand']), axis=1)\n",
    "    final_data_compared_bestfit['Confidence_interval'] = '#N/A'\n",
    "    final_data_compared_bestfit.loc[final_data_compared_bestfit['model_name']==\"Prophet\", 'Confidence_interval'] = 0.8\n",
    "    \n",
    "    def create_bucket(df, column_name, new_col_name):\n",
    "        # Define the conditions dynamically\n",
    "        conditions = [\n",
    "            (df[column_name] < 0.3),\n",
    "            (df[column_name] >= 0.3) & (df[column_name] < 0.5),\n",
    "            (df[column_name] >= 0.5) & (df[column_name] < 0.6),\n",
    "            (df[column_name] >= 0.6) & (df[column_name] < 0.79),\n",
    "            (df[column_name] >= 0.8)\n",
    "        ]\n",
    "\n",
    "        # Define the choices\n",
    "        choices = [\n",
    "            \"< 30\",\n",
    "            \"30-50\",\n",
    "            \"50-60\",\n",
    "            \"60-79\",\n",
    "            \"> 80\"\n",
    "        ]\n",
    "\n",
    "        # Use numpy.select to apply the conditions and choices dynamically\n",
    "        df['Revised_Bucket'] = np.select(conditions, choices, default=\"\")\n",
    "\n",
    "        # Display the DataFrame\n",
    "        return df\n",
    "\n",
    "    create_bucket(final_data_compared_bestfit, 'Closeness_to_accuracy', 'Revised_Bucket')\n",
    "\n",
    "    final_data_compared_bestfit['Avg_residual'] = (final_data_compared_bestfit['Total_Actual_Demand'] - final_data_compared_bestfit['Total_Forecasted_Demand'])/3\n",
    "    final_data_compared_bestfit = final_data_compared_bestfit.assign(Variation = lambda x: np.where(np.logical_and(x['Avg_residual'] > -10, x['Avg_residual'] < 10), 1, 0))\n",
    "    \n",
    "    def create_threshold(acc, var):\n",
    "        if acc >= 0.8:\n",
    "            return True\n",
    "        elif acc<0.8 and var==1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    final_data_compared_bestfit['Threshold'] = final_data_compared_bestfit.apply(lambda x:create_threshold(x['Closeness_to_accuracy'], x['Variation']), axis=1)\n",
    "    \n",
    "    # Melt the DataFrame\n",
    "    date_cols = date_columns_sorted[:]\n",
    "    df_melted = pd.melt(final_data_compared_bestfit, id_vars=['part_number', 'model_name', 'Total_Actual_Demand', 'Total_Forecasted_Demand', \n",
    "                                     'Closeness_to_accuracy', 'Confidence_interval', 'Revised_Bucket', 'Avg_residual', \n",
    "                                     'Variation', 'Threshold'],\n",
    "                        value_vars=date_cols, \n",
    "                        var_name='date_measure', \n",
    "                        value_name='value')\n",
    "\n",
    "    # Extract date and measure type\n",
    "    df_melted[['date', 'measure']] = df_melted['date_measure'].str.extract(r'(\\d{4}-\\d{2}-\\d{2})_(\\w+)')\n",
    "    df_melted.drop('date_measure', axis=1, inplace=True)\n",
    "\n",
    "    # Pivot the DataFrame\n",
    "    df_pivoted = df_melted.pivot_table(index=['part_number', 'model_name', 'date'], \n",
    "                                       columns='measure', \n",
    "                                       values='value', \n",
    "                                       aggfunc='first').reset_index()\n",
    "\n",
    "    # Merge with static columns\n",
    "    static_cols = final_data_compared_bestfit.drop(columns=date_cols).drop_duplicates(subset=['part_number', 'model_name'])\n",
    "    result = pd.merge(df_pivoted, static_cols, on=['part_number', 'model_name'])\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    result.columns.name = None\n",
    "    result = result.rename(columns={\n",
    "        'actual': 'Actual',\n",
    "        'forecasted': 'Forecasted',\n",
    "        'accuracy_pred': 'Accuracy_Pred'\n",
    "    })\n",
    "\n",
    "    result_subset_1 = result.drop(['Total_Actual_Demand', 'Total_Forecasted_Demand', 'Avg_residual'], axis=1)\n",
    "    return result_subset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a0ee74ee-7467-4520-beb5-7ee5dee14a7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:23:41.857757Z",
     "iopub.status.busy": "2024-09-15T10:23:41.857500Z",
     "iopub.status.idle": "2024-09-15T10:23:47.391704Z",
     "shell.execute_reply": "2024-09-15T10:23:47.391068Z"
    }
   },
   "outputs": [],
   "source": [
    "bucket_new = config.get_value('bucket_new')\n",
    "prefix_bestfit_file = config.get_value('prefix_bestfit_file')\n",
    "file_name_bestfit = f'{config.get_value(\"file_name_bestfit\")}{file_date_training}.xlsx'\n",
    "file_path_bestfit = f's3://{bucket_new}/{prefix_bestfit_file}/{file_name_bestfit}'\n",
    "\n",
    "final_data_compared_bestfit_last = final_data_bestfit_comparison(final_data_compared_bestfit_1)\n",
    "final_data_compared_bestfit_last.to_excel(file_path_bestfit, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7e943709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:23:47.394783Z",
     "iopub.status.busy": "2024-09-15T10:23:47.394460Z",
     "iopub.status.idle": "2024-09-15T10:23:47.398598Z",
     "shell.execute_reply": "2024-09-15T10:23:47.398081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://s3-dx-tdem-ie-nonprod-curated/curated-bo/curated-bo-demand-forecast/bestfit-selected-models/bestfit_selected_models_historic_months_2024-09-13.xlsx'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_bestfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b91a08a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:23:47.401025Z",
     "iopub.status.busy": "2024-09-15T10:23:47.400745Z",
     "iopub.status.idle": "2024-09-15T10:23:47.404650Z",
     "shell.execute_reply": "2024-09-15T10:23:47.404146Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19575, 11)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_compared_bestfit_last.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d16b6c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>part_number</th>\n",
       "      <th>model_name</th>\n",
       "      <th>date</th>\n",
       "      <th>Accuracy_Pred</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Forecasted</th>\n",
       "      <th>Closeness_to_accuracy</th>\n",
       "      <th>Confidence_interval</th>\n",
       "      <th>Revised_Bucket</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04000081B0</td>\n",
       "      <td>Prophet</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>0.897352</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.178818</td>\n",
       "      <td>0.753194</td>\n",
       "      <td>0.8</td>\n",
       "      <td>60-79</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04000081B0</td>\n",
       "      <td>Prophet</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>-0.947245</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.894491</td>\n",
       "      <td>0.753194</td>\n",
       "      <td>0.8</td>\n",
       "      <td>60-79</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04000081B0</td>\n",
       "      <td>Prophet</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>0.954943</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.135172</td>\n",
       "      <td>0.753194</td>\n",
       "      <td>0.8</td>\n",
       "      <td>60-79</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0411106101</td>\n",
       "      <td>Prophet</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.051309</td>\n",
       "      <td>-3.323518</td>\n",
       "      <td>0.8</td>\n",
       "      <td>&lt; 30</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0411106101</td>\n",
       "      <td>Prophet</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.613132</td>\n",
       "      <td>-3.323518</td>\n",
       "      <td>0.8</td>\n",
       "      <td>&lt; 30</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19570</th>\n",
       "      <td>PZ08100005</td>\n",
       "      <td>Ridge</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>0.988924</td>\n",
       "      <td>156.0</td>\n",
       "      <td>154.272150</td>\n",
       "      <td>0.953283</td>\n",
       "      <td>#N/A</td>\n",
       "      <td>&gt; 80</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19571</th>\n",
       "      <td>PZ08100005</td>\n",
       "      <td>Ridge</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>0.851722</td>\n",
       "      <td>153.0</td>\n",
       "      <td>130.313446</td>\n",
       "      <td>0.953283</td>\n",
       "      <td>#N/A</td>\n",
       "      <td>&gt; 80</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19572</th>\n",
       "      <td>PZT0038023</td>\n",
       "      <td>Prophet</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>0.747158</td>\n",
       "      <td>320.0</td>\n",
       "      <td>239.090712</td>\n",
       "      <td>0.933336</td>\n",
       "      <td>0.8</td>\n",
       "      <td>&gt; 80</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19573</th>\n",
       "      <td>PZT0038023</td>\n",
       "      <td>Prophet</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>0.927355</td>\n",
       "      <td>305.0</td>\n",
       "      <td>327.156868</td>\n",
       "      <td>0.933336</td>\n",
       "      <td>0.8</td>\n",
       "      <td>&gt; 80</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19574</th>\n",
       "      <td>PZT0038023</td>\n",
       "      <td>Prophet</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>0.997817</td>\n",
       "      <td>265.0</td>\n",
       "      <td>264.421596</td>\n",
       "      <td>0.933336</td>\n",
       "      <td>0.8</td>\n",
       "      <td>&gt; 80</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19575 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      part_number model_name        date  Accuracy_Pred  Actual  Forecasted  \\\n",
       "0      04000081B0    Prophet  2024-04-01       0.897352     8.0    7.178818   \n",
       "1      04000081B0    Prophet  2024-05-01      -0.947245     2.0    5.894491   \n",
       "2      04000081B0    Prophet  2024-06-01       0.954943     3.0    3.135172   \n",
       "3      0411106101    Prophet  2024-04-01       0.000000     0.0   16.051309   \n",
       "4      0411106101    Prophet  2024-05-01       0.000000     0.0    5.613132   \n",
       "...           ...        ...         ...            ...     ...         ...   \n",
       "19570  PZ08100005      Ridge  2024-05-01       0.988924   156.0  154.272150   \n",
       "19571  PZ08100005      Ridge  2024-06-01       0.851722   153.0  130.313446   \n",
       "19572  PZT0038023    Prophet  2024-04-01       0.747158   320.0  239.090712   \n",
       "19573  PZT0038023    Prophet  2024-05-01       0.927355   305.0  327.156868   \n",
       "19574  PZT0038023    Prophet  2024-06-01       0.997817   265.0  264.421596   \n",
       "\n",
       "       Closeness_to_accuracy Confidence_interval Revised_Bucket  Variation  \\\n",
       "0                   0.753194                 0.8          60-79          1   \n",
       "1                   0.753194                 0.8          60-79          1   \n",
       "2                   0.753194                 0.8          60-79          1   \n",
       "3                  -3.323518                 0.8           < 30          1   \n",
       "4                  -3.323518                 0.8           < 30          1   \n",
       "...                      ...                 ...            ...        ...   \n",
       "19570               0.953283                #N/A           > 80          1   \n",
       "19571               0.953283                #N/A           > 80          1   \n",
       "19572               0.933336                 0.8           > 80          0   \n",
       "19573               0.933336                 0.8           > 80          0   \n",
       "19574               0.933336                 0.8           > 80          0   \n",
       "\n",
       "       Threshold  \n",
       "0           True  \n",
       "1           True  \n",
       "2           True  \n",
       "3           True  \n",
       "4           True  \n",
       "...          ...  \n",
       "19570       True  \n",
       "19571       True  \n",
       "19572       True  \n",
       "19573       True  \n",
       "19574       True  \n",
       "\n",
       "[19575 rows x 11 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_compared_bestfit_last"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7ffb9",
   "metadata": {},
   "source": [
    "# power BI process starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6667d6e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:23:47.407120Z",
     "iopub.status.busy": "2024-09-15T10:23:47.406866Z",
     "iopub.status.idle": "2024-09-15T10:23:47.410187Z",
     "shell.execute_reply": "2024-09-15T10:23:47.409680Z"
    }
   },
   "outputs": [],
   "source": [
    "bucket_curated = config.get_value('bucket_new')\n",
    "prefix_gtmaster = config.get_value('prefix_gtmaster')\n",
    "prefix_proc = config.get_value('prefix_proc')\n",
    "prefix_pic_master_path = config.get_value(\"prefix_pic_master_path\")\n",
    "product_master_path_prefix = config.get_value('product_master_path_prefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a459c7b-4b30-4f07-8118-57c28d4d1df8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:23:47.412702Z",
     "iopub.status.busy": "2024-09-15T10:23:47.412451Z",
     "iopub.status.idle": "2024-09-15T10:23:47.418026Z",
     "shell.execute_reply": "2024-09-15T10:23:47.417446Z"
    }
   },
   "outputs": [],
   "source": [
    "# def dashboard_builder(final_data_compared_bestfit_last_1):    \n",
    "#     part_master=pd.read_excel(part_master_path_1)\n",
    "#     part_master['ICC_seg'] = part_master['ICC'].apply(lambda x:x[0])\n",
    "#     part_master['PART_NO'] = part_master['PART_NO'].astype(str)\n",
    "#     result_subset=final_data_compared_bestfit_last_1[:]\n",
    "#     result_subset['part_number'] = result_subset['part_number'].astype(str)\n",
    "#     result_subset = result_subset.merge(part_master[['PART_NO', 'ICC_seg']].drop_duplicates(), \n",
    "#                                         left_on='part_number', \n",
    "#                                         right_on='PART_NO', \n",
    "#                                         how='left').drop('PART_NO', \n",
    "#                                                          axis=1)\n",
    "#     result_subset = result_subset[['part_number', 'model_name', 'ICC_seg','Actual',\n",
    "#            'Forecasted','Accuracy_Pred', 'Closeness_to_accuracy', 'Confidence_interval',\n",
    "#            'Revised_Bucket', 'Variation', 'Threshold', 'date']]\n",
    "#     result_subset = result_subset.rename(columns = {'part_number': 'Part Number', 'model_name': 'Model Name', \n",
    "#                                                     'Accuracy_Pred': 'New Model Accuracy', 'Closeness_to_accuracy':'Total Accuracy',\n",
    "#                                                     'date': 'Date', 'ICC_seg':'ICC'\n",
    "#                                                    })\n",
    "#     result_subset['Lag'] = 'Lag1'\n",
    "#     result_subset['Part Number'] = result_subset['Part Number'].astype(str)\n",
    "\n",
    "#     # GT Part Master data\n",
    "#     # Specify the column names and their types\n",
    "#     dtype = {\n",
    "#         'PROCUREMENT_OPERATOR_ID': str  # 'Part Number' column will be read as strings\n",
    "#     }\n",
    "#     data_gt_master = pd.read_csv(data_gt_master_path_1)\n",
    "\n",
    "#     # PIC master data\n",
    "#     dtype_pic = {\n",
    "#         'ID': str  # 'Part Number' column will be read as strings\n",
    "#     }\n",
    "#     data_pic_master = pd.read_csv(data_pic_master_path_1)\n",
    "\n",
    "#     # Supplier master\n",
    "#     data_supp_master = pd.read_csv(data_supp_master_path_1)\n",
    "#     data_gt_master['PART_NO'] = data_gt_master['PART_NO'].astype(str)\n",
    "#     all_data_gt_concat = result_subset.merge(data_gt_master, left_on='Part Number', right_on='PART_NO', how='left')\n",
    "#     all_data_gt_concat_supp_pic = pd.merge(all_data_gt_concat,data_supp_master[['Vendor CD', 'SUPPLIER_NAME']], \n",
    "#                                            left_on='Vendor Code', \n",
    "#                                            right_on='Vendor CD', \n",
    "#                                            how='left').merge(data_pic_master, left_on='PROCUREMENT_OPERATOR_ID', right_on='ID', how='left')\n",
    "#     all_data_gt_concat_supp_pic_subset = all_data_gt_concat_supp_pic[['Part Number', 'Model Name', 'ICC', 'Actual', 'Forecasted',\n",
    "#            'New Model Accuracy', 'Threshold','Date', 'Lag',\n",
    "#            'PART_NAME', 'Vendor Code','SUPPLIER_NAME', 'PIC',\n",
    "#            'PIC (Abb)']]\n",
    "#     all_data_gt_concat_supp_pic_subset_pm_data = pd.merge(all_data_gt_concat_supp_pic_subset, \n",
    "#                                                           part_master[['PART_NO', 'IMPORT_CD', 'PRODUCT_GROUP']],\n",
    "#                                                           left_on='Part Number', right_on = 'PART_NO', how='left'\n",
    "#                                                          )\n",
    "#     all_data_gt_concat_supp_pic_subset_pm_data = all_data_gt_concat_supp_pic_subset_pm_data.drop('PART_NO', axis=1)\n",
    "\n",
    "#     all_data_sorted = all_data_gt_concat_supp_pic_subset_pm_data[['Part Number', 'Model Name', 'ICC','Date', 'Actual', 'Forecasted',\n",
    "#                                                                   'New Model Accuracy', 'PRODUCT_GROUP', 'IMPORT_CD', 'Lag', 'SUPPLIER_NAME', 'PIC', 'PIC (Abb)', 'Threshold']]\n",
    "\n",
    "#     all_data_sorted['Year'] = pd.to_datetime(all_data_sorted['Date']).dt.year\n",
    "#     all_data_sorted['Month Name'] = pd.to_datetime(all_data_sorted['Date']).dt.month_name()\n",
    "#     all_data_sorted['Month'] = pd.to_datetime(all_data_sorted['Date']).dt.month\n",
    "#     all_data_sorted['sort'] = pd.to_datetime(all_data_sorted['Date']).dt.year.astype(str) + pd.to_datetime(all_data_sorted['Date']).dt.month.astype(str)\n",
    "#     all_data_sorted['months'] = pd.to_datetime(all_data_sorted['Date']).dt.strftime('%b').astype(str)+pd.to_datetime(all_data_sorted['Date']).dt.strftime('%y').astype(str)\n",
    "\n",
    "#     col_names_list = \"\"\"Part Number\n",
    "#     Model Name\n",
    "#     ICC\n",
    "#     Date\n",
    "#     Actual\n",
    "#     Forecasted\n",
    "#     Old Model Accuracy\n",
    "#     New Model Accuracy\n",
    "#     Product Group\n",
    "#     Source Type\n",
    "#     Lag\n",
    "#     Supplier Name\n",
    "#     Person in Charge\n",
    "#     PIC (Abb)\n",
    "#     Threshold\n",
    "#     Year\n",
    "#     Month Name\n",
    "#     Month\n",
    "#     sort\n",
    "#     months\"\"\".split('\\n')\n",
    "#     col_names_list = list(map(lambda x:x.strip(), col_names_list))\n",
    "#     all_data_sorted['Old Model Accuracy']=np.nan\n",
    "#     all_data_sorted = all_data_sorted[['Part Number', 'Model Name', 'ICC', 'Date', 'Actual', 'Forecasted','Old Model Accuracy',\n",
    "#            'New Model Accuracy', 'PRODUCT_GROUP', 'IMPORT_CD', 'Lag',\n",
    "#            'SUPPLIER_NAME', 'PIC', 'PIC (Abb)', 'Threshold','Year', 'Month Name', 'Month',\n",
    "#            'sort', 'months']]\n",
    "#     all_data_sorted.columns = col_names_list\n",
    "#     all_data_sorted['Forecast_flag'] = 'Historic'\n",
    "#     return all_data_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e6aa7db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:23:47.420480Z",
     "iopub.status.busy": "2024-09-15T10:23:47.420227Z",
     "iopub.status.idle": "2024-09-15T10:23:47.431491Z",
     "shell.execute_reply": "2024-09-15T10:23:47.430888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to read a single Parquet file\n",
    "def read_parquet_file(file):\n",
    "    return pd.read_parquet(f's3://{file}', engine='pyarrow')\n",
    "\n",
    "def gt_parquet_read(bucket_curated_name, latest_gt_master_parquet):\n",
    "    # Recursively list all Parquet files in the nested S3 path\n",
    "#     parquet_files_gt_partm = fs.glob(f's3://{bucket_curated_name}/{gt_master_parquet}/*/*/*/*.parquet')\n",
    "    parquet_files_gt_partm = fs.glob(f's3://{bucket_curated_name}/{latest_gt_master_parquet}*.parquet') ###UPDATED\n",
    "\n",
    "    df_master_gt_part_master = pd.DataFrame()\n",
    "    for file in parquet_files_gt_partm:\n",
    "        df_gt = pd.read_parquet(f's3://{file}', engine='pyarrow')\n",
    "        df_master_gt_part_master = pd.concat([df_master_gt_part_master,df_gt],ignore_index=True)\n",
    "    df_master_gt_part_master_subset = df_master_gt_part_master.sort_values('sales_start_dt', ascending=False)\n",
    "    df_master_gt_part_master_subset = df_master_gt_part_master_subset.drop_duplicates(subset=['part_no'], keep='first')\n",
    "    df_master_gt_part_master_subset = df_master_gt_part_master_subset[[\"part_no\", \"country_origin_cd\"]].drop_duplicates()\n",
    "    df_master_gt_part_master_subset = df_master_gt_part_master_subset.assign(IMPORT_CD = \n",
    "                                                                             lambda x: np.where(x['country_origin_cd']=='TH', \n",
    "                                                                                                'LSP', \n",
    "                                                                                                np.where(x['country_origin_cd']=='JP', \n",
    "                                                                                                         'JSP', 'MSP')))\n",
    "    df_master_gt_part_master_subset = df_master_gt_part_master_subset.drop('country_origin_cd', axis=1)\n",
    "    df_master_gt_part_master_subset.columns = [col.upper() for col in df_master_gt_part_master_subset.columns]\n",
    "    return df_master_gt_part_master_subset\n",
    "\n",
    "def proc_parquet_read(bucket_curated_name, latest_proc_master_parquet):\n",
    "    # Recursively list all Parquet files in the nested S3 path\n",
    "#     parquet_files_proc = fs.glob(f's3://{bucket_curated_name}/{proc_master_parquet}/*/*/*/*.parquet')\n",
    "    parquet_files_proc = fs.glob(f's3://{bucket_curated_name}/{latest_proc_master_parquet}*.parquet') ###UPDATED\n",
    "\n",
    "    df_master_proc_master = pd.DataFrame()\n",
    "\n",
    "    for file in parquet_files_proc:\n",
    "    #     print(file)\n",
    "        df_proc = pd.read_parquet(f's3://{file}', engine='pyarrow')\n",
    "        df_master_proc_master = pd.concat([df_master_proc_master,df_proc],ignore_index=True)\n",
    "    df_master_proc_master = df_master_proc_master.sort_values(['start_dt'], ascending=[False])\n",
    "    df_master_proc_master = df_master_proc_master.drop_duplicates(subset='part_no', keep='first')\n",
    "    df_master_proc_master_subset = df_master_proc_master[['part_no', 'procurement_div_cd', 'procurement_operator_id', 'inventory_ctrl_class']].drop_duplicates()\n",
    "    df_master_proc_master_subset['procurement_div_cd'] = df_master_proc_master_subset['procurement_div_cd'].apply(lambda x:x[-3:])\n",
    "    df_master_proc_master_subset = df_master_proc_master_subset.rename(columns={'procurement_div_cd':'Vendor Code',\n",
    "                                                                                'part_no':'PART_NO',\n",
    "                                                                                'procurement_operator_id':'PROCUREMENT_OPERATOR_ID',\n",
    "                                                                                'inventory_ctrl_class': 'ICC_seg'\n",
    "                                                                       })\n",
    "    df_master_proc_master_subset['ICC_seg'] = df_master_proc_master_subset['ICC_seg'].apply(lambda x: x[0] if pd.notna(x) and len(x) > 1 else x)\n",
    "    return df_master_proc_master_subset\n",
    "\n",
    "def pic_parquet_read(bucket_curated_name, latest_pic_master_parquet):\n",
    "    # Recursively list all Parquet files in the nested S3 path\n",
    "    parquet_files_pic_master = fs.glob(f's3://{bucket_curated_name}/{latest_pic_master_parquet}*.parquet')\n",
    "\n",
    "    df_master_pic = pd.DataFrame()\n",
    "\n",
    "    for file in parquet_files_pic_master:\n",
    "    #     print(file)\n",
    "        df_pic = pd.read_parquet(f's3://{file}', engine='pyarrow')\n",
    "        df_master_pic = pd.concat([df_master_pic,df_pic],ignore_index=True)\n",
    "    \n",
    "    df_master_pic_subset = df_master_pic[['id', 'pic', 'pic_abb']].drop_duplicates().reset_index(drop=True)\n",
    "    df_master_pic_subset.columns = [col.upper() for col in df_master_pic_subset.columns]\n",
    "\n",
    "    return df_master_pic_subset\n",
    "\n",
    "def prod_master_read(bucket_curated_name, pg_master_path):\n",
    "    pg_master_filepath = \"s3://\"+fs.glob(f's3://{bucket_curated_name}/{pg_master_path}/*.csv')[0]\n",
    "    pg_master_df = read_file(pg_master_filepath)\n",
    "    pg_master_df = pg_master_df[['PART_NO', 'PRODUCT_GROUP']].drop_duplicates()\n",
    "    return pg_master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ab9bb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2024, 10, 22, 9, 22, 14, 338884)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "061d29fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket_cur = bucket_curated\n",
    "# gt_parquet_path = prefix_gtmaster\n",
    "# proc_parquet_path = prefix_proc\n",
    "# pic_parquet_path = prefix_pic_master_path\n",
    "# prod_path = product_master_path_prefix\n",
    "# gt_data = gt_parquet_read(bucket_cur, gt_parquet_path)\n",
    "# proc_data = proc_parquet_read(bucket_cur, proc_parquet_path)\n",
    "# pic_data = pic_parquet_read(bucket_cur, pic_parquet_path)\n",
    "# pg_data = prod_master_read(bucket_cur, prod_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9884f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lastest_partition_path(bucket_name, prefix):\n",
    "    if prefix[-1] != \"/\": prefix += \"/\"\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    year_month_day = {}\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "            for obj in page.get('Contents', []):\n",
    "                key = obj['Key']\n",
    "                key = key[len(prefix):]\n",
    "                parts = key.split('/')\n",
    "#                 print(parts)\n",
    "                if len(parts) >= 3:\n",
    "                    if \"=\" in parts[0] and \"=\" in parts[1]:\n",
    "                        year = parts[0].split('=')[-1]\n",
    "                        month = parts[1].split('=')[-1]\n",
    "                        if \"=\" in parts[2]:\n",
    "                            day = parts[2].split('=')[-1]\n",
    "                        else: day = \"\"\n",
    "                            \n",
    "                        if len(day) > 2: continue\n",
    "\n",
    "                        if year not in year_month_day:\n",
    "                            year_month_day[year] = {}\n",
    "                        if month not in year_month_day[year]:\n",
    "                            year_month_day[year][month] = set()\n",
    "                        year_month_day[year][month].add(day)\n",
    "\n",
    "#     print(year_month_day.keys())\n",
    "    \n",
    "    most_recent_year = max(year_month_day.keys(), key=int)\n",
    "    most_recent_month = max(year_month_day[most_recent_year].keys(), key=int)\n",
    "    if year_month_day[most_recent_year][most_recent_month] != {\"\"}:\n",
    "        most_recent_day = max(year_month_day[most_recent_year][most_recent_month], key=int)\n",
    "        final_path = f\"{prefix}year={most_recent_year}/month={most_recent_month}/day={most_recent_day}/\"\n",
    "    else:\n",
    "        final_path = f\"{prefix}year={most_recent_year}/month={most_recent_month}/\"\n",
    "        \n",
    "    return final_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a14cd47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:23:47.433960Z",
     "iopub.status.busy": "2024-09-15T10:23:47.433704Z",
     "iopub.status.idle": "2024-09-15T10:23:47.438849Z",
     "shell.execute_reply": "2024-09-15T10:23:47.438268Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_final_master_file(bucket_cur, gt_parquet_path, proc_parquet_path, pic_parquet_path, prod_path):\n",
    "    prefix_gtmaster = get_lastest_partition_path(bucket_cur, gt_parquet_path)\n",
    "    gt_data = gt_parquet_read(bucket_cur, prefix_gtmaster)\n",
    "    prefix_procmaster = get_lastest_partition_path(bucket_cur, proc_parquet_path)\n",
    "    proc_data = proc_parquet_read(bucket_cur, prefix_procmaster)\n",
    "    prefix_picmaster = get_lastest_partition_path(bucket_cur, pic_parquet_path)\n",
    "    pic_data = pic_parquet_read(bucket_cur, prefix_picmaster)\n",
    "    \n",
    "    pg_data = prod_master_read(bucket_cur, prod_path)\n",
    "    \n",
    "    pic_data['ID'] = pic_data['ID'].astype(str)\n",
    "    gt_data['PART_NO'] = gt_data['PART_NO'].astype(str)\n",
    "    proc_data['PART_NO'] = proc_data['PART_NO'].astype(str)\n",
    "    pg_data['PART_NO'] = pg_data['PART_NO'].astype(str)\n",
    "    \n",
    "    final_master_file_data = pd.merge(pd.merge(pd.merge(gt_data, proc_data, on='PART_NO', how='left'), \n",
    "                                 pic_data, left_on='PROCUREMENT_OPERATOR_ID', \n",
    "                                 right_on='ID', how='left').drop('ID', axis=1), \n",
    "                        pg_data, on='PART_NO', how='left')\n",
    "    \n",
    "    return final_master_file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2e63d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "266e47f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:23:47.441178Z",
     "iopub.status.busy": "2024-09-15T10:23:47.440900Z",
     "iopub.status.idle": "2024-09-15T10:24:01.796294Z",
     "shell.execute_reply": "2024-09-15T10:24:01.795654Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_master_details = create_final_master_file(bucket_curated, prefix_gtmaster, prefix_proc, prefix_pic_master_path, product_master_path_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "283d25f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2024, 10, 22, 9, 22, 32, 369684)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "654f2584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:24:01.799315Z",
     "iopub.status.busy": "2024-09-15T10:24:01.799027Z",
     "iopub.status.idle": "2024-09-15T10:24:01.812588Z",
     "shell.execute_reply": "2024-09-15T10:24:01.812083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((631326, 8), 631326)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_master_details.shape, full_master_details['PART_NO'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2b1b525",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:24:01.815107Z",
     "iopub.status.busy": "2024-09-15T10:24:01.814845Z",
     "iopub.status.idle": "2024-09-15T10:24:01.822994Z",
     "shell.execute_reply": "2024-09-15T10:24:01.822469Z"
    }
   },
   "outputs": [],
   "source": [
    "def dashboard_builder(final_data_compared_bestfit_last_1, final_master_data):    \n",
    "    data_train_bestfit_set = final_data_compared_bestfit_last_1[['part_number', 'model_name','Actual',\n",
    "                                         'Forecasted','Accuracy_Pred', 'Closeness_to_accuracy', \n",
    "                                         'Confidence_interval','Revised_Bucket', 'Variation', \n",
    "                                         'Threshold', 'date']]\n",
    "\n",
    "    data_train_bestfit_set = data_train_bestfit_set.rename(columns = {'part_number': 'Part Number', 'model_name': 'Model Name', \n",
    "                                                        'Accuracy_Pred': 'New Model Accuracy', 'Closeness_to_accuracy':'Total Accuracy',\n",
    "                                                        'date': 'Date'\n",
    "                                                       })\n",
    "    data_train_bestfit_set['Lag'] = 'Lag1'\n",
    "    \n",
    "    data_train_bestfit_set_master = data_train_bestfit_set.merge(final_master_data, \n",
    "                                                             left_on='Part Number', \n",
    "                                                             right_on='PART_NO', \n",
    "                                                             how='left').drop(['PART_NO', 'PROCUREMENT_OPERATOR_ID'], \n",
    "                                                                              axis=1).rename(columns={'ICC_seg':'ICC',\n",
    "                                                                                                      'Vendor Code':'SUPPLIER_NAME',\n",
    "                                                                                                      'PIC_ABB':'PIC (Abb)'\n",
    "                                                                                                     })\n",
    "    all_data_train_sorted = data_train_bestfit_set_master[['Part Number', 'Model Name', 'ICC',\n",
    "                                                           'Date', 'Actual', 'Forecasted','New Model Accuracy', \n",
    "                                                           'PRODUCT_GROUP', 'IMPORT_CD', 'Lag', 'SUPPLIER_NAME', \n",
    "                                                           'PIC', 'PIC (Abb)', 'Threshold']]\n",
    "    \n",
    "    all_data_train_sorted['Year'] = pd.to_datetime(all_data_train_sorted['Date']).dt.year\n",
    "    all_data_train_sorted['Month Name'] = pd.to_datetime(all_data_train_sorted['Date']).dt.month_name()\n",
    "    all_data_train_sorted['Month'] = pd.to_datetime(all_data_train_sorted['Date']).dt.month\n",
    "    all_data_train_sorted['sort'] = pd.to_datetime(all_data_train_sorted['Date']).dt.year.astype(str) + pd.to_datetime(all_data_train_sorted['Date']).dt.month.astype(str)\n",
    "    all_data_train_sorted['months'] = pd.to_datetime(all_data_train_sorted['Date']).dt.strftime('%b').astype(str)+pd.to_datetime(all_data_train_sorted['Date']).dt.strftime('%y').astype(str)\n",
    "\n",
    "    col_names_list = \"\"\"Part Number\n",
    "    Model Name\n",
    "    ICC\n",
    "    Date\n",
    "    Actual\n",
    "    Forecasted\n",
    "    Old Model Accuracy\n",
    "    New Model Accuracy\n",
    "    Product Group\n",
    "    Source Type\n",
    "    Lag\n",
    "    Supplier Name\n",
    "    Person in Charge\n",
    "    PIC (Abb)\n",
    "    Threshold\n",
    "    Year\n",
    "    Month Name\n",
    "    Month\n",
    "    sort\n",
    "    months\"\"\".split('\\n')\n",
    "    col_names_list = list(map(lambda x:x.strip(), col_names_list))\n",
    "    all_data_train_sorted['Old Model Accuracy']=np.nan\n",
    "    all_data_train_sorted = all_data_train_sorted[['Part Number', 'Model Name', 'ICC', 'Date', 'Actual', 'Forecasted','Old Model Accuracy',\n",
    "           'New Model Accuracy', 'PRODUCT_GROUP', 'IMPORT_CD', 'Lag',\n",
    "           'SUPPLIER_NAME', 'PIC', 'PIC (Abb)', 'Threshold','Year', 'Month Name', 'Month',\n",
    "           'sort', 'months']]\n",
    "    all_data_train_sorted.columns = col_names_list\n",
    "    all_data_train_sorted['Forecast_flag'] = 'Historic'\n",
    "    \n",
    "    return all_data_train_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8263188-0e7a-41d1-b462-ac01d815603c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:24:01.825615Z",
     "iopub.status.busy": "2024-09-15T10:24:01.825196Z",
     "iopub.status.idle": "2024-09-15T10:24:02.258876Z",
     "shell.execute_reply": "2024-09-15T10:24:02.258256Z"
    }
   },
   "outputs": [],
   "source": [
    "final_data_sorted=dashboard_builder(final_data_compared_bestfit_last, full_master_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6e77c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bestfit_selected_models_historic_months_powerbi_2024-09-13.xlsx'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{config.get_value(\"file_name_bestfit_powerbi\")}{file_date_training}.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9dba4b42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:24:02.261846Z",
     "iopub.status.busy": "2024-09-15T10:24:02.261552Z",
     "iopub.status.idle": "2024-09-15T10:24:12.122936Z",
     "shell.execute_reply": "2024-09-15T10:24:12.122241Z"
    }
   },
   "outputs": [],
   "source": [
    "file_name_bestfit_powerbi = f'{config.get_value(\"file_name_bestfit_powerbi\")}{file_date_training}.xlsx'\n",
    "file_path_bestfit_powerbi = f's3://{bucket_new}/{prefix_bestfit_file}/{file_name_bestfit_powerbi}'\n",
    "final_data_sorted.to_excel(file_path_bestfit_powerbi, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "17e23e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://s3-dx-tdem-ie-nonprod-curated/curated-bo/curated-bo-demand-forecast/bestfit-selected-models/bestfit_selected_models_historic_months_powerbi_2024-09-13.xlsx'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_bestfit_powerbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d3d3b0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:24:12.125991Z",
     "iopub.status.busy": "2024-09-15T10:24:12.125683Z",
     "iopub.status.idle": "2024-09-15T10:24:12.130345Z",
     "shell.execute_reply": "2024-09-15T10:24:12.129781Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Part Number', 'Model Name', 'ICC', 'Date', 'Actual', 'Forecasted',\n",
       "       'Old Model Accuracy', 'New Model Accuracy', 'Product Group',\n",
       "       'Source Type', 'Lag', 'Supplier Name', 'Person in Charge', 'PIC (Abb)',\n",
       "       'Threshold', 'Year', 'Month Name', 'Month', 'sort', 'months',\n",
       "       'Forecast_flag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_sorted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1be30f4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:24:12.132733Z",
     "iopub.status.busy": "2024-09-15T10:24:12.132471Z",
     "iopub.status.idle": "2024-09-15T10:24:12.135507Z",
     "shell.execute_reply": "2024-09-15T10:24:12.134946Z"
    }
   },
   "outputs": [],
   "source": [
    "prefix_bestfit_file_stop_instance = config.get_value(\"prefix_bestfit_file_stop_instance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd58556a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'curated-bo/curated-bo-demand-forecast/bestfit-selected-models/training-final-output'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.get_value(\"prefix_bestfit_file_stop_instance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2934eac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:24:12.137839Z",
     "iopub.status.busy": "2024-09-15T10:24:12.137592Z",
     "iopub.status.idle": "2024-09-15T10:24:12.141066Z",
     "shell.execute_reply": "2024-09-15T10:24:12.140491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken for Bestfit selection:: 4.116666666666666  minutes\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print(\"Total time taken for Bestfit selection::\", (end_time-start_time).seconds/60, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e762933",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:24:12.143348Z",
     "iopub.status.busy": "2024-09-15T10:24:12.143099Z",
     "iopub.status.idle": "2024-09-15T10:24:22.067130Z",
     "shell.execute_reply": "2024-09-15T10:24:22.066507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://s3-dx-tdem-ie-nonprod-curated/curated-bo/curated-bo-demand-forecast/bestfit-selected-models/training-final-output/bestfit_selected_models_historic_months_powerbi.xlsx'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_bestfit_powerbi_stop_instance_folder = f's3://{bucket_new}/{prefix_bestfit_file_stop_instance}/{config.get_value(\"file_name_bestfit_powerbi\")[:-1]}.xlsx'\n",
    "final_data_sorted.to_excel(file_path_bestfit_powerbi_stop_instance_folder, index=False)\n",
    "file_path_bestfit_powerbi_stop_instance_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dfee95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T10:24:22.070077Z",
     "iopub.status.busy": "2024-09-15T10:24:22.069798Z",
     "iopub.status.idle": "2024-09-15T10:24:22.102772Z",
     "shell.execute_reply": "2024-09-15T10:24:22.102241Z"
    }
   },
   "outputs": [],
   "source": [
    "final_data_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056c963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
