{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c05fc8f0-87b2-4712-9ff6-341f2feb5be0",
   "metadata": {},
   "source": [
    "# Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405ca52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0be39234",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:18.343227Z",
     "iopub.status.busy": "2024-09-13T20:52:18.342981Z",
     "iopub.status.idle": "2024-09-13T20:52:25.433105Z",
     "shell.execute_reply": "2024-09-13T20:52:25.432386Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prophet\n",
      "  Downloading prophet-1.1.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting cmdstanpy>=1.0.4 (from prophet)\n",
      "  Downloading cmdstanpy-1.2.4-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from prophet) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from prophet) (3.9.1)\n",
      "Requirement already satisfied: pandas>=1.0.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from prophet) (1.5.3)\n",
      "Collecting holidays>=0.25 (from prophet)\n",
      "  Downloading holidays-0.56-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from prophet) (4.66.4)\n",
      "Requirement already satisfied: importlib-resources in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from prophet) (6.4.0)\n",
      "Collecting stanio<2.0.0,>=0.4.0 (from cmdstanpy>=1.0.4->prophet)\n",
      "  Downloading stanio-0.5.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from holidays>=0.25->prophet) (2.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=2.0.0->prophet) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=2.0.0->prophet) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=2.0.0->prophet) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=2.0.0->prophet) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=2.0.0->prophet) (21.3)\n",
      "Requirement already satisfied: pillow>=8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=2.0.0->prophet) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=2.0.0->prophet) (3.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=1.0.4->prophet) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil->holidays>=0.25->prophet) (1.16.0)\n",
      "Downloading prophet-1.1.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.4/14.4 MB\u001b[0m \u001b[31m122.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading cmdstanpy-1.2.4-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.5/94.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading holidays-0.56-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading stanio-0.5.1-py3-none-any.whl (8.1 kB)\n",
      "Installing collected packages: stanio, holidays, cmdstanpy, prophet\n",
      "Successfully installed cmdstanpy-1.2.4 holidays-0.56 prophet-1.1.5 stanio-0.5.1\n",
      "Requirement already satisfied: openpyxl in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow\n",
    "!pip install prophet\n",
    "!pip install openpyxl\n",
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c410ea46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:25.436317Z",
     "iopub.status.busy": "2024-09-13T20:52:25.435997Z",
     "iopub.status.idle": "2024-09-13T20:52:25.439446Z",
     "shell.execute_reply": "2024-09-13T20:52:25.438902Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7c21775-d629-42cb-b5a9-912a655140fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:25.442264Z",
     "iopub.status.busy": "2024-09-13T20:52:25.441893Z",
     "iopub.status.idle": "2024-09-13T20:52:28.444719Z",
     "shell.execute_reply": "2024-09-13T20:52:28.444098Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import glob\n",
    "import boto3\n",
    "import s3fs\n",
    "from config_module import Config\n",
    "config = Config()\n",
    "\n",
    "# **IMPORTANT**: Have to do this line *before* importing tensorflow\n",
    "# os.environ['PYTHONHASHSEED'] = str(2)\n",
    "# os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "# os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# import torch\n",
    "# import tensorflow as tf\n",
    "# import tensorflow.keras as keras\n",
    "# from tensorflow.keras.models import save_model, model_from_json, Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from pickle import dump, load\n",
    "# from keras.models import load_model\n",
    "import warnings\n",
    "# from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "# def reset_random_seeds():\n",
    "#     os.environ['PYTHONHASHSEED'] = str(2)\n",
    "#     tf.random.set_seed(2)\n",
    "#     np.random.seed(2)\n",
    "#     random.seed(2)\n",
    "#     tf.keras.utils.set_random_seed(2024)\n",
    "\n",
    "# Make some random data\n",
    "# reset_random_seeds()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "# tf.random.set_seed(42)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Import modules\n",
    "import itertools\n",
    "import logging\n",
    "import math\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "# import tarfile\n",
    "import joblib\n",
    "# import tempfile\n",
    "import ast\n",
    "import seaborn as sn\n",
    "# import xgboost as xgb\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from prophet import Prophet\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression\n",
    "# from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import (mean_absolute_error, mean_absolute_percentage_error,\n",
    "                             mean_squared_error)\n",
    "from sklearn.model_selection import (GridSearchCV, TimeSeriesSplit,\n",
    "                                     train_test_split)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import statsmodels.api as sm\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "786a9ca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:28.447815Z",
     "iopub.status.busy": "2024-09-13T20:52:28.447422Z",
     "iopub.status.idle": "2024-09-13T20:52:28.685629Z",
     "shell.execute_reply": "2024-09-13T20:52:28.684904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-13 20:52:28.683229\n"
     ]
    }
   ],
   "source": [
    "# Initialize the S3 filesystem\n",
    "fs = s3fs.S3FileSystem()\n",
    "s3_client = boto3.client('s3')\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c23e1273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:28.688559Z",
     "iopub.status.busy": "2024-09-13T20:52:28.688283Z",
     "iopub.status.idle": "2024-09-13T20:52:28.691462Z",
     "shell.execute_reply": "2024-09-13T20:52:28.690880Z"
    }
   },
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd037f6b-fd22-45ff-9cb9-fe5c41793165",
   "metadata": {},
   "source": [
    "# Best fit for parts - Training and saving best fit parameters for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "313c7f38-4128-4a02-a78f-6e86275300d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:28.694076Z",
     "iopub.status.busy": "2024-09-13T20:52:28.693826Z",
     "iopub.status.idle": "2024-09-13T20:52:28.698041Z",
     "shell.execute_reply": "2024-09-13T20:52:28.697433Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    if file_extension == '.xlsx':\n",
    "        # Read Excel file\n",
    "        data = pd.read_excel(file_path)\n",
    "    elif file_extension == '.csv':\n",
    "        # Read CSV file\n",
    "        data = pd.read_csv(file_path)\n",
    "    elif file_extension == '.txt':\n",
    "        # Read TXT file\n",
    "        data = pd.read_csv(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format: {}\".format(file_extension))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aebdf141-cad9-4793-a5bb-13a4818e94b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:28.700557Z",
     "iopub.status.busy": "2024-09-13T20:52:28.700310Z",
     "iopub.status.idle": "2024-09-13T20:52:28.705354Z",
     "shell.execute_reply": "2024-09-13T20:52:28.704728Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_latest_file(folder_path, file_prefix):\n",
    "    # Construct the search pattern for the CSV files with the given prefix\n",
    "    search_pattern = os.path.join(folder_path, f\"{file_prefix}*\")\n",
    "    \n",
    "    \n",
    "    # Get the list of matching files\n",
    "    csv_files = glob.glob(search_pattern)\n",
    "    \n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found.\")\n",
    "        return None\n",
    "    \n",
    "    # Extract the datetime part and convert it to a datetime object\n",
    "    csv_files_with_dates = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            # Extract the datetime stamp from the file name\n",
    "            datetime_stamp = file[len(folder_path) + len(file_prefix) + 1:-5]\n",
    "            file_datetime = datetime.strptime(datetime_stamp, \"%y%m%d_%H%M%S\")\n",
    "            csv_files_with_dates.append((file, file_datetime))\n",
    "        except ValueError:\n",
    "            print(f\"Could not parse date from file name: {file}\")\n",
    "    \n",
    "    if not csv_files_with_dates:\n",
    "        print(\"No valid CSV files found.\")\n",
    "        return None\n",
    "    \n",
    "    # Sort the files by datetime in descending order and get the latest one\n",
    "    latest_file = max(csv_files_with_dates, key=lambda x: x[1])[0]\n",
    "    \n",
    "    return latest_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e28cc5be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:28.707937Z",
     "iopub.status.busy": "2024-09-13T20:52:28.707687Z",
     "iopub.status.idle": "2024-09-13T20:52:28.713592Z",
     "shell.execute_reply": "2024-09-13T20:52:28.713011Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_latest_s3_file(bucket_name, path_prefix, file_prefix):\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # List all objects under the given path_prefix\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=path_prefix)\n",
    "    \n",
    "    if 'Contents' not in response:\n",
    "        print(\"No files found.\")\n",
    "        return None\n",
    "    \n",
    "    # Filter objects that start with the specific file path_prefix\n",
    "    filtered_files = [\n",
    "        obj for obj in response['Contents'] \n",
    "        if obj['Key'].startswith(f\"{path_prefix}{file_prefix}\")\n",
    "    ]\n",
    "    \n",
    "    if not filtered_files:\n",
    "        print(\"No files found with the specific path_prefix.\")\n",
    "        return None\n",
    "    \n",
    "    # Extract the date from each file name and store along with the key\n",
    "    files_with_dates = []\n",
    "    for obj in filtered_files:\n",
    "        key = obj['Key']\n",
    "        # Assuming the date is at the end of the filename after the last underscore\n",
    "        try:\n",
    "            # Get the base name without directory path\n",
    "            base_name = os.path.basename(key)\n",
    "            name_without_extension = os.path.splitext(base_name)[0]\n",
    "            date_str = name_without_extension.split('_')[-1]  # Get the date string part\n",
    "            file_date = datetime.strptime(date_str, \"%Y-%m-%d\")  # Parse date\n",
    "            files_with_dates.append((key, file_date))\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing date from file name {key}: {e}\")\n",
    "\n",
    "    if not files_with_dates:\n",
    "        print(\"No valid files with dates found.\")\n",
    "        return None\n",
    "\n",
    "    # Find the file with the latest date\n",
    "    latest_file_key, latest_date = max(files_with_dates, key=lambda x: x[1])\n",
    "    \n",
    "    return latest_file_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ea3649f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:28.716058Z",
     "iopub.status.busy": "2024-09-13T20:52:28.715813Z",
     "iopub.status.idle": "2024-09-13T20:52:28.719095Z",
     "shell.execute_reply": "2024-09-13T20:52:28.718567Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters to search over\n",
    "param_grid = {\n",
    "    'seasonality_mode': ['additive', 'multiplicative'],\n",
    "    'yearly_seasonality': [True, False],\n",
    "    # 'changepoint_prior_scale': [0.01, 0.07, 0.09],\n",
    "    # 'seasonality_prior_scale': [0.1, 5.0, 10.0],\n",
    "    'changepoint_prior_scale': [0.01, 0.05, 0.07],\n",
    "    'seasonality_prior_scale': [0.1, 1.0, 3.0, 5.0],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1c66b36-bc21-4810-bc94-14152ec61e0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:28.721545Z",
     "iopub.status.busy": "2024-09-13T20:52:28.721304Z",
     "iopub.status.idle": "2024-09-13T20:52:46.983906Z",
     "shell.execute_reply": "2024-09-13T20:52:46.983269Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify your S3 bucket and path\n",
    "bucket = config.get_value('bucket')\n",
    "prefix_output_file = config.get_value('prefix_cleaned_training_file_path')\n",
    "file_name_prefix = config.get_value('cleaned_training_filename_prefix')\n",
    "file_path = get_latest_s3_file(bucket,prefix_output_file,file_name_prefix)\n",
    "\n",
    "old_data = pd.read_excel('s3://'+f'{bucket}/{file_path}')\n",
    "parts_list = old_data['part_number'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4858e03c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:46.987098Z",
     "iopub.status.busy": "2024-09-13T20:52:46.986797Z",
     "iopub.status.idle": "2024-09-13T20:52:46.993220Z",
     "shell.execute_reply": "2024-09-13T20:52:46.992585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'persist-bo/persist-bo-demand-forecast/cleaned-training-data/clean_data_imputed_2024-09-13.xlsx'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bc986da-c411-4d85-9dd6-7cb0f6d189ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:46.995816Z",
     "iopub.status.busy": "2024-09-13T20:52:46.995562Z",
     "iopub.status.idle": "2024-09-13T20:52:47.011657Z",
     "shell.execute_reply": "2024-09-13T20:52:47.011149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6525"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_data['part_number'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b49761b0-429b-4bb7-9be1-da6a4a145eea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:47.013942Z",
     "iopub.status.busy": "2024-09-13T20:52:47.013686Z",
     "iopub.status.idle": "2024-09-13T20:52:47.017788Z",
     "shell.execute_reply": "2024-09-13T20:52:47.017279Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_mape(x, y):\n",
    "    \"\"\"\n",
    "    Function to calculate the Mean Absolute Percentage Error (MAPE) between two values x and y.\n",
    "    \n",
    "    Parameters:\n",
    "    x: The true value.\n",
    "    y: The predicted value.\n",
    "    \n",
    "    Returns:\n",
    "    The MAPE value as a float.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculating MAPE for x={x} and y={y}\")\n",
    "    \n",
    "    # Check if x is NaN\n",
    "    if math.isnan(x):\n",
    "        return 0\n",
    "    \n",
    "    # Check if x and y are equal\n",
    "    if x == y:\n",
    "        # x and y are equal. Returning MAPE as 1.\")\n",
    "        return 1\n",
    "    \n",
    "    # Check if x is zero\n",
    "    elif x == 0:\n",
    "        # x is zero. Returning MAPE as 0 to avoid division by zero.\")\n",
    "        return 0\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    else:\n",
    "        mape = 1 - (abs(x - y) / x)\n",
    "        return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02a21fbb-a252-4ff9-b615-3a323534eb49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:47.020231Z",
     "iopub.status.busy": "2024-09-13T20:52:47.019979Z",
     "iopub.status.idle": "2024-09-13T20:52:47.058061Z",
     "shell.execute_reply": "2024-09-13T20:52:47.057513Z"
    }
   },
   "outputs": [],
   "source": [
    "def prophet_forecast(parts_data_period, parts_data_train, parts_data_test, hyperparameters, period_lag, m_path):\n",
    "    \"\"\"\n",
    "    Perform a forecast using Prophet for multiple hyperparameter combinations.\n",
    " \n",
    "    Args:\n",
    "    - data (DataFrame): Input DataFrame containing actual values for multiple products.\n",
    "    - hyperparameters (list of dict): List of dictionaries containing hyperparameter combinations.\n",
    " \n",
    "    Returns:\n",
    "    - DataFrame: DataFrame containing actuals, fitted values, and future predictions for each product and hyperparameter combination.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Apply Box-Cox transformation: Add 1 to avoid log(0), then take the natural log\n",
    "    parts_data_train['Total_qty_per_month_transformed'] = np.log(parts_data_train['Total_qty_per_month'] + 1)\n",
    "    \n",
    "    for params in ParameterGrid(hyperparameters):\n",
    "        model = Prophet(**params)\n",
    "        \n",
    "        # Fit the model using the transformed data\n",
    "        model.fit(parts_data_train.rename(columns={'period': 'ds', 'Total_qty_per_month_transformed': 'y'}))\n",
    "        \n",
    "        future = model.make_future_dataframe(periods=period_lag, freq='MS')\n",
    "        forecast = model.predict(future)\n",
    "        \n",
    "        # Reverse the Box-Cox transformation: Take exponent and subtract 1\n",
    "        forecast['yhat_transformed'] = np.exp(forecast['yhat']) - 1\n",
    "        \n",
    "        sample_result = forecast[['ds', 'yhat_transformed']].rename(columns={'ds': 'period', 'yhat_transformed': 'fitted'})\n",
    "        sample_result['part_number'] = parts_data_period['part_number'].iloc[0]\n",
    "        sample_result['actual'] = parts_data_period['Total_qty_per_month'].values\n",
    "        sample_result['type'] = None\n",
    "        sample_result['type'].iloc[:-period_lag] = 'Train'\n",
    "        sample_result['type'].iloc[-period_lag:] = 'Test'\n",
    "        \n",
    "        sample_result['model_name'] = str(params)\n",
    "        results.append(sample_result)\n",
    "        \n",
    "        pn = parts_data_period['part_number'].iloc[0]\n",
    "        \n",
    "#         output_file = os.path.join(f\"s3://{bucket}/{m_path}/model_folder\", f\"prophet-{pn}.joblib\")\n",
    "#         output_file_temp = f\"/tmp/prophet-{pn}.joblib\"\n",
    "#         with fs.open(output_file, 'wb') as f:\n",
    "#             joblib.dump(model, f)\n",
    "#         with open(output_file_temp, 'wb') as model_file:\n",
    "#             joblib.dump(model, model_file)\n",
    "\n",
    "#         tar_model_path = output_file_temp\n",
    "#         with tarfile.open(tar_model_path, \"w:gz\") as tar:\n",
    "#             tar.add(output_file_temp, arcname=os.path.basename(output_file_temp))\n",
    "#         # Upload the tar.gz archive to S3\n",
    "#         s3_model_path = f\"{m_path}/model_folder/prophet-{pn}.tar.gz\"\n",
    "#         s3_client.upload_file(tar_model_path, bucket, s3_model_path)\n",
    "    \n",
    "    return pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "950edf80-8298-41a9-ad81-5c46e0742e4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:47.060675Z",
     "iopub.status.busy": "2024-09-13T20:52:47.060406Z",
     "iopub.status.idle": "2024-09-13T20:52:47.065401Z",
     "shell.execute_reply": "2024-09-13T20:52:47.064904Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_test_complete_dataset_from_scratch(df, start_date, end_date, pn):\n",
    "    \"\"\"\n",
    "    Function to create a complete dataset from scratch for a given part number (pn) over a specified date range.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame (not used in this function but kept for consistency).\n",
    "    start_date (datetime): The start date for the period.\n",
    "    end_date (datetime): The end date for the period.\n",
    "    pn (str): The part number for which the dataset is being created.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A DataFrame with all months in the specified period and the specified part number.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Converting start and end dates to strings: {start_date} to {end_date}\n",
    "    start_date = start_date.strftime('%Y-%m-%d')\n",
    "    end_date = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Parsing start and end dates to datetime objects\n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "     \n",
    "    # Creating a DataFrame with all months in the specified period\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
    "    all_months_df = pd.DataFrame(date_range, columns=['period'])\n",
    "    all_months_df['period'] = all_months_df['period'].dt.strftime('%Y-%m')\n",
    "    \n",
    "    # Creating a DataFrame with the specified part number\n",
    "    all_product_df = pd.DataFrame({'part_number': [pn]})\n",
    "    \n",
    "    # Cross-merging date DataFrame and part number DataFrame\n",
    "    merged_df = all_months_df.merge(all_product_df, on=None, how='cross')\n",
    "    merged_df['period'] = pd.to_datetime(merged_df['period'])\n",
    "    \n",
    "    # Completed creating the test complete dataset\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcc6a786-4a70-4f4d-9f64-28cd7082c5c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:47.067792Z",
     "iopub.status.busy": "2024-09-13T20:52:47.067536Z",
     "iopub.status.idle": "2024-09-13T20:52:47.072786Z",
     "shell.execute_reply": "2024-09-13T20:52:47.072287Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_test_complete_dataset(df, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Function to create a complete dataset with all months in the specified period for each part number in the input DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame containing at least 'part_number' and 'period' columns.\n",
    "    start_date (datetime): The start date for the period.\n",
    "    end_date (datetime): The end date for the period.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A DataFrame with all months in the specified period for each part number in the input DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Converting start and end dates to strings\n",
    "    start_date = start_date.strftime('%Y-%m-%d')\n",
    "    end_date = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Parsing start and end dates to datetime objects\n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    \n",
    "    # Creating a DataFrame with all months in the specified period\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
    "    all_months_df = pd.DataFrame(date_range, columns=['period'])\n",
    "    all_months_df['period'] = all_months_df['period'].dt.strftime('%Y-%m')\n",
    "    \n",
    "    # Creating a DataFrame with all unique part numbers from the input DataFrame\n",
    "    all_product_df = pd.DataFrame({'part_number': df['part_number'].unique()})\n",
    "    \n",
    "    # Cross-merging date DataFrame and part number DataFrame\n",
    "    merged_df = all_months_df.merge(all_product_df, on=None, how='cross')\n",
    "    merged_df['period'] = pd.to_datetime(merged_df['period'])\n",
    "    \n",
    "    # Merging the created DataFrame with the input DataFrame on 'period' and 'part_number'\n",
    "    merged_df = merged_df.merge(df, left_on=['period', 'part_number'], right_on=['period', 'part_number'], how='left')\n",
    "    \n",
    "    # Completed creating the test complete dataset\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5d5e30c-cfe2-48ac-9482-c5c38a3cf881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:47.075217Z",
     "iopub.status.busy": "2024-09-13T20:52:47.074962Z",
     "iopub.status.idle": "2024-09-13T20:52:47.080096Z",
     "shell.execute_reply": "2024-09-13T20:52:47.079591Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_complete_dataset(data_new, train_start_date, train_end_date):\n",
    "    \"\"\"\n",
    "    Function to create training and testing datasets from the input data for the specified training period.\n",
    "    \n",
    "    Parameters:\n",
    "    data_new (DataFrame): The input DataFrame containing at least 'part_number' and 'period' columns.\n",
    "    train_start_date (str): The start date for the training period in 'YYYY-MM-DD' format.\n",
    "    train_end_date (str): The end date for the training period in 'YYYY-MM-DD' format.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing two DataFrames: the training dataset and the testing dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Converting 'period' column to datetime format and formatting it as 'YYYY-MM-DD'\n",
    "    data_new['period'] = pd.to_datetime(data_new['period']).dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Sorting the data by 'part_number' and 'period'\n",
    "    data_new = data_new.sort_values(['part_number', 'period'])\n",
    "    \n",
    "    # Filtering the data from the specified training start date\n",
    "    data_new = data_new[data_new['period'] >= train_start_date]\n",
    "    \n",
    "    # Converting 'period' column back to datetime format for further processing\n",
    "    data_new['period'] = pd.to_datetime(data_new['period'])\n",
    "    \n",
    "    df_filtered_1 = data_new.copy()\n",
    "    \n",
    "    # Creating the training dataset\n",
    "    df_train = df_filtered_1[df_filtered_1['period'] <= train_end_date]\n",
    "    \n",
    "    test_start_date = (pd.to_datetime(train_end_date) + pd.DateOffset(months=1)).strftime('%Y-%m-%d')\n",
    "    test_end_date = (pd.to_datetime(train_end_date) + pd.DateOffset(months=3)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Creating the testing dataset\n",
    "    df_test = df_filtered_1[(df_filtered_1['period'] >= test_start_date) & (df_filtered_1['period'] <= test_end_date)]\n",
    "    # logging.info(f\"Testing data period: {df_test['period'].min()} to {df_test['period'].max()}\")\n",
    "    \n",
    "    # Completed creating the training and testing datasets\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "055cbc61-60ce-4ebf-97c7-98184bdc5ca4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:47.082543Z",
     "iopub.status.busy": "2024-09-13T20:52:47.082291Z",
     "iopub.status.idle": "2024-09-13T20:52:47.088469Z",
     "shell.execute_reply": "2024-09-13T20:52:47.087973Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_TS_model_prophet(df, hyp_dict, p, t_start_date, t_end_date, lag_data, model_url):\n",
    "    \"\"\"\n",
    "    Function to create a time series model using Prophet, including handling for train-test split and missing test data.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame containing at least 'part_number' and 'period' columns.\n",
    "    hyp_dict (dict): Dictionary containing hyperparameters for the Prophet model.\n",
    "    p: Additional parameters required by the Prophet model.\n",
    "    t_start_date (str): The start date for the training period in 'YYYY-MM-DD' format.\n",
    "    t_end_date (str): The end date for the training period in 'YYYY-MM-DD' format.\n",
    "    lag_data: Additional lag data required by the model.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing the Prophet model forecasts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    \n",
    "    # Extracting unique part number from the DataFrame\n",
    "    pn = df['part_number'].unique().tolist()[0]\n",
    "    \n",
    "    # Creating a copy of the input DataFrame\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Splitting the data into training and testing sets\n",
    "    train, test = create_complete_dataset(df_new, t_start_date, t_end_date)\n",
    "    train_clean = train.copy()\n",
    "    \n",
    "    if test.empty:\n",
    "        test = create_test_complete_dataset_from_scratch(test, \n",
    "                                                         train_clean['period'].max() + pd.DateOffset(months=1), \n",
    "                                                         train_clean['period'].max() + pd.DateOffset(months=3),\n",
    "                                                         pn)\n",
    "    \n",
    "    if test.shape[0] < 3:\n",
    "        test = create_test_complete_dataset(test, \n",
    "                                            train_clean['period'].max() + pd.DateOffset(months=1), \n",
    "                                            train_clean['period'].max() + pd.DateOffset(months=3))\n",
    "    \n",
    "    # Extracting date ranges for training and testing sets\n",
    "    data_start_date = train['period'].min()\n",
    "    train_end_date = train['period'].max()\n",
    "    test_start_date = test['period'].min()\n",
    "    data_end_date = test['period'].max()\n",
    "    \n",
    "    # Combining training and testing data for the period data\n",
    "    period_data = pd.concat([train_clean, test], ignore_index=True)\n",
    "    \n",
    "    # Forecasting using the Prophet model\n",
    "    prophet_all_df = prophet_forecast(period_data, train_clean, test, hyp_dict, p, model_url)\n",
    "    \n",
    "    return prophet_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64bae308-b5a7-4d71-9f9f-2d5b6c4cb83f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:47.090871Z",
     "iopub.status.busy": "2024-09-13T20:52:47.090617Z",
     "iopub.status.idle": "2024-09-13T20:52:47.093998Z",
     "shell.execute_reply": "2024-09-13T20:52:47.093506Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_input_type(input_value):\n",
    "    \"\"\"\n",
    "    Function to check the type of the input value and convert it to a string if it is an integer.\n",
    "    \n",
    "    Parameters:\n",
    "    input_value: The value to be checked and possibly converted.\n",
    "    \n",
    "    Returns:\n",
    "    The input value converted to a string if it is an integer, otherwise the original input value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Checking the type of the input value\n",
    "    if isinstance(input_value, int):\n",
    "        # Input value is of type int: {input_value}. Converting to string.\n",
    "        return str(input_value)\n",
    "    else:\n",
    "        return input_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef718260-d0a3-4b89-8789-71d73f1f5e65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:47.096570Z",
     "iopub.status.busy": "2024-09-13T20:52:47.096153Z",
     "iopub.status.idle": "2024-09-13T20:52:47.101680Z",
     "shell.execute_reply": "2024-09-13T20:52:47.101188Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_metrics(parts_df_seg, data_set, train_start_date, train_end_date, lag_val, model_path_url):\n",
    "    \"\"\"\n",
    "    Function to extract metrics using time series modeling (Prophet) for each part number in the input DataFrame segment.\n",
    "    \n",
    "    Parameters:\n",
    "    parts_df_seg (list): List of part numbers to process.\n",
    "    data_set (DataFrame): The input dataset containing at least 'part_number' and 'period' columns.\n",
    "    train_start_date (str): The start date for the training period in 'YYYY-MM-DD' format.\n",
    "    train_end_date (str): The end date for the training period in 'YYYY-MM-DD' format.\n",
    "    lag_val: Additional lag value required for modeling.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing metrics extracted using time series modeling for each part number.\n",
    "    \"\"\"\n",
    "    \n",
    "    final_best_df_data = pd.DataFrame()\n",
    "    index_val = 0\n",
    "    \n",
    "    for parts in parts_df_seg:\n",
    "        try:\n",
    "            \n",
    "#             print(f\"Processing part number: {parts}: {index_val}\")\n",
    "            parts = check_input_type(parts)\n",
    "            # Converted part number to string\n",
    "            \n",
    "            # Filtering data for part number\n",
    "            parts_data = data_set[data_set['part_number'] == parts]\n",
    "    \n",
    "            # Creating time series model (Prophet) for part number\n",
    "            prophet_data = create_TS_model_prophet(parts_data, param_grid, 3, train_start_date, train_end_date, lag_val, model_path_url)\n",
    "            \n",
    "            # Pivoting Prophet model data for part number\n",
    "            pivot_data_prophet = prophet_data.pivot(index='period', columns='model_name', values='fitted')\n",
    "            \n",
    "            # Merging Prophet model data for part number\n",
    "            merged_data_prophet = prophet_data.drop(['fitted', 'model_name'], axis=1).drop_duplicates().merge(pivot_data_prophet, how='left', on='period')\n",
    "            \n",
    "            # Concatenating data for part number\n",
    "            final_best_df_data = pd.concat([final_best_df_data, merged_data_prophet], ignore_index=True)\n",
    "            \n",
    "            index_val += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing part number {parts}: {e}\")\n",
    "    print(\"Completed extracting metrics for all parts\")\n",
    "    return final_best_df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2321062-f427-428e-8db2-e0e61a15405f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:47.104245Z",
     "iopub.status.busy": "2024-09-13T20:52:47.103855Z",
     "iopub.status.idle": "2024-09-13T20:52:47.112139Z",
     "shell.execute_reply": "2024-09-13T20:52:47.111637Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_bestfit_prophet_train(list_parts_all, main_data, start_date_train, end_date_train, lag_val, model_path):\n",
    "    \"\"\"\n",
    "    Function to create the best fit Prophet model for training data and output the results to an Excel file.\n",
    "\n",
    "    Parameters:\n",
    "    list_parts_all (list): List of part numbers to process.\n",
    "    main_data (DataFrame): Main dataset containing all necessary columns including 'part_number' and 'period'.\n",
    "    start_date_train (str): Start date of the training period in 'YYYY-MM-DD' format.\n",
    "    end_date_train (str): End date of the training period in 'YYYY-MM-DD' format.\n",
    "    lag_val (int): Lag value used for modeling.\n",
    "    output_path_folder (str): Folder path where the output Excel file will be saved.\n",
    "    prophet_bestfit_filename (str): file name to store bestfit\n",
    "    all_prophet_fitted_forecast_filename (str): file name to store all prophet_results\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Starting best fit Prophet model creation and evaluation\n",
    "    # Extract metrics for all parts\n",
    "    parts_200_df = extract_metrics(list_parts_all, main_data, start_date_train, end_date_train, lag_val, model_path)\n",
    "    \n",
    "    \n",
    "    # Calculate MAPE for each model and store it in parts_200_df_mape\n",
    "    parts_200_df_mape = parts_200_df.iloc[:,:4]\n",
    "    for cols in parts_200_df.iloc[:,4:].columns:\n",
    "        parts_200_df_mape[cols] = parts_200_df.apply(lambda row: calculate_mape(row['actual'], row[cols]), axis=1)\n",
    "    \n",
    "    # Calculate average MAPE for each model\n",
    "    mape_df = pd.DataFrame()\n",
    "    for model_name in parts_200_df_mape.columns[4:].tolist():\n",
    "        model_mape_df = parts_200_df_mape[parts_200_df_mape['type']=='Test'].groupby(['part_number'])[model_name].mean().reset_index()\n",
    "        model_mape_df = model_mape_df.melt(id_vars='part_number', var_name=model_name, value_vars=model_name)\n",
    "        model_mape_df.columns = ['part_number', 'model', 'value']\n",
    "        mape_df = pd.concat([mape_df, model_mape_df], ignore_index=True)\n",
    "    \n",
    "    # Select the model with the highest MAPE for each part number\n",
    "    df = mape_df.loc[mape_df.groupby('part_number')['value'].idxmax()].reset_index(drop=True)\n",
    "    \n",
    "    # Filter final best fit data for 'Test' type\n",
    "    final_best_df_subset = parts_200_df[parts_200_df['type']=='Test']\n",
    "    \n",
    "    # Prepare final DataFrame with best fit Prophet model results\n",
    "    final_df_best_fit = pd.DataFrame()\n",
    "    for pnt in df['part_number'].unique():\n",
    "        best_model_name = df[df['part_number']==pnt]['model'].values[0]\n",
    "        forecasted_data_subset_pnt = final_best_df_subset.loc[final_best_df_subset['part_number']==pnt, \n",
    "                                                                ['part_number', 'period', \n",
    "                                                                 'type', 'actual', best_model_name]]\n",
    "        forecasted_data_subset_pnt['model_name'] = best_model_name\n",
    "        forecasted_data_subset_pnt.rename(columns={best_model_name:'forecasted_values'}, inplace=True)\n",
    "        final_df_best_fit = pd.concat([final_df_best_fit, forecasted_data_subset_pnt], ignore_index=True)\n",
    "    \n",
    "    # Calculate accuracy prediction based on actual and forecasted values\n",
    "    final_df_best_fit = final_df_best_fit.assign(accuracy_pred = lambda x: np.where(x['actual']!=0,\n",
    "                                                                                    (1-(abs(x['forecasted_values'] - x['actual'])/x['actual']))*100,\n",
    "                                                                                    0))\n",
    "    final_df_best_fit['accuracy_pred'].fillna(0, inplace=True)\n",
    "    \n",
    "    return final_df_best_fit, parts_200_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4385455",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:47.114674Z",
     "iopub.status.busy": "2024-09-13T20:52:47.114133Z",
     "iopub.status.idle": "2024-09-13T20:52:47.141921Z",
     "shell.execute_reply": "2024-09-13T20:52:47.141391Z"
    }
   },
   "outputs": [],
   "source": [
    "train_start_date = (pd.to_datetime(old_data['period']).min()).strftime('%Y-%m-%d')\n",
    "train_end_date = (pd.to_datetime(old_data['period']).max()+relativedelta(months=-3)).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f80a129b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:47.144480Z",
     "iopub.status.busy": "2024-09-13T20:52:47.144221Z",
     "iopub.status.idle": "2024-09-13T20:52:47.147447Z",
     "shell.execute_reply": "2024-09-13T20:52:47.146911Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"prophet\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"cmdstanpy\").disabled=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1feb9aeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:47.149712Z",
     "iopub.status.busy": "2024-09-13T20:52:47.149475Z",
     "iopub.status.idle": "2024-09-13T20:52:47.152736Z",
     "shell.execute_reply": "2024-09-13T20:52:47.152234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-13 20:52:47.150389\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de5c193b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:47.155016Z",
     "iopub.status.busy": "2024-09-13T20:52:47.154777Z",
     "iopub.status.idle": "2024-09-13T20:52:47.158001Z",
     "shell.execute_reply": "2024-09-13T20:52:47.157506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01 2024-03-01\n"
     ]
    }
   ],
   "source": [
    "print(train_start_date, train_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ead48bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:47.160289Z",
     "iopub.status.busy": "2024-09-13T20:52:47.160056Z",
     "iopub.status.idle": "2024-09-13T20:52:47.163107Z",
     "shell.execute_reply": "2024-09-13T20:52:47.162616Z"
    }
   },
   "outputs": [],
   "source": [
    "prefix_output = f\"{config.get_value('primary_path')}/{config.get_value('tpcap_model')}/{config.get_value('model_training_date')}/{config.get_value('model_output_folder_name')}\"\n",
    "\n",
    "prefix_output_prophet_model = f\"{config.get_value('primary_path')}/{config.get_value('tpcap_model')}/{config.get_value('model_training_date')}/{'prophet'}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dd18b22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:47.165646Z",
     "iopub.status.busy": "2024-09-13T20:52:47.165158Z",
     "iopub.status.idle": "2024-09-13T20:52:47.168931Z",
     "shell.execute_reply": "2024-09-13T20:52:47.168430Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'persist-bo/persist-bo-demand-forecast/tpcap-models/20240913/prophet'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_output_prophet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efab6fbe-3d45-4b3c-9e47-6e4bbf69e3e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T20:52:47.171196Z",
     "iopub.status.busy": "2024-09-13T20:52:47.170953Z",
     "iopub.status.idle": "2024-09-14T19:57:42.355445Z",
     "shell.execute_reply": "2024-09-14T19:57:42.354801Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed extracting metrics for all parts\n"
     ]
    }
   ],
   "source": [
    "parts_list_updated = parts_list[:1000]\n",
    "# parts_list_updated = parts_list_updated[:100]    # line to be commented post testing\n",
    "prophet_trained_bestfit_data, full_data_prophet = create_bestfit_prophet_train(parts_list_updated, old_data, train_start_date, train_end_date, 0, prefix_output_prophet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19178cf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T19:57:42.358453Z",
     "iopub.status.busy": "2024-09-14T19:57:42.358153Z",
     "iopub.status.idle": "2024-09-14T19:57:42.361730Z",
     "shell.execute_reply": "2024-09-14T19:57:42.361183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-14 19:57:42.359287\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "762f476f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T19:57:42.364102Z",
     "iopub.status.busy": "2024-09-14T19:57:42.363846Z",
     "iopub.status.idle": "2024-09-14T19:57:42.377216Z",
     "shell.execute_reply": "2024-09-14T19:57:42.376736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>part_number</th>\n",
       "      <th>period</th>\n",
       "      <th>type</th>\n",
       "      <th>actual</th>\n",
       "      <th>forecasted_values</th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04000081B0</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>Test</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.178818</td>\n",
       "      <td>{'changepoint_prior_scale': 0.01, 'seasonality...</td>\n",
       "      <td>89.735221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04000081B0</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>Test</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.894491</td>\n",
       "      <td>{'changepoint_prior_scale': 0.01, 'seasonality...</td>\n",
       "      <td>-94.724546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04000081B0</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>Test</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.135172</td>\n",
       "      <td>{'changepoint_prior_scale': 0.01, 'seasonality...</td>\n",
       "      <td>95.494282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0411106101</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.051309</td>\n",
       "      <td>{'changepoint_prior_scale': 0.01, 'seasonality...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0411106101</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.613132</td>\n",
       "      <td>{'changepoint_prior_scale': 0.01, 'seasonality...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19570</th>\n",
       "      <td>PZ08100005</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>Test</td>\n",
       "      <td>156.0</td>\n",
       "      <td>132.441210</td>\n",
       "      <td>{'changepoint_prior_scale': 0.01, 'seasonality...</td>\n",
       "      <td>84.898212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19571</th>\n",
       "      <td>PZ08100005</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>Test</td>\n",
       "      <td>153.0</td>\n",
       "      <td>152.967751</td>\n",
       "      <td>{'changepoint_prior_scale': 0.01, 'seasonality...</td>\n",
       "      <td>99.978923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19572</th>\n",
       "      <td>PZT0038023</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>Test</td>\n",
       "      <td>320.0</td>\n",
       "      <td>239.090712</td>\n",
       "      <td>{'changepoint_prior_scale': 0.05, 'seasonality...</td>\n",
       "      <td>74.715848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19573</th>\n",
       "      <td>PZT0038023</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>Test</td>\n",
       "      <td>305.0</td>\n",
       "      <td>327.156868</td>\n",
       "      <td>{'changepoint_prior_scale': 0.05, 'seasonality...</td>\n",
       "      <td>92.735453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19574</th>\n",
       "      <td>PZT0038023</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>Test</td>\n",
       "      <td>265.0</td>\n",
       "      <td>264.421596</td>\n",
       "      <td>{'changepoint_prior_scale': 0.05, 'seasonality...</td>\n",
       "      <td>99.781734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19575 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      part_number     period  type  actual  forecasted_values  \\\n",
       "0      04000081B0 2024-04-01  Test     8.0           7.178818   \n",
       "1      04000081B0 2024-05-01  Test     2.0           5.894491   \n",
       "2      04000081B0 2024-06-01  Test     3.0           3.135172   \n",
       "3      0411106101 2024-04-01  Test     0.0          16.051309   \n",
       "4      0411106101 2024-05-01  Test     0.0           5.613132   \n",
       "...           ...        ...   ...     ...                ...   \n",
       "19570  PZ08100005 2024-05-01  Test   156.0         132.441210   \n",
       "19571  PZ08100005 2024-06-01  Test   153.0         152.967751   \n",
       "19572  PZT0038023 2024-04-01  Test   320.0         239.090712   \n",
       "19573  PZT0038023 2024-05-01  Test   305.0         327.156868   \n",
       "19574  PZT0038023 2024-06-01  Test   265.0         264.421596   \n",
       "\n",
       "                                              model_name  accuracy_pred  \n",
       "0      {'changepoint_prior_scale': 0.01, 'seasonality...      89.735221  \n",
       "1      {'changepoint_prior_scale': 0.01, 'seasonality...     -94.724546  \n",
       "2      {'changepoint_prior_scale': 0.01, 'seasonality...      95.494282  \n",
       "3      {'changepoint_prior_scale': 0.01, 'seasonality...       0.000000  \n",
       "4      {'changepoint_prior_scale': 0.01, 'seasonality...       0.000000  \n",
       "...                                                  ...            ...  \n",
       "19570  {'changepoint_prior_scale': 0.01, 'seasonality...      84.898212  \n",
       "19571  {'changepoint_prior_scale': 0.01, 'seasonality...      99.978923  \n",
       "19572  {'changepoint_prior_scale': 0.05, 'seasonality...      74.715848  \n",
       "19573  {'changepoint_prior_scale': 0.05, 'seasonality...      92.735453  \n",
       "19574  {'changepoint_prior_scale': 0.05, 'seasonality...      99.781734  \n",
       "\n",
       "[19575 rows x 7 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prophet_trained_bestfit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6dc8684b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T19:57:42.379555Z",
     "iopub.status.busy": "2024-09-14T19:57:42.379278Z",
     "iopub.status.idle": "2024-09-14T19:57:42.382497Z",
     "shell.execute_reply": "2024-09-14T19:57:42.381981Z"
    }
   },
   "outputs": [],
   "source": [
    "training_date = pd.to_datetime(config.get_value('model_training_date'))\n",
    "file_date = training_date.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a06c5ce3-f812-4ea2-a627-20ed21cf6d19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T19:57:42.384818Z",
     "iopub.status.busy": "2024-09-14T19:57:42.384564Z",
     "iopub.status.idle": "2024-09-14T19:57:46.321623Z",
     "shell.execute_reply": "2024-09-14T19:57:46.321014Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify your S3 bucket and path\n",
    "# prefix_output = f\"{config.get_value('primary_path')}/{config.get_value('tpcap_model')}/{config.get_value('model_training_date')}/{config.get_value('model_output_folder_name')}\"\n",
    "# file_name_prophet_train = f'{config.get_value(\"file_name_prophet_train\")}{datetime.now().strftime(\"%Y-%m-%d\")}.xlsx'\n",
    "file_path_bestfit_prophet = f\"s3://{bucket}/{prefix_output}/{config.get_value('file_name_prophet_train')}{file_date}.xlsx\"\n",
    "\n",
    "# file_name_all_prophet_config_output = f'{config.get_value(\"file_name_all_prophet_config_output\")}{datetime.now().strftime(\"%Y-%m-%d\")}.xlsx'\n",
    "# file_path_all_prophet_config_output = f's3://{bucket}/{prefix_output_prophet_data}/{file_name_all_prophet_config_output}'\n",
    "\n",
    "\n",
    "\n",
    "def generate_data(prophet_bestfit_trained, prophet_all_data):\n",
    "    prophet_bestfit_trained.to_excel(file_path_bestfit_prophet, index=False)\n",
    "#     prophet_all_data.to_excel(file_path_all_prophet_config_output, index=False)\n",
    "\n",
    "generate_data(prophet_trained_bestfit_data, full_data_prophet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd08e8e",
   "metadata": {},
   "source": [
    "### Generate best fit prophet configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f58f5a78-221d-4c68-8838-d79e70a3f359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T19:57:46.324708Z",
     "iopub.status.busy": "2024-09-14T19:57:46.324414Z",
     "iopub.status.idle": "2024-09-14T19:57:46.714546Z",
     "shell.execute_reply": "2024-09-14T19:57:46.713929Z"
    }
   },
   "outputs": [],
   "source": [
    "parts_bestconfig_prophet = prophet_trained_bestfit_data[['part_number', 'model_name']].drop_duplicates().reset_index(drop=True)\n",
    "# prefix_output_prophet_model = f\"{config.get_value('primary_path')}/{config.get_value('tpcap_model')}/{config.get_value('model_training_date')}/{'prophet'}\"\n",
    "filename_bestfit_prophet_config = config.get_value('filename_bestfit_prophet_config')\n",
    "filepath_prophet_config = f\"s3://{bucket}/{prefix_output_prophet_model}/{filename_bestfit_prophet_config}{file_date}.xlsx\"\n",
    "def generate_prophet_config(prophet_config_data, path_to_store):\n",
    "    prophet_config_data.to_excel(path_to_store, index=False)\n",
    "\n",
    "generate_prophet_config(parts_bestconfig_prophet, filepath_prophet_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43e37dac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T19:57:46.717510Z",
     "iopub.status.busy": "2024-09-14T19:57:46.717213Z",
     "iopub.status.idle": "2024-09-14T19:57:46.721054Z",
     "shell.execute_reply": "2024-09-14T19:57:46.720527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken for prophet training:: 1385.3  minutes\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print(\"Total time taken for prophet training::\", (end_time-start_time).seconds/60, \" minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
