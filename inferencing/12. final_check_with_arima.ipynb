{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc05c6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:12.594837Z",
     "iopub.status.busy": "2024-11-05T11:35:12.594594Z",
     "iopub.status.idle": "2024-11-05T11:35:13.635930Z",
     "shell.execute_reply": "2024-11-05T11:35:13.635190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dfcb16e-a5f9-4716-899b-acae67533036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:13.638180Z",
     "iopub.status.busy": "2024-11-05T11:35:13.637840Z",
     "iopub.status.idle": "2024-11-05T11:35:15.047374Z",
     "shell.execute_reply": "2024-11-05T11:35:15.046776Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import boto3\n",
    "import s3fs\n",
    "import tempfile\n",
    "import joblib\n",
    "# import h5py\n",
    "from config_module import Config\n",
    "config = Config()\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Get the current datetime\n",
    "current_datetime = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c27dfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:15.049708Z",
     "iopub.status.busy": "2024-11-05T11:35:15.049305Z",
     "iopub.status.idle": "2024-11-05T11:35:15.301433Z",
     "shell.execute_reply": "2024-11-05T11:35:15.300849Z"
    }
   },
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem()\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa7627d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:15.303664Z",
     "iopub.status.busy": "2024-11-05T11:35:15.303381Z",
     "iopub.status.idle": "2024-11-05T11:35:15.308502Z",
     "shell.execute_reply": "2024-11-05T11:35:15.307974Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    if file_extension == '.xlsx':\n",
    "        # Read Excel file\n",
    "        data = pd.read_excel(file_path)\n",
    "    elif file_extension == '.csv':\n",
    "        # Read CSV file\n",
    "        data = pd.read_csv(file_path)\n",
    "    elif file_extension == '.txt':\n",
    "        # Read TXT file\n",
    "        data = pd.read_csv(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format: {}\".format(file_extension))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def calculate_mape(x,y):\n",
    "    import math\n",
    "    if x==y:\n",
    "        mape=1\n",
    "    elif x==0:\n",
    "        mape=0\n",
    "    elif math.isnan(x):\n",
    "        mape=0\n",
    "    else:\n",
    "        mape=1-(abs(x-y)/x)\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd1f1369",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:15.310222Z",
     "iopub.status.busy": "2024-11-05T11:35:15.309965Z",
     "iopub.status.idle": "2024-11-05T11:35:15.313639Z",
     "shell.execute_reply": "2024-11-05T11:35:15.313131Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_forecast(df_forecast, colname):\n",
    "    df_forecast[colname] = df_forecast[colname].apply(lambda x:int(round(x)))\n",
    "    df_forecast[colname] = df_forecast[colname].apply(lambda x: np.where(x<0,max(x,0),x))\n",
    "    return df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a73cfdbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:15.315323Z",
     "iopub.status.busy": "2024-11-05T11:35:15.315081Z",
     "iopub.status.idle": "2024-11-05T11:35:15.320943Z",
     "shell.execute_reply": "2024-11-05T11:35:15.320438Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_latest_s3_file(bucket_name, path_prefix, file_prefix):\n",
    "    s3 = boto3.client('s3')\n",
    "    # List all objects under the given path_prefix\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=path_prefix)\n",
    "    if 'Contents' not in response:\n",
    "        print(\"No files found.\")\n",
    "        return None\n",
    "    # Filter objects that start with the specific file path_prefix\n",
    "    filtered_files = [\n",
    "        obj for obj in response['Contents'] \n",
    "        if obj['Key'].startswith(f\"{path_prefix}{file_prefix}\")\n",
    "    ]\n",
    "    if not filtered_files:\n",
    "        print(\"No files found with the specific path_prefix.\")\n",
    "        return None\n",
    "    # Extract the date from each file name and store along with the key\n",
    "    files_with_dates = []\n",
    "    for obj in filtered_files:\n",
    "        key = obj['Key']\n",
    "        # Assuming the date is at the end of the filename after the last underscore\n",
    "        try:\n",
    "            # Get the base name without directory path\n",
    "            base_name = os.path.basename(key)\n",
    "            name_without_extension = os.path.splitext(base_name)[0]\n",
    "            date_str = name_without_extension.split('_')[-1]  # Get the date string part\n",
    "            file_date = datetime.strptime(date_str, \"%Y-%m-%d\")  # Parse date\n",
    "            files_with_dates.append((key, file_date))\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing date from file name {key}: {e}\")\n",
    " \n",
    "    if not files_with_dates:\n",
    "        print(\"No valid files with dates found.\")\n",
    "        return None\n",
    " \n",
    "    # Find the file with the latest date\n",
    "    latest_file_key, latest_date = max(files_with_dates, key=lambda x: x[1])\n",
    "    return latest_file_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ba7e20c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:15.322565Z",
     "iopub.status.busy": "2024-11-05T11:35:15.322319Z",
     "iopub.status.idle": "2024-11-05T11:35:33.732314Z",
     "shell.execute_reply": "2024-11-05T11:35:33.731620Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fsspec/registry.py:279: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    }
   ],
   "source": [
    "bucket = config.get_value('bucket')\n",
    "bucket_new = config.get_value('bucket_new')\n",
    "prefix_output_file = config.get_value('prefix_cleaned_training_file_path')\n",
    "file_name_prefix = config.get_value('cleaned_training_filename_prefix')\n",
    "file_path = get_latest_s3_file(bucket,prefix_output_file,file_name_prefix)\n",
    "\n",
    "data_old_lag1 = pd.read_excel('s3://'+f'{bucket}/{file_path}')\n",
    "parts_list = data_old_lag1['part_number'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a1ba0fd-6415-4010-a44b-b7b93ff9cb8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:33.779443Z",
     "iopub.status.busy": "2024-11-05T11:35:33.779164Z",
     "iopub.status.idle": "2024-11-05T11:35:33.839846Z",
     "shell.execute_reply": "2024-11-05T11:35:33.839163Z"
    }
   },
   "outputs": [],
   "source": [
    "data_old_lag1 = data_old_lag1.sort_values(['part_number', 'period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73c200c4-4d8a-475a-baf8-b1a829fc36e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:33.842060Z",
     "iopub.status.busy": "2024-11-05T11:35:33.841733Z",
     "iopub.status.idle": "2024-11-05T11:35:33.848364Z",
     "shell.execute_reply": "2024-11-05T11:35:33.847788Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_mape(x, y):\n",
    "    \"\"\"\n",
    "    Function to calculate the Mean Absolute Percentage Error (MAPE) between two values x and y.\n",
    "    \n",
    "    Parameters:\n",
    "    x: The true value.\n",
    "    y: The predicted value.\n",
    "    \n",
    "    Returns:\n",
    "    The MAPE value as a float.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if x is NaN\n",
    "    if math.isnan(x):\n",
    "        return 0\n",
    "    \n",
    "    # Check if x and y are equal\n",
    "    if x == y:\n",
    "        return 1\n",
    "    \n",
    "    # Check if x is zero\n",
    "    elif x == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    else:\n",
    "        mape = 1 - (abs(x - y) / x)\n",
    "        return mape\n",
    "\n",
    "def find_outliers_iqr_new(data, column_name):\n",
    "    out_data = pd.DataFrame()\n",
    "    for part in data['part_number'].unique().tolist():\n",
    "        df = data[data['part_number']==part]\n",
    "        # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "        Q1 = df[column_name].quantile(0.25)\n",
    "        Q3 = df[column_name].quantile(0.75)\n",
    "        # Calculate the IQR (Interquartile Range)\n",
    "        IQR = Q3 - Q1\n",
    "        # Determine the outlier cutoffs\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        # Identify outliers\n",
    "        df_treated = df.copy()\n",
    "        df_treated['olt_cnt'] = [np.nan]*len(df_treated)\n",
    "        df_treated.loc[df_treated[column_name]>upper_bound,'olt_flg'] = 1\n",
    "        df_treated['olt_cnt'] = df_treated['olt_flg'].groupby(df_treated['part_number']).transform('sum')\n",
    "        out_data = pd.concat([out_data,df_treated],ignore_index=True)\n",
    "    out_data['olt_flg'] = out_data['olt_flg'].fillna(0)    \n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fdec306-5e5e-4227-91b5-a3b7d77c03aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:33.850418Z",
     "iopub.status.busy": "2024-11-05T11:35:33.850168Z",
     "iopub.status.idle": "2024-11-05T11:35:33.875445Z",
     "shell.execute_reply": "2024-11-05T11:35:33.874863Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_threshold(acc, var):\n",
    "        if acc >= 0.8:\n",
    "            return True\n",
    "        elif acc<0.8 and var==1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def replace_sales_missing(row):\n",
    "    if pd.isna(row['Total_qty_per_month']):\n",
    "        if pd.notna(row['last_6_months_avg']):\n",
    "            return row['last_6_months_avg']\n",
    "        elif pd.notna(row['last_3_months_avg']):\n",
    "            return row['last_3_months_avg']\n",
    "        else:\n",
    "            if row['olt_cnt'] > 0:\n",
    "                return row['sales_median']\n",
    "            else:\n",
    "                return row['sales_mean']\n",
    "    else:\n",
    "        return row['Total_qty_per_month']\n",
    "\n",
    "\n",
    "def replace_outliers(row):\n",
    "    if row['olt_flg']>0:\n",
    "        if pd.notna(row['last_6_months_avg']):\n",
    "            return row['last_6_months_avg']\n",
    "        elif pd.notna(row['last_3_months_avg']):\n",
    "            return row['last_3_months_avg']\n",
    "        else:\n",
    "            if row['olt_flg'] > 0:\n",
    "                return row['sales_median']\n",
    "    else:\n",
    "        return row['Total_qty_per_month']\n",
    "\n",
    "\n",
    "def impute_missing_sales(df):\n",
    "    # Sort DataFrame by part_number and date\n",
    "    df.sort_values(['part_number', 'period'], inplace=True)\n",
    " \n",
    "    def rolling_avg(row, window):\n",
    "        part_data = df[(df['part_number'] == row['part_number']) & (df['period'] < row['period'])& (df['olt_flg']==0) &\n",
    "                    pd.notna(df['Total_qty_per_month'])]\n",
    "        if len(part_data) >= window:\n",
    "            return part_data.tail(window)['Total_qty_per_month'].mean()\n",
    "        else:\n",
    "            return None\n",
    " \n",
    "    # Calculate last 6 months average sales considering missing values\n",
    "    df['last_6_months_avg'] = df.apply(lambda row: rolling_avg(row, 6), axis=1)\n",
    "   \n",
    "    # Calculate last 3 months average sales considering missing values\n",
    "    df['last_3_months_avg'] = df.apply(lambda row: rolling_avg(row, 3), axis=1)\n",
    "   \n",
    "    # Calculate mean and median for each part\n",
    "    df_agg = df.groupby('part_number')['Total_qty_per_month'].agg(['median', 'mean']).reset_index()\n",
    "    df_agg.columns = ['part_number', 'sales_median', 'sales_mean']\n",
    " \n",
    "    # Merge aggregated data with the main DataFrame\n",
    "    df = pd.merge(df, df_agg, on='part_number', how='left')\n",
    " \n",
    "    # Apply replacement function\n",
    "    df['Total_qty_per_month'] = df.apply(replace_sales_missing, axis=1)\n",
    "   \n",
    "    return df\n",
    "\n",
    "def impute_outlier_sales(df):\n",
    "    \n",
    "    # Sort DataFrame by part_number and date\n",
    "    df.sort_values(['part_number', 'period'], inplace=True)\n",
    "   \n",
    "    # Initialize consecutive_non_missing count\n",
    "    def rolling_avg(row, window):\n",
    "        part_data = df[(df['part_number'] == row['part_number']) & (df['period'] < row['period']) & (df['olt_flg']==0) &\n",
    "                    pd.notna(df['Total_qty_per_month'])]\n",
    "        if len(part_data) >= window:\n",
    "            return part_data.tail(window)['Total_qty_per_month'].mean()\n",
    "        else:\n",
    "            return None\n",
    "   \n",
    "    # Calculate last 6 months average sales considering missing values\n",
    "    df['last_6_months_avg'] = df.apply(lambda row: rolling_avg(row, 6), axis=1)\n",
    "   \n",
    "    # Calculate last 3 months average sales considering missing values\n",
    "    df['last_3_months_avg'] = df.apply(lambda row: rolling_avg(row, 3), axis=1)\n",
    "   \n",
    "    # Calculate mean and median for each part\n",
    "    df_agg = df.groupby('part_number')['Total_qty_per_month'].agg(['median', 'mean']).reset_index()\n",
    "    df_agg.columns = ['part_number', 'sales_median', 'sales_mean']\n",
    " \n",
    "    # Merge aggregated data with the main DataFrame\n",
    "    df = pd.merge(df, df_agg, on='part_number', how='left')\n",
    " \n",
    "    # Apply replacement function\n",
    "    df['Total_qty_per_month'] = df.apply(replace_outliers, axis=1)\n",
    "   \n",
    "    return df\n",
    "\n",
    "def missing_val_df(df_train,max_tr_date):\n",
    "# Group by part number and calculate min and max dates for each part\n",
    "    min_max_dates = df_train.groupby('part_number')['period'].agg(['min'])\n",
    "    min_max_dates['max'] = pd.to_datetime(max_tr_date)\n",
    "   \n",
    " \n",
    "    # Reindex the DataFrame with all months for each part\n",
    "    dfs = []\n",
    "    for part, (min_date, max_date) in min_max_dates.iterrows():\n",
    "        date_range = pd.date_range(start=min_date, end=max_date, freq='MS')\n",
    "        part_df = pd.DataFrame({'period': date_range})\n",
    "        part_df['part_number'] = part\n",
    "        dfs.append(part_df)\n",
    " \n",
    "    df_all = pd.concat(dfs)\n",
    " \n",
    "    # Merge with original data to retain sales values\n",
    "    df_all = pd.merge(df_all, df_train, on=['part_number', 'period'], how='left')\n",
    " \n",
    "    df_all['olt_cnt'] = df_all['olt_cnt'].fillna(method = 'bfill')\n",
    "    df_all['olt_flg'] = df_all['olt_flg'].fillna(0)\n",
    " \n",
    "    # Replace missing sales values based on outlier count\n",
    "    df_train_missing_treated = impute_missing_sales(df_all)\n",
    "    df_train_missing_treated=df_train_missing_treated.drop(['last_6_months_avg', 'last_3_months_avg', 'sales_median', 'sales_mean'],axis=1)\n",
    "    return df_train_missing_treated\n",
    "\n",
    "# Group by part number and calculate min and max dates for each part\n",
    "def oltlier_df(df_train):\n",
    "    df_train_missing_olt_treated = impute_outlier_sales(df_train)\n",
    "    df_train_missing_olt_treated=df_train_missing_olt_treated.drop(['last_6_months_avg', 'last_3_months_avg', 'sales_median', 'sales_mean'],axis=1)\n",
    "    return df_train_missing_olt_treated\n",
    "\n",
    "def create_test_complete_dataset_from_scratch(df, start_date, end_date, pn):\n",
    "    start_date = start_date.strftime('%Y-%m-%d')\n",
    "    end_date = end_date.strftime('%Y-%m-%d')\n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "     \n",
    "    # Create a DataFrame with all months in the specified period\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
    "    all_months_df = pd.DataFrame(date_range, columns=['period'])\n",
    "    all_months_df['period'] = all_months_df['period'].dt.strftime('%Y-%m')\n",
    "    all_product_df = pd.DataFrame({'part_number':[pn]})\n",
    "    \n",
    "    merged_df = all_months_df.merge(all_product_df, on=None, how='cross')\n",
    "    merged_df['period'] = pd.to_datetime(merged_df['period'])\n",
    "    return merged_df\n",
    "\n",
    "def create_test_complete_dataset(df, start_date, end_date):\n",
    "    # if range_val == 3:\n",
    "    start_date = start_date.strftime('%Y-%m-%d')\n",
    "    end_date = end_date.strftime('%Y-%m-%d')\n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "     \n",
    "    # Create a DataFrame with all months in the specified period\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
    "    all_months_df = pd.DataFrame(date_range, columns=['period'])\n",
    "    all_months_df['period'] = all_months_df['period'].dt.strftime('%Y-%m')\n",
    "    all_product_df = pd.DataFrame({'part_number':df['part_number'].unique()})\n",
    "     \n",
    "    merged_df = all_months_df.merge(all_product_df, on=None, how='cross')\n",
    "    merged_df['period'] = pd.to_datetime(merged_df['period'])\n",
    "    merged_df = merged_df.merge(df, left_on=['period','part_number'],\n",
    "                                right_on=['period','part_number'], how='left')\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "def create_complete_dataset(data_new, train_start_date, train_end_date):\n",
    "    data_new['period'] = pd.to_datetime(data_new['period'])\n",
    "    data_new['period'] = data_new['period'].dt.strftime('%Y-%m-%d')\n",
    "    data_new = data_new.sort_values(['part_number', 'period'])\n",
    "    data_new = data_new[(data_new['period']>=train_start_date)]\n",
    "    data_new['period'] = pd.to_datetime(data_new['period'])\n",
    "\n",
    "    df_filtered_1=data_new.copy()\n",
    "    df_train = df_filtered_1[df_filtered_1['period']<=(train_end_date)]\n",
    "    \n",
    "\n",
    "    test_start_date = pd.to_datetime(pd.to_datetime(train_end_date)+pd.DateOffset(months=1)).strftime('%Y-%m-%d')\n",
    "    test_end_date = pd.to_datetime(pd.to_datetime(train_end_date)+pd.DateOffset(months=3)).strftime('%Y-%m-%d')\n",
    "    df_test = df_filtered_1[(df_filtered_1['period']>=(test_start_date))&(df_filtered_1['period']<=(test_end_date))]\n",
    "    return df_train, df_test\n",
    "\n",
    "def get_consecutive_months(start_date, n_months=3):\n",
    "    \"\"\"\n",
    "    Generate a list of consecutive month start dates.\n",
    "\n",
    "    Args:\n",
    "    - start_date (str): Starting date in 'YYYY-MM-DD' format.\n",
    "    - n_months (int): Number of consecutive months to generate.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of consecutive month start dates in 'YYYY-MM-DD' format.\n",
    "    \"\"\"\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    month_list = [(start_date + pd.DateOffset(months=i)).strftime('%Y-%m-%d') for i in range(1, n_months + 1)]\n",
    "    return month_list\n",
    "\n",
    "def get_latest_s3_file(bucket_name, path_prefix, file_prefix):\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # List all objects under the given path_prefix\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=path_prefix)\n",
    "    \n",
    "    if 'Contents' not in response:\n",
    "        print(\"No files found.\")\n",
    "        return None\n",
    "    \n",
    "    # Filter objects that start with the specific file path_prefix\n",
    "    filtered_files = [\n",
    "        obj for obj in response['Contents'] \n",
    "        if obj['Key'].startswith(f\"{path_prefix}{file_prefix}\")\n",
    "    ]\n",
    "    \n",
    "    if not filtered_files:\n",
    "        print(\"No files found with the specific path_prefix.\")\n",
    "        return None\n",
    "    \n",
    "    # Extract the date from each file name and store along with the key\n",
    "    files_with_dates = []\n",
    "    for obj in filtered_files:\n",
    "        key = obj['Key']\n",
    "        # Assuming the date is at the end of the filename after the last underscore\n",
    "        try:\n",
    "            # Get the base name without directory path\n",
    "            base_name = os.path.basename(key)\n",
    "            name_without_extension = os.path.splitext(base_name)[0]\n",
    "            date_str = name_without_extension.split('_')[-1]  # Get the date string part\n",
    "            file_date = datetime.strptime(date_str, \"%Y-%m-%d\")  # Parse date\n",
    "            files_with_dates.append((key, file_date))\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing date from file name {key}: {e}\")\n",
    "\n",
    "    if not files_with_dates:\n",
    "        print(\"No valid files with dates found.\")\n",
    "        return None\n",
    "\n",
    "    # Find the file with the latest date\n",
    "    latest_file_key, latest_date = max(files_with_dates, key=lambda x: x[1])\n",
    "    \n",
    "    return latest_file_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b7f03",
   "metadata": {},
   "source": [
    "#### Setting up all negative forecasted parts for run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30b60033",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:33.877317Z",
     "iopub.status.busy": "2024-11-05T11:35:33.877059Z",
     "iopub.status.idle": "2024-11-05T11:35:36.556075Z",
     "shell.execute_reply": "2024-11-05T11:35:36.555469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curated-bo/curated-bo-demand-forecast/bestfit-selected-models/bestfit_selected_models_historic_months_powerbi_2024-11-03.xlsx\n"
     ]
    }
   ],
   "source": [
    "prefix_bestfit_trained_data = config.get_value('prefix_bestfit_file')\n",
    "# prefix_bestfit_trained_data = prefix_bestfit_trained_data +'/'\n",
    "bestfit_trained_data_filename = config.get_value(\"file_name_bestfit_powerbi\")\n",
    "latest_bestfit_trained_data_path = get_latest_s3_file(bucket_new, prefix_bestfit_trained_data+\"/\", bestfit_trained_data_filename)\n",
    "print(latest_bestfit_trained_data_path)\n",
    "trained_bestfit_data = pd.read_excel('s3://'+f'{bucket_new}/{latest_bestfit_trained_data_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f575938f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:36.558330Z",
     "iopub.status.busy": "2024-11-05T11:35:36.558036Z",
     "iopub.status.idle": "2024-11-05T11:35:43.407736Z",
     "shell.execute_reply": "2024-11-05T11:35:43.407131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curated-bo/curated-bo-demand-forecast/selected-models-forecasted-output/forecast_selected_models_future_months_powerbi_2024-11-03.xlsx\n"
     ]
    }
   ],
   "source": [
    "prefix_forecasted_data = config.get_value('prefix_forecast_file')\n",
    "bestfit_forecasted_data_filename = config.get_value('filename_forecast_powerbi')\n",
    "latest_bestfit_forecasted_data_path = get_latest_s3_file(bucket_new, prefix_forecasted_data, bestfit_forecasted_data_filename)\n",
    "print(latest_bestfit_forecasted_data_path)\n",
    "forecasted_bestfit_data = pd.read_excel('s3://'+f'{bucket_new}/{latest_bestfit_forecasted_data_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8959fa75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:43.414272Z",
     "iopub.status.busy": "2024-11-05T11:35:43.414013Z",
     "iopub.status.idle": "2024-11-05T11:35:43.422763Z",
     "shell.execute_reply": "2024-11-05T11:35:43.422260Z"
    }
   },
   "outputs": [],
   "source": [
    "total_powerbi_data = pd.concat([trained_bestfit_data, forecasted_bestfit_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "954044d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:43.424340Z",
     "iopub.status.busy": "2024-11-05T11:35:43.424089Z",
     "iopub.status.idle": "2024-11-05T11:35:43.434665Z",
     "shell.execute_reply": "2024-11-05T11:35:43.434166Z"
    }
   },
   "outputs": [],
   "source": [
    "negative_parts = total_powerbi_data[total_powerbi_data['Forecasted']<0]['Part Number'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f98c3496",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:43.436229Z",
     "iopub.status.busy": "2024-11-05T11:35:43.435977Z",
     "iopub.status.idle": "2024-11-05T11:35:43.439758Z",
     "shell.execute_reply": "2024-11-05T11:35:43.439262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce9ac5",
   "metadata": {},
   "source": [
    "### ARIMA Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d16569e5-21e3-44df-9a4b-f3c9cbf47f6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:43.441536Z",
     "iopub.status.busy": "2024-11-05T11:35:43.441288Z",
     "iopub.status.idle": "2024-11-05T11:35:43.459105Z",
     "shell.execute_reply": "2024-11-05T11:35:43.458575Z"
    }
   },
   "outputs": [],
   "source": [
    "def arima_model(data_ts, test_data, p_range, d_range, q_range, pn, train_x, valid_x):\n",
    "    final_arima_df = pd.DataFrame()\n",
    "    internal_arima_dict = pd.DataFrame()\n",
    "    final_df = pd.DataFrame()\n",
    "    model_list = []\n",
    "    for p in range(0, p_range):  # AR parameter\n",
    "        for d in range(0, d_range):  # Differencing parameter\n",
    "            for q in range(0, q_range):  # MA parameter\n",
    "                model = ARIMA(data_ts, order=(p, d, q))\n",
    "                try: \n",
    "                    fitted_model = model.fit()\n",
    "                    forecast_valid = fitted_model.forecast(steps=3)\n",
    "                    forecast_valid = forecast_valid.tolist()\n",
    "                    forecast_valid = list(map(lambda x: round(x, 2), forecast_valid))\n",
    "                    fitted_values = fitted_model.fittedvalues.tolist()\n",
    "                    fitted_values = list(map(lambda x: round(x, 2), fitted_values))\n",
    "                    model_name = f\"ARIMA({p},{d},{q})\"\n",
    "                    final_arima_df_test = pd.DataFrame({\n",
    "                        'part_number': [pn]*test_data.shape[0],\n",
    "                        'period':valid_x.values.flatten(),\n",
    "                        'type':['test']*test_data.shape[0],\n",
    "                        'actual':test_data.values,\n",
    "                        'fitted':forecast_valid,\n",
    "                        'model_name':[model_name]*test_data.shape[0]})\n",
    "                    final_arima_df_train = pd.DataFrame({\n",
    "                        'part_number': [pn]*data_ts.shape[0],\n",
    "                        'period':train_x.values.flatten(),\n",
    "                        'type':['train']*data_ts.shape[0],\n",
    "                        'actual':data_ts.values,\n",
    "                        'fitted':fitted_values,\n",
    "                        'model_name':[model_name]*data_ts.shape[0]})\n",
    "                    final_df = pd.concat([final_df, final_arima_df_train, final_arima_df_test], ignore_index=True)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5da1e0c3-da80-4ed6-b23b-5aaee743660d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:43.461179Z",
     "iopub.status.busy": "2024-11-05T11:35:43.460774Z",
     "iopub.status.idle": "2024-11-05T11:35:43.467181Z",
     "shell.execute_reply": "2024-11-05T11:35:43.466657Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_TS_model_arima(df, p_range, d_range, q_range, t_start_date, t_end_date, lag_val):\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    pn = df['part_number'].unique().tolist()[0]\n",
    "    df_new = df[:]\n",
    "\n",
    "    train, test = create_complete_dataset(df_new, t_start_date, t_end_date)\n",
    "    train_clean = train[:]\n",
    "    if test.empty:\n",
    "        print(pn)\n",
    "        test = create_test_complete_dataset_from_scratch(test, \n",
    "                                                         train_clean['period'].max()+pd.DateOffset(months=1), \n",
    "                                                         train_clean['period'].max()+pd.DateOffset(months=3),\n",
    "                                                         pn)\n",
    "    if (test.shape[0]<3):\n",
    "        test = create_test_complete_dataset(test, \n",
    "                                            train_clean['period'].max()+pd.DateOffset(months=1), \n",
    "                                            train_clean['period'].max()+pd.DateOffset(months=3))\n",
    "    data_start_date = train['period'].min()\n",
    "    train_end_date = train['period'].max()\n",
    "    test_start_date = test['period'].min()\n",
    "    data_end_date = test['period'].max()\n",
    "    df_new['period'] = pd.to_datetime(df_new['period'])\n",
    "    print(\"Total data date::\", data_start_date, train_end_date, test_start_date, data_end_date)\n",
    "    period_data = pd.concat([train_clean, test], ignore_index=True)\n",
    "    arima_all_df = arima_model(train['Total_qty_per_month'][:], test['Total_qty_per_month'][:], p_range, \n",
    "                                               d_range, q_range, pn, train['period'], test['period'])\n",
    "    return arima_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b365c24e-53f3-459c-9962-057f00bb3749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:43.468841Z",
     "iopub.status.busy": "2024-11-05T11:35:43.468610Z",
     "iopub.status.idle": "2024-11-05T11:35:43.471815Z",
     "shell.execute_reply": "2024-11-05T11:35:43.471306Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a549c01-bcfa-4f00-947f-9fb468bd5d14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:43.473459Z",
     "iopub.status.busy": "2024-11-05T11:35:43.473228Z",
     "iopub.status.idle": "2024-11-05T11:35:43.476405Z",
     "shell.execute_reply": "2024-11-05T11:35:43.475893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-05 12:58:05.577676\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba80ecb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:43.478056Z",
     "iopub.status.busy": "2024-11-05T11:35:43.477791Z",
     "iopub.status.idle": "2024-11-05T11:35:43.505307Z",
     "shell.execute_reply": "2024-11-05T11:35:43.504789Z"
    }
   },
   "outputs": [],
   "source": [
    "training_end_date = (pd.to_datetime(data_old_lag1['period']).max() + relativedelta(months=-3)).strftime('%Y-%m-%d')\n",
    "training_start_date = (pd.to_datetime(data_old_lag1['period']).min()).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "711f851d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:43.507204Z",
     "iopub.status.busy": "2024-11-05T11:35:43.506955Z",
     "iopub.status.idle": "2024-11-05T11:35:43.519115Z",
     "shell.execute_reply": "2024-11-05T11:35:43.518602Z"
    }
   },
   "outputs": [],
   "source": [
    "def arima_training(parts_list_updated, train_data, start_date_train, end_date_train):\n",
    "    ind = 0\n",
    "    final_best_df = pd.DataFrame()\n",
    "    # index_val=0\n",
    "    arima_data = pd.DataFrame()\n",
    "    for parts in parts_list_updated:\n",
    "\n",
    "        print(parts, ind)\n",
    "\n",
    "        print('-------------------Start-----------------------')\n",
    "\n",
    "        for lag_val in range(1):\n",
    "            if lag_val==0:\n",
    "                print(1)\n",
    "                parts_data_new = train_data[train_data['part_number']==parts].drop_duplicates()\n",
    "                arima_df = create_TS_model_arima(parts_data_new, 5,2,5, start_date_train, end_date_train, lag_val)\n",
    "                arima_data = pd.concat([arima_data, arima_df], ignore_index=True)\n",
    "                pivot_df = arima_df.pivot(index='period', \n",
    "                                          columns='model_name', \n",
    "                                          values='fitted')\n",
    "                merged_df_arima = arima_df.drop(['fitted', \n",
    "                                                 'model_name'], axis=1).drop_duplicates().merge(pivot_df, \n",
    "                                                                                                    how='left', \n",
    "                                                                                                    on='period')\n",
    "                final_best_df = pd.concat([final_best_df, merged_df_arima], ignore_index=True)\n",
    "        ind+=1\n",
    "        print('-------------------End-----------------------')\n",
    "    \n",
    "#     Training concludes\n",
    "    \n",
    "    arima_data = arima_data[arima_data['model_name']!='ARIMA(0,0,0)'].reset_index(drop=True)\n",
    "    final_best_df.drop('ARIMA(0,0,0)', axis=1, inplace=True)\n",
    "    \n",
    "#   Set validation set timeline\n",
    "    validation_test_timeline = get_consecutive_months(end_date_train)\n",
    "    final_best_df = final_best_df[final_best_df['period'].isin(validation_test_timeline)]\n",
    "    \n",
    "    final_best_df_mape = final_best_df.iloc[:,:4]\n",
    "    for cols in final_best_df.iloc[:,4:].columns:\n",
    "        final_best_df_mape[cols] = final_best_df.apply(lambda row: calculate_mape(row['actual'], row[cols]), axis=1)\n",
    "#   Check for bestfit config for each parts in ARIMA\n",
    "    all_forecasted_data_mape_df = pd.DataFrame()\n",
    "    for model_name in final_best_df_mape.columns[4:].tolist():\n",
    "        print(model_name)\n",
    "        model_mape_df = final_best_df_mape.groupby(['part_number'])[model_name].mean().reset_index()\n",
    "        model_mape_df = model_mape_df.melt(id_vars='part_number', \n",
    "                                           var_name=model_name, \n",
    "                                           value_vars=model_name)\n",
    "        model_mape_df.columns = ['part_number', 'model', 'value']\n",
    "        all_forecasted_data_mape_df = pd.concat([all_forecasted_data_mape_df, model_mape_df], ignore_index=True)\n",
    "    \n",
    "    all_forecasted_data_mape_df_max = all_forecasted_data_mape_df.loc[all_forecasted_data_mape_df.groupby('part_number')['value'].idxmax()].reset_index(drop=True)\n",
    "    \n",
    "    final_df_best_fit_arima = pd.DataFrame()\n",
    "    for pnt in all_forecasted_data_mape_df_max['part_number'].unique():\n",
    "        best_model_name = all_forecasted_data_mape_df_max[all_forecasted_data_mape_df_max['part_number']==pnt]['model'].values[0]\n",
    "        forecasted_data_subset_pnt = final_best_df.loc[final_best_df['part_number']==pnt, \n",
    "                                                       ['part_number', 'period', \n",
    "                                                        'actual', best_model_name]]\n",
    "        forecasted_data_subset_pnt['model_name'] = best_model_name\n",
    "        forecasted_data_subset_pnt.rename(columns={best_model_name:'forecasted_values'}, inplace=True)\n",
    "        final_df_best_fit_arima = pd.concat([final_df_best_fit_arima, forecasted_data_subset_pnt], ignore_index=True)\n",
    "    \n",
    "    final_df_best_fit_arima = final_df_best_fit_arima.assign(accuracy_pred = \n",
    "                                                             lambda x: np.where(x['actual']!=0,\n",
    "                                                                                (1-(abs(x['forecasted_values'] - x['actual'])/x['actual'])), 0))\n",
    "    final_df_best_fit_arima['period'] = pd.to_datetime(final_df_best_fit_arima['period']).astype(str)\n",
    "#   Extract bestfit ARIMA config for each parts - to be used during forecasting\n",
    "    bestfit_arima_models_config = final_df_best_fit_arima[['part_number', 'model_name']].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    final_df_best_fit_arima['model_name'] = \"ARIMA\"\n",
    "    final_df_best_fit_arima = final_df_best_fit_arima.rename(columns={'part_number':'Part Number',\n",
    "                                                                            'model_name': 'Model Name',\n",
    "                                                                            'actual': 'Actual',\n",
    "                                                                            'forecasted_values': 'Forecasted',\n",
    "                                                                            'period': 'Date',\n",
    "                                                                            'accuracy_pred': 'New Model Accuracy'                    \n",
    "                                                                           })\n",
    "    final_df_best_fit_arima.loc[final_df_best_fit_arima['Forecasted']<0, 'Forecasted'] = 0\n",
    "    final_df_best_fit_arima.loc[final_df_best_fit_arima['Forecasted']<0, 'New Model Accuracy'] = 0\n",
    "    \n",
    "    return final_df_best_fit_arima, bestfit_arima_models_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4684dd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:43.526698Z",
     "iopub.status.busy": "2024-11-05T11:35:43.526442Z",
     "iopub.status.idle": "2024-11-05T11:49:31.786357Z",
     "shell.execute_reply": "2024-11-05T11:49:31.785751Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2330062030 0\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-07-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "47459BZ050 1\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-02-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "532860D380 2\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "7461002260B0 3\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "82210BZ020 4\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2022-05-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "90109T0074 5\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2022-11-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "9014860022 6\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-04-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "9030199090 7\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-07-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "044650K450 8\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "04465YZZQ5 9\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "091110K082 10\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2022-09-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "11701BZ09103 11\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "121010Y080 12\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "1320109673 13\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "162060L010 14\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "164000T460 15\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "16711BZ180 16\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-07-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "171180Y031 17\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-02-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "174030L081 18\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "177500V051 19\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "220300Y020 20\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "288000M061 21\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "312500K170 22\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "312500K231 23\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "330340K100 24\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "333480K010 25\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "351030D040 26\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "411150K020 27\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "413360K060 28\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "42410BZ180 29\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2022-07-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "426020K070 30\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "4261126220 31\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-07-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "42638BZ080 32\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "452030K010 33\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "455170D040 34\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "464100K020 35\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "464200K011 36\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "464300D130 37\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "464300K010 38\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "477810K160 39\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------End-----------------------\n",
      "477820K050 40\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "4806909071 41\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "482100K231 42\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "4851009Y40 43\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "485108Z175 44\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "4852009440 45\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "4852009R50 46\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "485208Z059 47\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "485300DC90 48\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "4853109460 49\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "4865AKK010 50\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "517020K160 51\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "517710K400 52\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "52119BZD11 53\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "521260K240 54\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "52159BZJ00 55\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "525360D290 56\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "526110D070 57\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "526110D080 58\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "526110D181 59\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2022-09-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "5310002650 60\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "531010D040 61\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "531010D500 62\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "53112YP080 63\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "53121BZ240B0 64\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "53201BZ340 65\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "532880D290 66\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "534510K050 67\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "535100A050 68\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-06-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "53510BZ220 69\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "538050K221 70\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "53812BZ380 71\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "538230A020 72\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-10-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "55041KK070C3 73\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "55410KK020C0 74\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "554120K150C0 75\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "554460K060C0 76\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "56117BZ130 77\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "583170K011 78\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "58841BZ170C1 79\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-11-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------End-----------------------\n",
      "61611BZ470 80\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2022-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "623320A010 81\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "67003BZ640 82\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "681050K430 83\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "68162YE010 84\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "681710D070TH 85\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "681880K110 86\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "6900502K20 87\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "690400K070 88\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "692050D070B3 89\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "697300K170 90\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "69801BZ110 91\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "715020K031 92\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "7432089134B04 93\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "7459802040 94\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "75392BZ020 95\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "754710D270 96\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "755550A010 97\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "75642BZ020 98\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "757560D070 99\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "75816DX010 100\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "7589702020 101\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "76082YP010 102\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "76083YP010 103\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "76625BZ050 104\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "76626BZ050 105\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "769250K010C0 106\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "811100D841 107\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "811100KB10 108\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "811300K350 109\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "811300K390 110\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "811500B100 111\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "811500KB91 112\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "81150BZ400 113\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "811700D791 114\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "811700K390 115\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "815100K010 116\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "8155002790 117\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "82121KK523 118\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "840400K021 119\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------End-----------------------\n",
      "842500K110C1 120\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "846520D081 121\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "871070K010 122\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "8713902100 123\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "8713902140 124\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "879100K561 125\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "879100K911 126\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "87910BZF60 127\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "87917BZ150 128\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-02-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "879310KA00 129\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "879400KA41 130\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "879400KB81 131\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "8831002A02 132\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "885010K160 133\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "887230K280 134\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "8934133220C4 135\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-07-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "9004A81023 136\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "9007560026 137\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "9008091228 138\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "90105T0023 139\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "90105T0130 140\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "90105T0335 141\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "9011909004 142\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-07-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "9016660005 143\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-07-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "90387T0010 144\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "9046709191 145\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "9046709204 146\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "90467T0041 147\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "90520T0164 148\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "9451200800 149\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "P41880K002 150\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n",
      "-------------------End-----------------------\n",
      "PC1700K01T 151\n",
      "-------------------Start-----------------------\n",
      "1\n",
      "Total data date:: 2021-01-01 00:00:00 2024-03-01 00:00:00 2024-04-01 00:00:00 2024-06-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "if negative_parts != []:\n",
    "    arima_training_data, bestfit_config_arima = arima_training(negative_parts, data_old_lag1, \n",
    "                                                               training_start_date, training_end_date)\n",
    "    prefix_arima_train_data_path = f\"{config.get_value('primary_path')}/{config.get_value('tpcap_model')}/{config.get_value('model_training_date')}/{config.get_value('model_output_folder_name')}\"\n",
    "    prefix_arima_pickle = f\"{config.get_value('primary_path')}/{config.get_value('tpcap_model')}/{config.get_value('model_training_date')}/{'arima'}\"\n",
    "    arima_train_data_file_name = config.get_value('arima_train_data_file_name')\n",
    "    arima_bestfit_config_file_name = config.get_value('arima_bestfit_config_file_name')\n",
    "    arima_training_data.to_excel(f\"s3://{bucket}/{prefix_arima_train_data_path}/{arima_train_data_file_name}{pd.to_datetime(config.get_value('model_training_date')).strftime('%Y-%m-%d')}.xlsx\", index=False)\n",
    "    bestfit_config_arima.to_excel(f\"s3://{bucket}/{prefix_arima_pickle}/{arima_bestfit_config_file_name}{pd.to_datetime(config.get_value('model_training_date')).strftime('%Y-%m-%d')}.xlsx\", index=False)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a4f2b6",
   "metadata": {},
   "source": [
    "#### ARIMA Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01d325c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:49:31.788562Z",
     "iopub.status.busy": "2024-11-05T11:49:31.788282Z",
     "iopub.status.idle": "2024-11-05T11:49:31.795350Z",
     "shell.execute_reply": "2024-11-05T11:49:31.794800Z"
    }
   },
   "outputs": [],
   "source": [
    "def arima_model_forecast(data_ts, test_data, p_range, d_range, q_range, pn, train_x, valid_x):\n",
    "    final_arima_df = pd.DataFrame()\n",
    "    internal_arima_dict = pd.DataFrame()\n",
    "    final_df = pd.DataFrame()\n",
    "    model_list = []\n",
    "    model = ARIMA(data_ts, order=(p_range, d_range, q_range))\n",
    "    try: \n",
    "        fitted_model = model.fit()\n",
    "        forecast_valid = fitted_model.forecast(steps=3)\n",
    "        forecast_valid = forecast_valid.tolist()\n",
    "        forecast_valid = list(map(lambda x: round(x, 2), forecast_valid))\n",
    "        fitted_values = fitted_model.fittedvalues.tolist()\n",
    "        fitted_values = list(map(lambda x: round(x, 2), fitted_values))\n",
    "        model_name = f\"ARIMA({p_range},{d_range},{q_range})\"\n",
    "        final_arima_df_test = pd.DataFrame({\n",
    "            'part_number': [pn]*test_data.shape[0],\n",
    "            'period':valid_x.values.flatten(),\n",
    "            'type':['test']*test_data.shape[0],\n",
    "            'actual':np.nan,\n",
    "            'fitted':forecast_valid,\n",
    "            'model_name':[model_name]*test_data.shape[0]})\n",
    "        final_arima_df_train = pd.DataFrame({\n",
    "            'part_number': [pn]*data_ts.shape[0],\n",
    "            'period':train_x.values.flatten(),\n",
    "            'type':['train']*data_ts.shape[0],\n",
    "            'actual':data_ts.values,\n",
    "            'fitted':fitted_values,\n",
    "            'model_name':[model_name]*data_ts.shape[0]})\n",
    "        final_df = pd.concat([final_df, final_arima_df_train, final_arima_df_test], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(pn, \" :: \", e)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e8df07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:49:31.797322Z",
     "iopub.status.busy": "2024-11-05T11:49:31.796857Z",
     "iopub.status.idle": "2024-11-05T11:49:31.803399Z",
     "shell.execute_reply": "2024-11-05T11:49:31.802847Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_TS_model_arima_forecast(df, p_range, d_range, q_range, t_start_date, t_end_date, lag_val):\n",
    "    # Split data into training and testing sets\n",
    "    pn = df['part_number'].unique().tolist()[0]\n",
    "    df_new = df[:]\n",
    "\n",
    "    train, test = create_complete_dataset(df_new, t_start_date, t_end_date)\n",
    "    train_clean = train[:]\n",
    "    if test.empty:\n",
    "        print(pn)\n",
    "        test = create_test_complete_dataset_from_scratch(test, \n",
    "                                                         train_clean['period'].max()+pd.DateOffset(months=1), \n",
    "                                                         train_clean['period'].max()+pd.DateOffset(months=3),\n",
    "                                                         pn)\n",
    "    if (test.shape[0]<3):\n",
    "        test = create_test_complete_dataset(test, \n",
    "                                            train_clean['period'].max()+pd.DateOffset(months=1), \n",
    "                                            train_clean['period'].max()+pd.DateOffset(months=3))\n",
    "    data_start_date = train['period'].min()\n",
    "    train_end_date = train['period'].max()\n",
    "    test_start_date = test['period'].min()\n",
    "    data_end_date = test['period'].max()\n",
    "    df_new['period'] = pd.to_datetime(df_new['period'])\n",
    "    print(\"Total data date::\", data_start_date, train_end_date, test_start_date, data_end_date)\n",
    "    period_data = pd.concat([train_clean, test], ignore_index=True)\n",
    "    print(train.shape, test.shape)\n",
    "    arima_all_df = arima_model_forecast(train['Total_qty_per_month'][:], test[:], p_range, \n",
    "                                               d_range, q_range, pn, train['period'], test['period'])\n",
    "    return arima_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8774b441",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:49:31.805119Z",
     "iopub.status.busy": "2024-11-05T11:49:31.804880Z",
     "iopub.status.idle": "2024-11-05T11:49:31.811219Z",
     "shell.execute_reply": "2024-11-05T11:49:31.810686Z"
    }
   },
   "outputs": [],
   "source": [
    "def arima_forecast(part_list_neg, parts_config_df, clean_train_data, start_date_forecast, end_date_forecast):\n",
    "    ind = 0\n",
    "    arima_data_forecast = pd.DataFrame()\n",
    "\n",
    "    parts_list_best_fit = part_list_neg[:]\n",
    "    for parts in parts_list_best_fit:\n",
    "        mod_name = parts_config_df[parts_config_df['part_number']==parts]['model_name'].values[0]\n",
    "        p1,d1,q1 = list(map(lambda x: int(x),mod_name[6:-1].split(',')))\n",
    "        print('-------------------Start-----------------------')\n",
    "        for lag_val in range(1):\n",
    "            if lag_val==0:\n",
    "                parts_data_new = clean_train_data[clean_train_data['part_number']==parts].drop_duplicates()\n",
    "                arima_df = create_TS_model_arima_forecast(parts_data_new, p1,d1,q1, start_date_forecast, end_date_forecast, lag_val)\n",
    "                arima_data_forecast = pd.concat([arima_data_forecast, arima_df], ignore_index=True)\n",
    "        ind+=1\n",
    "        print('-------------------End-----------------------')\n",
    "    arima_data_forecast = arima_data_forecast[arima_data_forecast['type']=='test'].reset_index(drop=True)\n",
    "    arima_data_forecast['period'] = pd.to_datetime(arima_data_forecast['period']).astype(str)\n",
    "    \n",
    "    arima_data_forecast['model_name'] = \"ARIMA\"\n",
    "    arima_data_forecast = arima_data_forecast.rename(columns={'part_number':'Part Number',\n",
    "                                                                            'model_name': 'Model Name',\n",
    "                                                                            'actual': 'Actual',\n",
    "                                                                            'fitted': 'Forecasted',\n",
    "                                                                            'period': 'Date'\n",
    "                                                                           }).drop(['type'], axis=1)\n",
    "    \n",
    "    return arima_data_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f1c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_end_date = (pd.to_datetime(data_old_lag1['period']).max().strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68af1a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:49:31.812945Z",
     "iopub.status.busy": "2024-11-05T11:49:31.812695Z",
     "iopub.status.idle": "2024-11-05T11:49:53.449383Z",
     "shell.execute_reply": "2024-11-05T11:49:53.448788Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if negative_parts != []:\n",
    "    forecast_end_date = (pd.to_datetime(data_old_lag1['period']).max().strftime('%Y-%m-%d'))\n",
    "    final_forecasted_arima_negative_data = arima_forecast(negative_parts, bestfit_config_arima, data_old_lag1, \n",
    "                                                          training_start_date, forecast_end_date)\n",
    "    prefix_arima_forecasted_data_path = config.get_value('prefix_model_output_forecast_path')\n",
    "    arima_forecasted_file_name = config.get_value('arima_forecasted_file_name')\n",
    "    final_forecasted_arima_negative_data.to_excel(f\"s3://{bucket}/{prefix_arima_forecasted_data_path}/{config.get_value('model_forecasting_date')}/{arima_forecasted_file_name}{pd.to_datetime(config.get_value('model_forecasting_date')).strftime('%Y-%m-%d')}.xlsx\", index=False)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f682d",
   "metadata": {},
   "source": [
    "### ARIMA process concluded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5f786",
   "metadata": {},
   "source": [
    "### PowerBI data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb65a9ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:49:53.451691Z",
     "iopub.status.busy": "2024-11-05T11:49:53.451405Z",
     "iopub.status.idle": "2024-11-05T11:49:53.455335Z",
     "shell.execute_reply": "2024-11-05T11:49:53.454781Z"
    }
   },
   "outputs": [],
   "source": [
    "bucket_curated = config.get_value('bucket_new')\n",
    "prefix_gtmaster = config.get_value('prefix_gtmaster')\n",
    "prefix_proc = config.get_value('prefix_proc')\n",
    "prefix_pic_master_path = config.get_value(\"prefix_pic_master_path\")\n",
    "product_master_path_prefix = config.get_value('product_master_path_prefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed6b835",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:49:53.457220Z",
     "iopub.status.busy": "2024-11-05T11:49:53.456973Z",
     "iopub.status.idle": "2024-11-05T11:49:53.468117Z",
     "shell.execute_reply": "2024-11-05T11:49:53.467578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to read a single Parquet file\n",
    "def read_parquet_file(file):\n",
    "    return pd.read_parquet(f's3://{file}', engine='pyarrow')\n",
    "\n",
    "def gt_parquet_read(bucket_curated_name, gt_master_parquet):\n",
    "    # Recursively list all Parquet files in the nested S3 path\n",
    "#     parquet_files_gt_partm = fs.glob(f's3://{bucket_curated_name}/{gt_master_parquet}/*/*/*/*.parquet')\n",
    "    parquet_files_gt_partm = fs.glob(f's3://{bucket_curated_name}/{gt_master_parquet}*.parquet')\n",
    "\n",
    "    df_master_gt_part_master = pd.DataFrame()\n",
    "    for file in parquet_files_gt_partm:\n",
    "        df_gt = pd.read_parquet(f's3://{file}', engine='pyarrow')\n",
    "        df_master_gt_part_master = pd.concat([df_master_gt_part_master,df_gt],ignore_index=True)\n",
    "    df_master_gt_part_master_subset = df_master_gt_part_master.sort_values('sales_start_dt', ascending=False)\n",
    "    df_master_gt_part_master_subset = df_master_gt_part_master_subset.drop_duplicates(subset=['part_no'], keep='first')\n",
    "    df_master_gt_part_master_subset = df_master_gt_part_master_subset[[\"part_no\", \"country_origin_cd\"]].drop_duplicates()\n",
    "    df_master_gt_part_master_subset = df_master_gt_part_master_subset.assign(IMPORT_CD = \n",
    "                                                                             lambda x: np.where(x['country_origin_cd']=='TH', \n",
    "                                                                                                'LSP', \n",
    "                                                                                                np.where(x['country_origin_cd']=='JP', \n",
    "                                                                                                         'JSP', 'MSP')))\n",
    "    df_master_gt_part_master_subset = df_master_gt_part_master_subset.drop('country_origin_cd', axis=1)\n",
    "    df_master_gt_part_master_subset.columns = [col.upper() for col in df_master_gt_part_master_subset.columns]\n",
    "    return df_master_gt_part_master_subset\n",
    "\n",
    "def proc_parquet_read(bucket_curated_name, proc_master_parquet):\n",
    "    # Recursively list all Parquet files in the nested S3 path\n",
    "#     parquet_files_proc = fs.glob(f's3://{bucket_curated_name}/{proc_master_parquet}/*/*/*/*.parquet')\n",
    "    parquet_files_proc = fs.glob(f's3://{bucket_curated_name}/{proc_master_parquet}*.parquet')\n",
    "\n",
    "    df_master_proc_master = pd.DataFrame()\n",
    "\n",
    "    for file in parquet_files_proc:\n",
    "    #     print(file)\n",
    "        df_proc = pd.read_parquet(f's3://{file}', engine='pyarrow')\n",
    "        df_master_proc_master = pd.concat([df_master_proc_master,df_proc],ignore_index=True)\n",
    "    df_master_proc_master = df_master_proc_master.sort_values(['start_dt'], ascending=[False])\n",
    "    df_master_proc_master = df_master_proc_master.drop_duplicates(subset='part_no', keep='first')\n",
    "    df_master_proc_master_subset = df_master_proc_master[['part_no', 'procurement_div_cd', 'procurement_operator_id', 'inventory_ctrl_class']].drop_duplicates()\n",
    "    df_master_proc_master_subset['procurement_div_cd'] = df_master_proc_master_subset['procurement_div_cd'].apply(lambda x:x[-3:])\n",
    "    df_master_proc_master_subset = df_master_proc_master_subset.rename(columns={'procurement_div_cd':'Vendor Code',\n",
    "                                                                                'part_no':'PART_NO',\n",
    "                                                                                'procurement_operator_id':'PROCUREMENT_OPERATOR_ID',\n",
    "                                                                                'inventory_ctrl_class': 'ICC_seg'\n",
    "                                                                       })\n",
    "    df_master_proc_master_subset['ICC_seg'] = df_master_proc_master_subset['ICC_seg'].apply(lambda x: x[0] if pd.notna(x) and len(x) > 1 else x)\n",
    "    return df_master_proc_master_subset\n",
    "\n",
    "def pic_parquet_read(bucket_curated_name, pic_master_parquet):\n",
    "    # Recursively list all Parquet files in the nested S3 path\n",
    "#     parquet_files_pic_master = fs.glob(f's3://{bucket_curated_name}/{pic_master_parquet}/*/*/*/*.parquet')\n",
    "    parquet_files_pic_master = fs.glob(f's3://{bucket_curated_name}/{pic_master_parquet}*.parquet')\n",
    "\n",
    "\n",
    "    df_master_pic = pd.DataFrame()\n",
    "\n",
    "    for file in parquet_files_pic_master:\n",
    "    #     print(file)\n",
    "        df_pic = pd.read_parquet(f's3://{file}', engine='pyarrow')\n",
    "        df_master_pic = pd.concat([df_master_pic,df_pic],ignore_index=True)\n",
    "    \n",
    "    df_master_pic_subset = df_master_pic[['id', 'pic', 'pic_abb']].drop_duplicates().reset_index(drop=True)\n",
    "    df_master_pic_subset.columns = [col.upper() for col in df_master_pic_subset.columns]\n",
    "\n",
    "    return df_master_pic_subset\n",
    "\n",
    "def prod_master_read(bucket_curated_name, pg_master_path):\n",
    "    pg_master_filepath = \"s3://\"+fs.glob(f's3://{bucket_curated_name}/{pg_master_path}/*.csv')[0]\n",
    "    pg_master_df = read_file(pg_master_filepath)\n",
    "    pg_master_df = pg_master_df[['PART_NO', 'PRODUCT_GROUP']].drop_duplicates()\n",
    "    return pg_master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4694e11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:49:53.469781Z",
     "iopub.status.busy": "2024-11-05T11:49:53.469539Z",
     "iopub.status.idle": "2024-11-05T11:49:53.476815Z",
     "shell.execute_reply": "2024-11-05T11:49:53.476272Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_lastest_partition_path(bucket_name, prefix):\n",
    "    if prefix[-1] != \"/\": prefix += \"/\"\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    year_month_day = {}\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "            for obj in page.get('Contents', []):\n",
    "                key = obj['Key']\n",
    "                key = key[len(prefix):]\n",
    "                parts = key.split('/')\n",
    "#                 print(parts)\n",
    "                if len(parts) >= 3:\n",
    "                    if \"=\" in parts[0] and \"=\" in parts[1]:\n",
    "                        year = parts[0].split('=')[-1]\n",
    "                        month = parts[1].split('=')[-1]\n",
    "                        if \"=\" in parts[2]:\n",
    "                            day = parts[2].split('=')[-1]\n",
    "                        else: day = \"\"\n",
    "                            \n",
    "                        if len(day) > 2: continue\n",
    "\n",
    "                        if year not in year_month_day:\n",
    "                            year_month_day[year] = {}\n",
    "                        if month not in year_month_day[year]:\n",
    "                            year_month_day[year][month] = set()\n",
    "                        year_month_day[year][month].add(day)\n",
    "\n",
    "#     print(year_month_day.keys())\n",
    "    \n",
    "    most_recent_year = max(year_month_day.keys(), key=int)\n",
    "    most_recent_month = max(year_month_day[most_recent_year].keys(), key=int)\n",
    "    if year_month_day[most_recent_year][most_recent_month] != {\"\"}:\n",
    "        most_recent_day = max(year_month_day[most_recent_year][most_recent_month], key=int)\n",
    "        final_path = f\"{prefix}year={most_recent_year}/month={most_recent_month}/day={most_recent_day}/\"\n",
    "    else:\n",
    "        final_path = f\"{prefix}year={most_recent_year}/month={most_recent_month}/\"\n",
    "        \n",
    "    return final_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13962e55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:49:53.478597Z",
     "iopub.status.busy": "2024-11-05T11:49:53.478336Z",
     "iopub.status.idle": "2024-11-05T11:49:53.483782Z",
     "shell.execute_reply": "2024-11-05T11:49:53.483213Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_final_master_file(bucket_cur, gt_parquet_path, proc_parquet_path, pic_parquet_path, prod_path):\n",
    "    prefix_gtmaster = get_lastest_partition_path(bucket_cur, gt_parquet_path)\n",
    "    gt_data = gt_parquet_read(bucket_cur, prefix_gtmaster)\n",
    "    prefix_procmaster = get_lastest_partition_path(bucket_cur, proc_parquet_path)\n",
    "    proc_data = proc_parquet_read(bucket_cur, prefix_procmaster)\n",
    "    prefix_picmaster = get_lastest_partition_path(bucket_cur, pic_parquet_path)\n",
    "    pic_data = pic_parquet_read(bucket_cur, prefix_picmaster)\n",
    "    \n",
    "    pg_data = prod_master_read(bucket_cur, prod_path)\n",
    "    \n",
    "    pic_data['ID'] = pic_data['ID'].astype(str)\n",
    "    gt_data['PART_NO'] = gt_data['PART_NO'].astype(str)\n",
    "    proc_data['PART_NO'] = proc_data['PART_NO'].astype(str)\n",
    "    pg_data['PART_NO'] = pg_data['PART_NO'].astype(str)\n",
    "    \n",
    "    final_master_file_data = pd.merge(pd.merge(pd.merge(gt_data, proc_data, on='PART_NO', how='left'), \n",
    "                                 pic_data, left_on='PROCUREMENT_OPERATOR_ID', \n",
    "                                 right_on='ID', how='left').drop('ID', axis=1), \n",
    "                        pg_data, on='PART_NO', how='left')\n",
    "    \n",
    "    return final_master_file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3424b6c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:49:53.485458Z",
     "iopub.status.busy": "2024-11-05T11:49:53.485216Z",
     "iopub.status.idle": "2024-11-05T11:50:09.932655Z",
     "shell.execute_reply": "2024-11-05T11:50:09.932035Z"
    }
   },
   "outputs": [],
   "source": [
    "full_master_details = create_final_master_file(bucket_curated, prefix_gtmaster, prefix_proc, prefix_pic_master_path, product_master_path_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084b557",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:09.934858Z",
     "iopub.status.busy": "2024-11-05T11:50:09.934555Z",
     "iopub.status.idle": "2024-11-05T11:50:10.247523Z",
     "shell.execute_reply": "2024-11-05T11:50:10.246870Z"
    }
   },
   "outputs": [],
   "source": [
    "full_master_details.shape, full_master_details['PART_NO'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0e06e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:10.283572Z",
     "iopub.status.busy": "2024-11-05T11:50:10.283299Z",
     "iopub.status.idle": "2024-11-05T11:50:10.288966Z",
     "shell.execute_reply": "2024-11-05T11:50:10.288399Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_powerbi_data(arima_train_data, arima_forecasted_data, powerbi_data, parts_list_neg):\n",
    "    powerbi_data_nonneg = powerbi_data[~powerbi_data['Part Number'].isin(parts_list_neg)]\n",
    "    powerbi_data_neg = powerbi_data[powerbi_data['Part Number'].isin(parts_list_neg)]\n",
    "    \n",
    "    full_arima_data = pd.concat([arima_train_data, arima_forecasted_data], ignore_index=True)\n",
    "    \n",
    "    full_arima_data = normalize_forecast(full_arima_data, 'Forecasted')\n",
    "    \n",
    "    col_to_attach = powerbi_data_neg.drop(['Actual', 'Forecasted', 'Model Name','New Model Accuracy', 'Threshold'], axis=1).columns\n",
    "    \n",
    "    new_arima_powerbi_data = pd.merge(powerbi_data_neg[col_to_attach], \n",
    "                                      full_arima_data, on=['Part Number', 'Date'], how='left')\n",
    "    \n",
    "    new_total_powerbi = pd.concat([powerbi_data_nonneg, \n",
    "                                   new_arima_powerbi_data[powerbi_data_nonneg.columns]], ignore_index=True)\n",
    "    new_total_powerbi['Actual'] = new_total_powerbi['Actual'].apply(lambda x: int(x) if pd.notna(x) else x)\n",
    "    new_total_powerbi['Forecasted'] = new_total_powerbi['Forecasted'].apply(lambda x: int(x) if pd.notna(x) else x)\n",
    "    return new_total_powerbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cef7de1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:10.290834Z",
     "iopub.status.busy": "2024-11-05T11:50:10.290586Z",
     "iopub.status.idle": "2024-11-05T11:50:10.310468Z",
     "shell.execute_reply": "2024-11-05T11:50:10.309935Z"
    }
   },
   "outputs": [],
   "source": [
    "if negative_parts != []:\n",
    "    training_grp = arima_training_data.groupby('Part Number').agg({'Actual':'sum', 'Forecasted':'sum'}).reset_index()\n",
    "\n",
    "    training_grp['Closeness_to_accuracy'] = training_grp.apply(lambda row: \n",
    "                                                                 calculate_mape(row['Actual'], \n",
    "                                                                    row['Forecasted']), axis=1)\n",
    "\n",
    "    training_grp['Avg_residual'] = (training_grp['Actual'] - training_grp['Forecasted'])/3\n",
    "    training_grp = training_grp.assign(Variation = lambda x: np.where(np.logical_and(x['Avg_residual'] > -10, x['Avg_residual'] < 10), 1, 0))\n",
    "    training_grp['Threshold'] = training_grp.apply(lambda x:create_threshold(x['Closeness_to_accuracy'], x['Variation']), axis=1)\n",
    "\n",
    "    arima_training_data_new = arima_training_data.merge(training_grp[['Part Number', 'Threshold']].drop_duplicates(), on='Part Number', how='left')\n",
    "    final_forecasted_arima_negative_data_new = final_forecasted_arima_negative_data.merge(training_grp[['Part Number', 'Threshold']].drop_duplicates(), on='Part Number', how='left')\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c684c64b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:10.312199Z",
     "iopub.status.busy": "2024-11-05T11:50:10.311945Z",
     "iopub.status.idle": "2024-11-05T11:50:10.387463Z",
     "shell.execute_reply": "2024-11-05T11:50:10.386872Z"
    }
   },
   "outputs": [],
   "source": [
    "if negative_parts != []:\n",
    "    final_powerbi_data = create_powerbi_data(arima_training_data_new, final_forecasted_arima_negative_data_new, \n",
    "                                             total_powerbi_data, negative_parts)\n",
    "else:\n",
    "    final_powerbi_data = total_powerbi_data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f230bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:10.389543Z",
     "iopub.status.busy": "2024-11-05T11:50:10.389257Z",
     "iopub.status.idle": "2024-11-05T11:50:10.392655Z",
     "shell.execute_reply": "2024-11-05T11:50:10.392099Z"
    }
   },
   "outputs": [],
   "source": [
    "old_col_names_pos = final_powerbi_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe63f623",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:10.394487Z",
     "iopub.status.busy": "2024-11-05T11:50:10.394241Z",
     "iopub.status.idle": "2024-11-05T11:50:10.600775Z",
     "shell.execute_reply": "2024-11-05T11:50:10.600164Z"
    }
   },
   "outputs": [],
   "source": [
    "full_master_details = full_master_details.rename(columns={'ICC_seg':'ICC',\n",
    "                                                          'Vendor Code':'Supplier Name',\n",
    "                                                          'PIC_ABB':'PIC (Abb)',\n",
    "                                                          'PIC':'Person in Charge',\n",
    "                                                          'PRODUCT_GROUP': 'Product Group',\n",
    "                                                          'IMPORT_CD': 'Source Type'\n",
    "                                                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361877db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:10.603024Z",
     "iopub.status.busy": "2024-11-05T11:50:10.602731Z",
     "iopub.status.idle": "2024-11-05T11:50:10.853996Z",
     "shell.execute_reply": "2024-11-05T11:50:10.853365Z"
    }
   },
   "outputs": [],
   "source": [
    "final_powerbi_data_sub = final_powerbi_data.drop(['ICC', 'Source Type', 'Product Group', 'Supplier Name', 'Person in Charge', 'PIC (Abb)'], axis=1)\n",
    "final_powerbi_data_new = final_powerbi_data_sub.merge(full_master_details, left_on='Part Number', right_on='PART_NO', how='left').drop('PART_NO', axis=1)\n",
    "final_powerbi_data_new = final_powerbi_data_new[old_col_names_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6581b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:10.856206Z",
     "iopub.status.busy": "2024-11-05T11:50:10.855915Z",
     "iopub.status.idle": "2024-11-05T11:50:10.859614Z",
     "shell.execute_reply": "2024-11-05T11:50:10.859042Z"
    }
   },
   "outputs": [],
   "source": [
    "final_forecasted_file_path_prefix = config.get_value('prefix_forecast_file')\n",
    "final_file_path_prefix = config.get_value('final_file_path_prefix')\n",
    "final_file_path_bucket = config.get_value('bucket_new')\n",
    "final_file_name = config.get_value('final_file_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeaa08a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:10.861343Z",
     "iopub.status.busy": "2024-11-05T11:50:10.861076Z",
     "iopub.status.idle": "2024-11-05T11:50:11.273769Z",
     "shell.execute_reply": "2024-11-05T11:50:11.273168Z"
    }
   },
   "outputs": [],
   "source": [
    "final_powerbi_data_new.to_csv(f\"s3://{final_file_path_bucket}/{final_forecasted_file_path_prefix}{final_file_name}{pd.to_datetime(config.get_value('model_forecasting_date')).strftime('%Y-%m-%d')}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d128819f",
   "metadata": {},
   "source": [
    "#### Add historical data to dashboard data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eff958d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:11.276125Z",
     "iopub.status.busy": "2024-11-05T11:50:11.275846Z",
     "iopub.status.idle": "2024-11-05T11:50:11.287148Z",
     "shell.execute_reply": "2024-11-05T11:50:11.286627Z"
    }
   },
   "outputs": [],
   "source": [
    "def attach_historical_data(master_df, df, end_date_validation, end_date_training):\n",
    "    test_months = get_consecutive_months(end_date_training)\n",
    "    master_df = master_df.drop(columns=\"Count_of_order\", axis=1)\n",
    "    master_df = master_df.rename(columns={\"Total_qty_per_month\": \"Actual\"})\n",
    "    master_df[\"period\"] = pd.to_datetime(master_df[\"period\"])\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df_hist = df[df['Date']<=end_date_validation]\n",
    "    df_future = df[df['Date']>end_date_validation]\n",
    "    df_hist[\"Date\"] = pd.to_datetime(df_hist[\"Date\"])\n",
    "    if master_df['part_number'].nunique()>df_hist['Part Number'].nunique():\n",
    "        master_df=master_df[master_df['part_number'].isin(df_hist['Part Number'].unique())]\n",
    "    else:\n",
    "        master_df=master_df[:]\n",
    "    \n",
    "    merged_df = pd.merge(master_df, df_hist, left_on=[\"part_number\", \"period\"], right_on=[\"Part Number\", \"Date\"], how=\"left\")\n",
    "    merged_df = merged_df.sort_values(by=[\"part_number\", \"period\"])\n",
    "    merged_df.drop(columns=[\"Part Number\", \"Date\"], axis=1, inplace=True)\n",
    "    merged_df.drop(columns=[\"Actual_y\"], axis=1, inplace=True)\n",
    "    merged_df.rename(columns={\"Actual_x\": \"Actual\"}, inplace=True)\n",
    "\n",
    "    columns_to_bfill = ['Model Name', 'ICC', 'Product Group', 'Source Type', 'Lag', \n",
    "                        'Supplier Name', 'Person in Charge', 'PIC (Abb)', 'Threshold', \n",
    "                        'Forecast_flag']\n",
    "    merged_df = merged_df.sort_values(['part_number', 'period'])\n",
    "\n",
    "    merged_df['Year'] = merged_df['period'].dt.year\n",
    "\n",
    "    merged_df['Month Name'] = merged_df['period'].dt.month_name()\n",
    "\n",
    "    merged_df['Month'] = merged_df['period'].dt.month\n",
    "\n",
    "    merged_df['sort'] = merged_df['period'].dt.year.astype(str) + merged_df['period'].dt.month.astype(str)\n",
    "\n",
    "    merged_df['months'] = merged_df['period'].dt.strftime('%b').astype(str) + merged_df['period'].dt.strftime('%y').astype(str)\n",
    "\n",
    "    merged_df.rename(columns={\"period\": \"Date\"}, inplace=True)\n",
    "\n",
    "    merged_df.rename(columns={\"part_number\": \"Part Number\"}, inplace=True)\n",
    "\n",
    "    merged_df = merged_df[['Part Number', 'Model Name', 'ICC', 'Date', 'Actual', 'Forecasted', 'Old Model Accuracy',\n",
    "        'New Model Accuracy', 'Product Group', 'Source Type', 'Lag', 'Supplier Name', 'Person in Charge', 'PIC (Abb)',\n",
    "        'Threshold', 'Year', 'Month Name', 'Month', 'sort', 'months','Forecast_flag'\n",
    "    ]]\n",
    "    \n",
    "    merged_df_future = pd.concat([merged_df, df_future], ignore_index=True)\n",
    "    \n",
    "    merged_df_future.loc[merged_df_future['Date'].isin(test_months), 'Forecast_flag'] = 'History_dash'\n",
    "    \n",
    "    merged_df_future['Actual'] = merged_df_future['Actual'].apply(lambda x: int(x) if pd.notna(x) else x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    columns_to_fill = ['Model Name', 'ICC', 'Product Group', 'Source Type', 'Lag', \n",
    "                        'Supplier Name', 'Person in Charge', 'PIC (Abb)', 'Threshold', 'Forecast_flag']\n",
    "    # Vectorized fill operation\n",
    "    merged_df_future[columns_to_fill] = merged_df_future.groupby('Part Number')[columns_to_fill] \\\n",
    "                                           .apply(lambda group: group.bfill().ffill()) \\\n",
    "                                           .reset_index(level=0, drop=True)\n",
    "\n",
    "    merged_df_future = merged_df_future.rename(columns={'Threshold':'Model Selection Criteria'})\n",
    "    return merged_df_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e0b64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:11.289016Z",
     "iopub.status.busy": "2024-11-05T11:50:11.288756Z",
     "iopub.status.idle": "2024-11-05T11:50:11.292712Z",
     "shell.execute_reply": "2024-11-05T11:50:11.292232Z"
    }
   },
   "outputs": [],
   "source": [
    "# final_powerbi_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acce2c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:11.294476Z",
     "iopub.status.busy": "2024-11-05T11:50:11.294234Z",
     "iopub.status.idle": "2024-11-05T11:50:27.934246Z",
     "shell.execute_reply": "2024-11-05T11:50:27.933599Z"
    }
   },
   "outputs": [],
   "source": [
    "final_dashboard_data_withhistory = attach_historical_data(data_old_lag1, final_powerbi_data_new, forecast_end_date,training_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17583fba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:27.936406Z",
     "iopub.status.busy": "2024-11-05T11:50:27.936130Z",
     "iopub.status.idle": "2024-11-05T11:50:27.940434Z",
     "shell.execute_reply": "2024-11-05T11:50:27.939957Z"
    }
   },
   "outputs": [],
   "source": [
    "final_dashboard_data_withhistory.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd0785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:27.942235Z",
     "iopub.status.busy": "2024-11-05T11:50:27.941990Z",
     "iopub.status.idle": "2024-11-05T11:50:28.043467Z",
     "shell.execute_reply": "2024-11-05T11:50:28.042847Z"
    }
   },
   "outputs": [],
   "source": [
    "final_dashboard_data_withhistory = final_dashboard_data_withhistory.sort_values(by=['Part Number', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d228e6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:28.045647Z",
     "iopub.status.busy": "2024-11-05T11:50:28.045370Z",
     "iopub.status.idle": "2024-11-05T11:50:28.108473Z",
     "shell.execute_reply": "2024-11-05T11:50:28.107901Z"
    }
   },
   "outputs": [],
   "source": [
    "# Backfill columns A, B, C where D is 'F' and the values are NaN\n",
    "final_dashboard_data_withhistory.loc[final_dashboard_data_withhistory['Forecast_flag'] == 'Future', ['Supplier Name', 'Person in Charge', 'PIC (Abb)']] = final_dashboard_data_withhistory.loc[final_dashboard_data_withhistory['Forecast_flag'] == 'Future', ['Supplier Name', 'Person in Charge', 'PIC (Abb)']].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211f5f94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:28.110514Z",
     "iopub.status.busy": "2024-11-05T11:50:28.110238Z",
     "iopub.status.idle": "2024-11-05T11:50:28.113323Z",
     "shell.execute_reply": "2024-11-05T11:50:28.112822Z"
    }
   },
   "outputs": [],
   "source": [
    "final_file_with_history_name = config.get_value('final_file_with_history_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aeab06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:28.115072Z",
     "iopub.status.busy": "2024-11-05T11:50:28.114820Z",
     "iopub.status.idle": "2024-11-05T11:50:28.117724Z",
     "shell.execute_reply": "2024-11-05T11:50:28.117241Z"
    }
   },
   "outputs": [],
   "source": [
    "archive_path = config.get_value('final_file_with_history_archive_prefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2124e86c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:28.119444Z",
     "iopub.status.busy": "2024-11-05T11:50:28.119201Z",
     "iopub.status.idle": "2024-11-05T11:50:28.122830Z",
     "shell.execute_reply": "2024-11-05T11:50:28.122355Z"
    }
   },
   "outputs": [],
   "source": [
    "archive_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39385552",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:28.124483Z",
     "iopub.status.busy": "2024-11-05T11:50:28.124250Z",
     "iopub.status.idle": "2024-11-05T11:50:28.127691Z",
     "shell.execute_reply": "2024-11-05T11:50:28.127208Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_last_month_date():\n",
    "    # Get the current date\n",
    "    current_date = datetime.now()\n",
    "\n",
    "    # Subtract one month and set the day to 1\n",
    "    first_date_previous_month = current_date.replace(day=1) - relativedelta(months=1)\n",
    "    first_date_previous_month = first_date_previous_month.replace(day=1)\n",
    "\n",
    "    # Format the date in YYYY-MM-DD\n",
    "    formatted_date = first_date_previous_month.strftime('%Y-%m-%d')\n",
    "\n",
    "    return formatted_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06710323",
   "metadata": {},
   "source": [
    "Data Continuity - Get old data and add new months' updated data\n",
    "\n",
    "for example - \n",
    "- last run has data trained till Jun-2024 and forecasted from Jul-Sep 2024\n",
    "- for new run, saved models will capture data till Jul-2024 and forecast for Aug-Oct 2024 data; \n",
    "- during the process, the report will be having old Jul data with no actuals as per algorithm\n",
    "- the below process updates Jul data with actual value and new master data details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fded656",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:50:28.129601Z",
     "iopub.status.busy": "2024-11-05T11:50:28.129364Z",
     "iopub.status.idle": "2024-11-05T11:50:36.875371Z",
     "shell.execute_reply": "2024-11-05T11:50:36.874773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if dashboard data exists in the folder: \n",
    "# if yes - continue adding new month data to current dashboard data; \n",
    "# if no - its a new run\n",
    "\n",
    "# Specify your S3 path\n",
    "s3_path = f\"s3://{final_file_path_bucket}/{final_file_path_prefix}/{final_file_with_history_name[:-1]}.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "files = fs.glob(s3_path)\n",
    "\n",
    "if files:\n",
    "    # Previous month dashboard data\n",
    "    old_dash_data = read_file(\"s3://\"+files[0])\n",
    "    old_dash_data['Date'] = pd.to_datetime(old_dash_data['Date'], format='%Y-%m-%d')\n",
    "    # get last month date\n",
    "    last_month_date = get_last_month_date()\n",
    "\n",
    "    # Clean data\n",
    "    data_old_lag1['period'] = pd.to_datetime(data_old_lag1['period'], format='%Y-%m-%d')\n",
    "    \n",
    "    # new month data\n",
    "    dash_data_middle_months = old_dash_data[old_dash_data['Date']==last_month_date]\n",
    "    # Update new month data with actual values\n",
    "    dash_data_middle_months_merge = dash_data_middle_months.merge(data_old_lag1.drop('Count_of_order', axis=1), \n",
    "                                                              left_on=['Part Number', \n",
    "                                                                       'Date'], \n",
    "                                                              right_on=['part_number', \n",
    "                                                                        'period'], \n",
    "                                                              how='left').drop(['period', \n",
    "                                                                                'part_number', \n",
    "                                                                                'Actual'], \n",
    "                                                                               axis=1).rename(columns={'Total_qty_per_month':'Actual'})\n",
    "    dash_data_middle_months_merge = dash_data_middle_months_merge[dash_data_middle_months.columns]\n",
    "    dash_data_middle_months_merge['Actual'] = dash_data_middle_months_merge['Actual'].apply(lambda x: int(x) if pd.notna(x) else x)\n",
    "    final_dashboard_data_withhistory = final_dashboard_data_withhistory[final_dashboard_data_withhistory['Date']!=last_month_date].reset_index(drop=True)\n",
    "    # extract master data details from new dashboard data\n",
    "    new_full_data_pn_model = final_dashboard_data_withhistory[['Part Number', 'Model Name', 'Product Group',\n",
    "           'Source Type', 'Lag', 'Supplier Name', 'Person in Charge', 'PIC (Abb)',\n",
    "           'Model Selection Criteria']].drop_duplicates()\n",
    "    # merge master data details to new data\n",
    "    dash_data_middle_months_merge = dash_data_middle_months_merge.drop(['Model Name', 'Product Group',\n",
    "       'Source Type', 'Lag', 'Supplier Name', 'Person in Charge', 'PIC (Abb)',\n",
    "       'Model Selection Criteria'], axis=1).merge(new_full_data_pn_model, on='Part Number')\n",
    "    dash_data_middle_months_merge = dash_data_middle_months_merge[dash_data_middle_months.columns]\n",
    "    dash_data_middle_months_merge['New Model Accuracy'] = dash_data_middle_months_merge.apply(lambda row: \n",
    "                                                                                          calculate_mape(row['Actual'], \n",
    "                                                                                                         row['Forecasted']), axis=1)\n",
    "#     dash_data_middle_months_merge.loc[dash_data_middle_months_merge['Date']==last_month_date, 'Forecast_flag'] = 'History_dash'\n",
    "    # update new months details with master data to new dashboard data\n",
    "    new_full_data_complete = pd.concat([final_dashboard_data_withhistory, \n",
    "                                        dash_data_middle_months_merge], ignore_index=True).sort_values(['Part Number', 'Date'])\n",
    "    \n",
    "    # Update flags\n",
    "    forecasted_bestfit_data['Date'] = pd.to_datetime(forecasted_bestfit_data['Date'])\n",
    "    start_date = pd.to_datetime(forecasted_bestfit_data['Date'].min().strftime(\"%Y-%m-%d\"))\n",
    "    historic_dash_month_list = [(start_date - pd.DateOffset(months=i)).strftime('%Y-%m-%d') for i in range(1, 4)]\n",
    "    # get list of new historic dash months\n",
    "    new_full_data_complete.loc[new_full_data_complete['Date'].isin(historic_dash_month_list), \n",
    "                               'Forecast_flag'] = 'History_dash'\n",
    "    \n",
    "    forecasted_bestfit_data['Date'] = pd.to_datetime(forecasted_bestfit_data['Date']).dt.strftime('%Y-%m-%d')\n",
    "    forecasted_month_list = forecasted_bestfit_data['Date'].unique().tolist()\n",
    "    \n",
    "    months_list = forecasted_month_list+historic_dash_month_list\n",
    "    \n",
    "    # Update rest months to 'History' tagged\n",
    "    new_full_data_complete.loc[~new_full_data_complete['Date'].isin(months_list), 'Forecast_flag'] = 'History'\n",
    "    new_full_data_complete['Forecasted'] = new_full_data_complete['Forecasted'].apply(lambda x: np.where(x<0,max(x,0),x))\n",
    "    \n",
    "    # Changing for Athena table\n",
    "    new_full_data_complete['Forecasted'] = new_full_data_complete['Forecasted'].astype(float)\n",
    "    new_full_data_complete['Actual'] = new_full_data_complete['Actual'].astype(float)\n",
    "    new_full_data_complete['Old Model Accuracy'] = new_full_data_complete['Old Model Accuracy'].astype(float)\n",
    "    new_full_data_complete['New Model Accuracy'] = new_full_data_complete['New Model Accuracy'].astype(float)\n",
    "    # save data to s3 folder\n",
    "    # Below line saves data in archive folder\n",
    "    new_full_data_complete.to_csv(f\"s3://{final_file_path_bucket}/{archive_path}/{final_file_with_history_name}{pd.to_datetime(config.get_value('model_forecasting_date')).strftime('%Y-%m-%d')}.csv\", index=False)\n",
    "    new_full_data_complete.to_csv(f\"s3://{final_file_path_bucket}/{final_file_path_prefix}/{final_file_with_history_name[:-1]}.csv\", index=False)\n",
    "else:\n",
    "    # Changing for Athena table\n",
    "    final_dashboard_data_withhistory['Forecasted'] = final_dashboard_data_withhistory['Forecasted'].astype(float)\n",
    "    final_dashboard_data_withhistory['Actual'] = final_dashboard_data_withhistory['Actual'].astype(float)\n",
    "    final_dashboard_data_withhistory['Old Model Accuracy'] = final_dashboard_data_withhistory['Old Model Accuracy'].astype(float)\n",
    "    final_dashboard_data_withhistory['New Model Accuracy'] = final_dashboard_data_withhistory['New Model Accuracy'].astype(float)\n",
    "    # Below line saves data in archive folder\n",
    "    final_dashboard_data_withhistory.to_csv(f\"s3://{final_file_path_bucket}/{archive_path}/{final_file_with_history_name}{pd.to_datetime(config.get_value('model_forecasting_date')).strftime('%Y-%m-%d')}.csv\", index=False)\n",
    "    final_dashboard_data_withhistory.to_csv(f\"s3://{final_file_path_bucket}/{final_file_path_prefix}/{final_file_with_history_name[:-1]}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73785b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
