{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0930a810",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:00.949945Z",
     "iopub.status.busy": "2024-11-05T11:34:00.949655Z",
     "iopub.status.idle": "2024-11-05T11:34:01.999835Z",
     "shell.execute_reply": "2024-11-05T11:34:01.999118Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (3.1.5)\r\n",
      "Requirement already satisfied: et-xmlfile in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openpyxl) (2.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e791bf-363c-48f3-875b-711fa117a786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:02.002224Z",
     "iopub.status.busy": "2024-11-05T11:34:02.001923Z",
     "iopub.status.idle": "2024-11-05T11:34:07.433775Z",
     "shell.execute_reply": "2024-11-05T11:34:07.433165Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import glob\n",
    "import s3fs\n",
    "import boto3\n",
    "import random\n",
    "from config_module import Config\n",
    "config = Config()\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(2)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# import torch\n",
    "# import tensorflow as tf\n",
    "# import tensorflow.keras as keras\n",
    "# from tensorflow.keras.models import save_model, model_from_json, Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from pickle import dump, load\n",
    "# from keras.models import load_model\n",
    "import warnings\n",
    "# from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "# def reset_random_seeds():\n",
    "#     os.environ['PYTHONHASHSEED'] = str(2)\n",
    "#     tf.random.set_seed(2)\n",
    "#     np.random.seed(2)\n",
    "#     random.seed(2)\n",
    "#     tf.keras.utils.set_random_seed(2024)\n",
    "\n",
    "# # Make some random data\n",
    "# reset_random_seeds()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "# tf.random.set_seed(42)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Import modules\n",
    "import itertools\n",
    "import logging\n",
    "import math\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import ast\n",
    "import seaborn as sn\n",
    "#import xgboost as xgb\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "#from prophet import Prophet\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import (mean_absolute_error, mean_absolute_percentage_error,\n",
    "                             mean_squared_error)\n",
    "from sklearn.model_selection import (GridSearchCV, TimeSeriesSplit,\n",
    "                                     train_test_split)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import statsmodels.api as sm\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f80c148-3929-4987-9d0a-2161f009fa38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:07.436134Z",
     "iopub.status.busy": "2024-11-05T11:34:07.435748Z",
     "iopub.status.idle": "2024-11-05T11:34:07.442482Z",
     "shell.execute_reply": "2024-11-05T11:34:07.441934Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    if file_extension == '.xlsx':\n",
    "        # Read Excel file\n",
    "        data = pd.read_excel(file_path)\n",
    "    elif file_extension == '.csv':\n",
    "        # Read CSV file\n",
    "        data = pd.read_csv(file_path)\n",
    "    elif file_extension == '.txt':\n",
    "        # Read TXT file\n",
    "        data = pd.read_csv(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format: {}\".format(file_extension))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_latest_file(folder_path, file_prefix):\n",
    "    # Construct the search pattern for the CSV files with the given prefix\n",
    "    search_pattern = os.path.join(folder_path, f\"{file_prefix}*.csv\")\n",
    "    \n",
    "    # Get the list of matching files\n",
    "    csv_files = glob.glob(search_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found.\")\n",
    "        return None\n",
    "    \n",
    "    # Extract the datetime part and convert it to a datetime object\n",
    "    csv_files_with_dates = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            # Extract the datetime stamp from the file name\n",
    "            datetime_stamp = file[len(folder_path) + len(file_prefix) + 1:-5]\n",
    "            file_datetime = datetime.strptime(datetime_stamp, \"%y%m%d_%H%M%S\")\n",
    "            csv_files_with_dates.append((file, file_datetime))\n",
    "        except ValueError:\n",
    "            print(f\"Could not parse date from file name: {file}\")\n",
    "    \n",
    "    if not csv_files_with_dates:\n",
    "        print(\"No valid CSV files found.\")\n",
    "        return None\n",
    "    \n",
    "    # Sort the files by datetime in descending order and get the latest one\n",
    "    latest_file = max(csv_files_with_dates, key=lambda x: x[1])[0]\n",
    "    \n",
    "    return latest_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e5c15f5-9bae-4a0b-bec0-efd024fc81e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:07.444215Z",
     "iopub.status.busy": "2024-11-05T11:34:07.443974Z",
     "iopub.status.idle": "2024-11-05T11:34:07.447942Z",
     "shell.execute_reply": "2024-11-05T11:34:07.447446Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_consecutive_months(start_date, n_months=3):\n",
    "    \"\"\"\n",
    "    Generate a list of consecutive month start dates.\n",
    "\n",
    "    Args:\n",
    "    - start_date (str): Starting date in 'YYYY-MM-DD' format.\n",
    "    - n_months (int): Number of consecutive months to generate.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of consecutive month start dates in 'YYYY-MM-DD' format.\n",
    "    \"\"\"\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    month_list = [(start_date + pd.DateOffset(months=i)).strftime('%Y-%m-%d') for i in range(1, n_months + 1)]\n",
    "    return month_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a50361-4e8e-4d4a-9ee5-286f66f71770",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:07.449577Z",
     "iopub.status.busy": "2024-11-05T11:34:07.449340Z",
     "iopub.status.idle": "2024-11-05T11:34:07.452544Z",
     "shell.execute_reply": "2024-11-05T11:34:07.452048Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file_from_name(output_path_url, model_filename_preffix):\n",
    "    model_file_path = get_latest_file(output_path_url, model_filename_preffix)\n",
    "    print(model_file_path)\n",
    "    model_forecast_data = read_file(model_file_path)\n",
    "    return model_forecast_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98df0fcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:07.454199Z",
     "iopub.status.busy": "2024-11-05T11:34:07.453961Z",
     "iopub.status.idle": "2024-11-05T11:34:07.615961Z",
     "shell.execute_reply": "2024-11-05T11:34:07.615371Z"
    }
   },
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem()\n",
    "bucket = config.get_value('bucket')\n",
    "#prefix = config.get_value('prefix')\n",
    "prefix_inp = config.get_value('prefix_inp')\n",
    "inp_path_forecast = config.get_value('inp_path_forecast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1b2b9ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:07.618308Z",
     "iopub.status.busy": "2024-11-05T11:34:07.617874Z",
     "iopub.status.idle": "2024-11-05T11:34:07.621021Z",
     "shell.execute_reply": "2024-11-05T11:34:07.620513Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# part_master_path_1 = f\"s3://s3-dx-tdem-ie-nonprod-persist/persist-bo/persist-bo-demand-forecast/to-be-removed/test_data/TPCAP Part Master New.xlsx\"\n",
    "# data_gt_master_path_1=f\"s3://s3-dx-tdem-ie-nonprod-persist/persist-bo/persist-bo-demand-forecast/to-be-removed/master_data/job3/etljob3input/GT_part_master.csv\"\n",
    "# data_pic_master_path_1=f\"s3://s3-dx-tdem-ie-nonprod-persist/persist-bo/persist-bo-demand-forecast/to-be-removed/master_data/job3/etljob3input/PIC_master.csv\"\n",
    "# data_supp_master_path_1=f\"s3://s3-dx-tdem-ie-nonprod-persist/persist-bo/persist-bo-demand-forecast/to-be-removed/master_data/job3/etljob3input/Supplier_master.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca240270",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:07.622867Z",
     "iopub.status.busy": "2024-11-05T11:34:07.622626Z",
     "iopub.status.idle": "2024-11-05T11:34:07.625941Z",
     "shell.execute_reply": "2024-11-05T11:34:07.625451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persist-bo/persist-bo-demand-forecast/model-forecasted-output/\n"
     ]
    }
   ],
   "source": [
    "print(inp_path_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c129a486",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:07.627703Z",
     "iopub.status.busy": "2024-11-05T11:34:07.627458Z",
     "iopub.status.idle": "2024-11-05T11:34:07.633417Z",
     "shell.execute_reply": "2024-11-05T11:34:07.632925Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_latest_s3_file(bucket_name, path_prefix, file_prefix):\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # List all objects under the given path_prefix\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=path_prefix)\n",
    "    \n",
    "    if 'Contents' not in response:\n",
    "        print(\"No files found.\")\n",
    "        return None\n",
    "    \n",
    "    # Filter objects that start with the specific file path_prefix\n",
    "    filtered_files = [\n",
    "        obj for obj in response['Contents'] \n",
    "        if obj['Key'].startswith(f\"{path_prefix}{file_prefix}\")\n",
    "    ]\n",
    "    \n",
    "    if not filtered_files:\n",
    "        print(\"No files found with the specific path_prefix.\")\n",
    "        return None\n",
    "    \n",
    "    # Extract the date from each file name and store along with the key\n",
    "    files_with_dates = []\n",
    "    for obj in filtered_files:\n",
    "        key = obj['Key']\n",
    "        # Assuming the date is at the end of the filename after the last underscore\n",
    "        try:\n",
    "            # Get the base name without directory path\n",
    "            base_name = os.path.basename(key)\n",
    "            name_without_extension = os.path.splitext(base_name)[0]\n",
    "            date_str = name_without_extension.split('_')[-1]  # Get the date string part\n",
    "            file_date = datetime.strptime(date_str, \"%Y-%m-%d\")  # Parse date\n",
    "            files_with_dates.append((key, file_date))\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing date from file name {key}: {e}\")\n",
    "\n",
    "    if not files_with_dates:\n",
    "        print(\"No valid files with dates found.\")\n",
    "        return None\n",
    "\n",
    "    # Find the file with the latest date\n",
    "    latest_file_key, latest_date = max(files_with_dates, key=lambda x: x[1])\n",
    "    \n",
    "    return latest_file_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52b1c070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:07.635143Z",
     "iopub.status.busy": "2024-11-05T11:34:07.634897Z",
     "iopub.status.idle": "2024-11-05T11:34:07.640733Z",
     "shell.execute_reply": "2024-11-05T11:34:07.640250Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'persist-bo/persist-bo-demand-forecast/model-forecasted-output/20241103/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_path_forecast+f\"{config.get_value('model_forecasting_date')}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95a4a2aa-887f-448e-ae12-c5fdb67fd350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:07.642466Z",
     "iopub.status.busy": "2024-11-05T11:34:07.642227Z",
     "iopub.status.idle": "2024-11-05T11:34:36.390538Z",
     "shell.execute_reply": "2024-11-05T11:34:36.389890Z"
    }
   },
   "outputs": [],
   "source": [
    "prophet_forecast_file_name = config.get_value('prophet_forecast_file_name')\n",
    "prophet_forecast_data_path = get_latest_s3_file(bucket, inp_path_forecast+f\"{config.get_value('model_forecasting_date')}/\", prophet_forecast_file_name)\n",
    "prophet_forecast_data_1 = pd.read_excel('s3://'+f'{bucket}/{prophet_forecast_data_path}')\n",
    "\n",
    "\n",
    "lstm_forecast_file_name = config.get_value('lstm_forecast_file_name')\n",
    "lstm_forecast_data_path = get_latest_s3_file(bucket, inp_path_forecast+f\"{config.get_value('model_forecasting_date')}/\", lstm_forecast_file_name)\n",
    "lstm_forecast_data_1 = pd.read_excel('s3://'+f'{bucket}/{lstm_forecast_data_path}')\n",
    "\n",
    "lasso_forecast_file_name = config.get_value('lasso_forecast_file_name')\n",
    "lasso_forecast_data_path = get_latest_s3_file(bucket, inp_path_forecast+f\"{config.get_value('model_forecasting_date')}/\", lasso_forecast_file_name)\n",
    "lasso_forecast_data_1 = pd.read_excel('s3://'+f'{bucket}/{lasso_forecast_data_path}')\n",
    "\n",
    "ridge_forecast_file_name = config.get_value('ridge_forecast_file_name')\n",
    "ridge_forecast_data_path = get_latest_s3_file(bucket, inp_path_forecast+f\"{config.get_value('model_forecasting_date')}/\", ridge_forecast_file_name)\n",
    "ridge_forecast_data_1 = pd.read_excel('s3://'+f'{bucket}/{ridge_forecast_data_path}')\n",
    "\n",
    "xgb_forecast_file_name = config.get_value('xgb_forecast_file_name')\n",
    "xgb_forecast_data_path = get_latest_s3_file(bucket, inp_path_forecast+f\"{config.get_value('model_forecasting_date')}/\", xgb_forecast_file_name)\n",
    "xgb_forecast_data_1 = pd.read_excel('s3://'+f'{bucket}/{xgb_forecast_data_path}')\n",
    "\n",
    "last_train_date = (pd.to_datetime(prophet_forecast_data_1['period']).max() + relativedelta(months=-3)).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc43b470",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:36.392765Z",
     "iopub.status.busy": "2024-11-05T11:34:36.392487Z",
     "iopub.status.idle": "2024-11-05T11:34:36.396314Z",
     "shell.execute_reply": "2024-11-05T11:34:36.395772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persist-bo/persist-bo-demand-forecast/model-forecasted-output/20241103/prophet_forecast_data_2024-11-03.xlsx\n",
      "persist-bo/persist-bo-demand-forecast/model-forecasted-output/20241103/xgb_forecast_data_2024-11-03.xlsx\n",
      "persist-bo/persist-bo-demand-forecast/model-forecasted-output/20241103/lstm_forecast_data_2024-11-03.xlsx\n",
      "persist-bo/persist-bo-demand-forecast/model-forecasted-output/20241103/ridge_forecast_data_2024-11-03.xlsx\n",
      "persist-bo/persist-bo-demand-forecast/model-forecasted-output/20241103/lasso_forecast_data_2024-11-03.xlsx\n"
     ]
    }
   ],
   "source": [
    "print(prophet_forecast_data_path)\n",
    "print(xgb_forecast_data_path)\n",
    "print(lstm_forecast_data_path)\n",
    "print(ridge_forecast_data_path)\n",
    "print(lasso_forecast_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "169126d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:36.398117Z",
     "iopub.status.busy": "2024-11-05T11:34:36.397845Z",
     "iopub.status.idle": "2024-11-05T11:34:36.417660Z",
     "shell.execute_reply": "2024-11-05T11:34:36.417155Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1101, 3741, 155, 795, 733)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_forecast_data_1['part_number'].nunique(), prophet_forecast_data_1['part_number'].nunique(), lstm_forecast_data_1['part_number'].nunique(), lasso_forecast_data_1['part_number'].nunique(), ridge_forecast_data_1['part_number'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc6767b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:36.419418Z",
     "iopub.status.busy": "2024-11-05T11:34:36.419166Z",
     "iopub.status.idle": "2024-11-05T11:34:36.437555Z",
     "shell.execute_reply": "2024-11-05T11:34:36.437057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1101, 3741, 155, 795, 733)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_forecast_data_1['part_number'].nunique(), prophet_forecast_data_1['part_number'].nunique(), lstm_forecast_data_1['part_number'].nunique(), lasso_forecast_data_1['part_number'].nunique(), ridge_forecast_data_1['part_number'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3a2fe6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:36.439270Z",
     "iopub.status.busy": "2024-11-05T11:34:36.439018Z",
     "iopub.status.idle": "2024-11-05T11:34:36.442627Z",
     "shell.execute_reply": "2024-11-05T11:34:36.442115Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_forecast(df_forecast, colname):\n",
    "    df_forecast[colname] = df_forecast[colname].apply(lambda x:int(round(x)))\n",
    "    df_forecast[colname] = df_forecast[colname].apply(lambda x: np.where(x<0,max(x,0),x))\n",
    "    return df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57cd3c19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:36.444546Z",
     "iopub.status.busy": "2024-11-05T11:34:36.444298Z",
     "iopub.status.idle": "2024-11-05T11:34:36.458260Z",
     "shell.execute_reply": "2024-11-05T11:34:36.457727Z"
    }
   },
   "outputs": [],
   "source": [
    "def forecast_data_merge(prophet_forecast_data,lstm_forecast_data,lasso_forecast_data,ridge_forecast_data,xgb_forecast_data,\n",
    "                        last_training_date, fitted_data):\n",
    "    \n",
    "    forecasted_data_prophet = prophet_forecast_data[prophet_forecast_data['period'].isin(get_consecutive_months(last_training_date))].reset_index(drop=True)\n",
    "    forecasted_data_lstm = lstm_forecast_data[lstm_forecast_data['period'].isin(get_consecutive_months(last_training_date))].reset_index(drop=True)\n",
    "    forecasted_data_lasso = lasso_forecast_data[lasso_forecast_data['period'].isin(get_consecutive_months(last_training_date))].reset_index(drop=True)\n",
    "    forecasted_data_ridge = ridge_forecast_data[ridge_forecast_data['period'].isin(get_consecutive_months(last_training_date))].reset_index(drop=True)\n",
    "    forecasted_data_xgb = xgb_forecast_data[xgb_forecast_data['period'].isin(get_consecutive_months(last_training_date))].reset_index(drop=True)\n",
    "    \n",
    "    forecasted_data_prophet_subset = forecasted_data_prophet.drop(['actual', 'type', 'model_name'], axis=1).rename(columns={'fitted':'forecasted'})\n",
    "    forecasted_data_prophet_subset['model_name']='Prophet'\n",
    "    forecasted_data_lstm_subset = forecasted_data_lstm.drop(['Actual', 'last_history'], axis=1).rename(columns={'Forecast':'forecasted'})\n",
    "    forecasted_data_lstm_subset['model_name'] = 'LSTM single model'\n",
    "    forecasted_data_lasso_subset = forecasted_data_lasso.drop(['Actual_values'], axis=1).rename(columns={'Predicted_values_lag1':'forecasted'})\n",
    "    forecasted_data_lasso_subset['model_name'] = 'Lasso'\n",
    "    forecasted_data_ridge_subset = forecasted_data_ridge.drop(['Actual_values'], axis=1).rename(columns={'Predicted_values_lag1':'forecasted'})\n",
    "    forecasted_data_ridge_subset['model_name'] = 'Ridge'\n",
    "    forecasted_data_xgb_subset = forecasted_data_xgb.drop(['Actual_values'], axis=1).rename(columns={'Predicted_values_lag1':'forecasted'})\n",
    "    forecasted_data_xgb_subset['model_name'] = 'XGBoost'\n",
    "\n",
    "    forecasted_data_prophet_subset = forecasted_data_prophet_subset[forecasted_data_lstm_subset.columns]\n",
    "    forecasted_data_ridge_subset = forecasted_data_ridge_subset[forecasted_data_lstm_subset.columns]\n",
    "    forecasted_data_xgb_subset = forecasted_data_xgb_subset[forecasted_data_lstm_subset.columns]\n",
    "    forecasted_data_lasso_subset = forecasted_data_lasso_subset[forecasted_data_lstm_subset.columns]\n",
    "\n",
    "    # Concating all forecasts\n",
    "    final_forecasted_data = pd.concat([forecasted_data_prophet_subset, \n",
    "                                       forecasted_data_lstm_subset, \n",
    "                                       forecasted_data_ridge_subset, \n",
    "                                       forecasted_data_xgb_subset, \n",
    "                                       forecasted_data_lasso_subset], ignore_index=True)\n",
    "    \n",
    "#     final_forecasted_data = normalize_forecast(final_forecasted_data, 'forecasted')\n",
    "    final_forecasted_data['period'] = pd.to_datetime(final_forecasted_data['period']).astype(str)\n",
    "    pivot_df = final_forecasted_data.pivot_table(index=['part_number', 'model_name'], \n",
    "                                                  columns='period', \n",
    "                                                  values=['forecasted'], \n",
    "                                                  sort=False)\n",
    "    # Flatten the column names\n",
    "    pivot_df.columns = [f'{col[1]}_{col[0]}' for col in pivot_df.columns]\n",
    "    \n",
    "    # Sort the columns\n",
    "    pivot_df = pivot_df.sort_index(axis=1).reset_index()\n",
    "    \n",
    "#     part_master = pd.read_excel(part_master_path)\n",
    "#     part_master['ICC_seg'] = part_master['ICC'].apply(lambda x:x[0])\n",
    "    \n",
    "    # Extract the static columns and the dynamic date columns\n",
    "    static_columns = ['part_number', 'model_name']\n",
    "    date_columns = [col for col in pivot_df.columns if col not in static_columns]\n",
    "\n",
    "    # Function to extract date and type\n",
    "    def extract_date_and_type(col_name):\n",
    "        parts = col_name.split('_')\n",
    "        date = parts[0]\n",
    "        col_type = '_'.join(parts[1:])\n",
    "        return date, col_type\n",
    "\n",
    "    # Sort the date columns based on date and type\n",
    "    sorted_date_columns = sorted(date_columns, key=lambda x: (extract_date_and_type(x)[0], ['forecasted'].index(extract_date_and_type(x)[1])))\n",
    "\n",
    "    # Combine static columns with sorted date columns\n",
    "    sorted_columns = static_columns + sorted_date_columns\n",
    "\n",
    "    # Reorder the DataFrame columns\n",
    "    final_data_compared_bestfit = pivot_df[sorted_columns]\n",
    "    final_data_compared_bestfit['Confidence_interval'] = '#N/A'\n",
    "    final_data_compared_bestfit.loc[final_data_compared_bestfit['model_name']==\"Prophet\", 'Confidence_interval'] = 0.8\n",
    "    \n",
    "    fitted_data_threshold = fitted_data[['Part Number', 'Threshold']].drop_duplicates().reset_index(drop=True)\n",
    "    final_data_compared_bestfit_merged = final_data_compared_bestfit.merge(fitted_data_threshold, left_on='part_number', \n",
    "                                                                             right_on='Part Number', how='left')\n",
    "    df_melted = pd.melt(final_data_compared_bestfit_merged, id_vars=['part_number', 'model_name', 'Confidence_interval','Threshold'],\n",
    "                        value_vars=date_columns, \n",
    "                        var_name='date_measure', \n",
    "                        value_name='value')\n",
    "\n",
    "    # Extract date and measure type\n",
    "    df_melted[['date', 'measure']] = df_melted['date_measure'].str.extract(r'(\\d{4}-\\d{2}-\\d{2})_(\\w+)')\n",
    "    df_melted.drop('date_measure', axis=1, inplace=True)\n",
    "\n",
    "    # Pivot the DataFrame\n",
    "    df_pivoted = df_melted.pivot_table(index=['part_number', 'model_name', 'date'], \n",
    "                                       columns='measure', \n",
    "                                       values='value', \n",
    "                                       aggfunc='first').reset_index()\n",
    "\n",
    "    # Merge with static columns\n",
    "    static_cols = final_data_compared_bestfit_merged.drop(columns=date_columns).drop_duplicates(subset=['part_number', 'model_name'])\n",
    "    result = pd.merge(df_pivoted, static_cols, on=['part_number', 'model_name'])\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    result.columns.name = None\n",
    "    result['actual'] = np.NaN\n",
    "    result['accuracy_pred'] = np.NaN\n",
    "    result = result.rename(columns={\n",
    "        'actual': 'Actual',\n",
    "        'forecasted': 'Forecasted',\n",
    "        'accuracy_pred': 'Accuracy_Pred'\n",
    "    })\n",
    "\n",
    "    result_subset = result[['part_number', 'model_name', 'date', 'Actual', 'Forecasted', \n",
    "                            'Accuracy_Pred', 'Confidence_interval', 'Threshold']]\n",
    "    return result_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "773bf76f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:36.459926Z",
     "iopub.status.busy": "2024-11-05T11:34:36.459687Z",
     "iopub.status.idle": "2024-11-05T11:34:36.462876Z",
     "shell.execute_reply": "2024-11-05T11:34:36.462348Z"
    }
   },
   "outputs": [],
   "source": [
    "# prefix_bestfit_file+\"/\"\n",
    "# bestfit_powerbi_path\n",
    "# print('s3://'+f'{bucket_new}/{bestfit_powerbi_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf51c0fb-05c6-4513-8c96-69ff04d59f37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:36.464587Z",
     "iopub.status.busy": "2024-11-05T11:34:36.464349Z",
     "iopub.status.idle": "2024-11-05T11:34:43.178749Z",
     "shell.execute_reply": "2024-11-05T11:34:43.178122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "curated-bo/curated-bo-demand-forecast/bestfit-selected-models/bestfit_selected_models_historic_months_powerbi_2024-11-03.xlsx\n"
     ]
    }
   ],
   "source": [
    "bucket_new = config.get_value('bucket_new')\n",
    "best_file_base_name_powerbi = config.get_value('best_file_base_name_powerbi')\n",
    "prefix_bestfit_file = config.get_value('prefix_bestfit_file')\n",
    "# prefix_bestfit_file = prefix_bestfit_file+'/'\n",
    "bestfit_powerbi_path = get_latest_s3_file(bucket_new, prefix_bestfit_file+\"/\", best_file_base_name_powerbi)\n",
    "print(1)\n",
    "print(bestfit_powerbi_path)\n",
    "fitted_data_1 = pd.read_excel('s3://'+f'{bucket_new}/{bestfit_powerbi_path}')\n",
    "\n",
    "prefix_forecast_file = config.get_value('prefix_forecast_file')\n",
    "file_name_bestfit_forecast = f'{config.get_value(\"file_name_bestfit_forecast\")}{datetime.now().strftime(\"%Y-%m-%d\")}.xlsx'\n",
    "file_path_bestfit_forecast = f's3://{bucket_new}/{prefix_forecast_file}{file_name_bestfit_forecast}'\n",
    "\n",
    "final_data_compared_bestfit_1 = forecast_data_merge(prophet_forecast_data_1,lstm_forecast_data_1,lasso_forecast_data_1,\n",
    "                                ridge_forecast_data_1,xgb_forecast_data_1,last_train_date,fitted_data_1)\n",
    "final_data_compared_bestfit_1.to_excel(file_path_bestfit_forecast, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "212ed4df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:43.180977Z",
     "iopub.status.busy": "2024-11-05T11:34:43.180697Z",
     "iopub.status.idle": "2024-11-05T11:34:43.184767Z",
     "shell.execute_reply": "2024-11-05T11:34:43.184259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://s3-dx-tdem-ie-nonprod-curated/curated-bo/curated-bo-demand-forecast/selected-models-forecasted-output/forecast_selected_models_future_months_2024-11-05.xlsx'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_bestfit_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4b995a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:43.186566Z",
     "iopub.status.busy": "2024-11-05T11:34:43.186320Z",
     "iopub.status.idle": "2024-11-05T11:34:43.199978Z",
     "shell.execute_reply": "2024-11-05T11:34:43.199477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>part_number</th>\n",
       "      <th>model_name</th>\n",
       "      <th>date</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Forecasted</th>\n",
       "      <th>Accuracy_Pred</th>\n",
       "      <th>Confidence_interval</th>\n",
       "      <th>Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04000081B0</td>\n",
       "      <td>Prophet</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.010885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04000081B0</td>\n",
       "      <td>Prophet</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.912933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04000081B0</td>\n",
       "      <td>Prophet</td>\n",
       "      <td>2024-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.558024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0411106101</td>\n",
       "      <td>Prophet</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.247474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0411106101</td>\n",
       "      <td>Prophet</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.626274</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19570</th>\n",
       "      <td>PZ08100005</td>\n",
       "      <td>Ridge</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158.944408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#N/A</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19571</th>\n",
       "      <td>PZ08100005</td>\n",
       "      <td>Ridge</td>\n",
       "      <td>2024-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160.446001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#N/A</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19572</th>\n",
       "      <td>PZT0038023</td>\n",
       "      <td>Prophet</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>324.423829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19573</th>\n",
       "      <td>PZT0038023</td>\n",
       "      <td>Prophet</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>310.349778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19574</th>\n",
       "      <td>PZT0038023</td>\n",
       "      <td>Prophet</td>\n",
       "      <td>2024-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>355.514223</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19575 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      part_number model_name        date  Actual  Forecasted  Accuracy_Pred  \\\n",
       "0      04000081B0    Prophet  2024-07-01     NaN    5.010885            NaN   \n",
       "1      04000081B0    Prophet  2024-08-01     NaN    7.912933            NaN   \n",
       "2      04000081B0    Prophet  2024-09-01     NaN    8.558024            NaN   \n",
       "3      0411106101    Prophet  2024-07-01     NaN   16.247474            NaN   \n",
       "4      0411106101    Prophet  2024-08-01     NaN    9.626274            NaN   \n",
       "...           ...        ...         ...     ...         ...            ...   \n",
       "19570  PZ08100005      Ridge  2024-08-01     NaN  158.944408            NaN   \n",
       "19571  PZ08100005      Ridge  2024-09-01     NaN  160.446001            NaN   \n",
       "19572  PZT0038023    Prophet  2024-07-01     NaN  324.423829            NaN   \n",
       "19573  PZT0038023    Prophet  2024-08-01     NaN  310.349778            NaN   \n",
       "19574  PZT0038023    Prophet  2024-09-01     NaN  355.514223            NaN   \n",
       "\n",
       "      Confidence_interval  Threshold  \n",
       "0                     0.8       True  \n",
       "1                     0.8       True  \n",
       "2                     0.8       True  \n",
       "3                     0.8       True  \n",
       "4                     0.8       True  \n",
       "...                   ...        ...  \n",
       "19570                #N/A       True  \n",
       "19571                #N/A       True  \n",
       "19572                 0.8       True  \n",
       "19573                 0.8       True  \n",
       "19574                 0.8       True  \n",
       "\n",
       "[19575 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_compared_bestfit_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae4ad2",
   "metadata": {},
   "source": [
    "#### power BI process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49d01d5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:43.201678Z",
     "iopub.status.busy": "2024-11-05T11:34:43.201431Z",
     "iopub.status.idle": "2024-11-05T11:34:43.204790Z",
     "shell.execute_reply": "2024-11-05T11:34:43.204275Z"
    }
   },
   "outputs": [],
   "source": [
    "bucket_curated = config.get_value('bucket_new')\n",
    "prefix_gtmaster = config.get_value('prefix_gtmaster')\n",
    "prefix_proc = config.get_value('prefix_proc')\n",
    "prefix_pic_master_path = config.get_value(\"prefix_pic_master_path\")\n",
    "product_master_path_prefix = config.get_value('product_master_path_prefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4f1f626",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:43.206668Z",
     "iopub.status.busy": "2024-11-05T11:34:43.206422Z",
     "iopub.status.idle": "2024-11-05T11:34:43.217142Z",
     "shell.execute_reply": "2024-11-05T11:34:43.216641Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to read a single Parquet file\n",
    "def read_parquet_file(file):\n",
    "    return pd.read_parquet(f's3://{file}', engine='pyarrow')\n",
    "\n",
    "def gt_parquet_read(bucket_curated_name, gt_master_parquet):\n",
    "    # Recursively list all Parquet files in the nested S3 path\n",
    "#     parquet_files_gt_partm = fs.glob(f's3://{bucket_curated_name}/{gt_master_parquet}/*/*/*/*.parquet')\n",
    "    parquet_files_gt_partm = fs.glob(f's3://{bucket_curated_name}/{gt_master_parquet}*.parquet')\n",
    "    df_master_gt_part_master = pd.DataFrame()\n",
    "    for file in parquet_files_gt_partm:\n",
    "        df_gt = pd.read_parquet(f's3://{file}', engine='pyarrow')\n",
    "        df_master_gt_part_master = pd.concat([df_master_gt_part_master,df_gt],ignore_index=True)\n",
    "    df_master_gt_part_master_subset = df_master_gt_part_master.sort_values('sales_start_dt', ascending=False)\n",
    "    df_master_gt_part_master_subset = df_master_gt_part_master_subset.drop_duplicates(subset=['part_no'], keep='first')\n",
    "    df_master_gt_part_master_subset = df_master_gt_part_master_subset[[\"part_no\", \"country_origin_cd\"]].drop_duplicates()\n",
    "    df_master_gt_part_master_subset = df_master_gt_part_master_subset.assign(IMPORT_CD = \n",
    "                                                                             lambda x: np.where(x['country_origin_cd']=='TH', \n",
    "                                                                                                'LSP', \n",
    "                                                                                                np.where(x['country_origin_cd']=='JP', \n",
    "                                                                                                         'JSP', 'MSP')))\n",
    "    df_master_gt_part_master_subset = df_master_gt_part_master_subset.drop('country_origin_cd', axis=1)\n",
    "    df_master_gt_part_master_subset.columns = [col.upper() for col in df_master_gt_part_master_subset.columns]\n",
    "    return df_master_gt_part_master_subset\n",
    "\n",
    "def proc_parquet_read(bucket_curated_name, proc_master_parquet):\n",
    "    # Recursively list all Parquet files in the nested S3 path\n",
    "#     parquet_files_proc = fs.glob(f's3://{bucket_curated_name}/{proc_master_parquet}/*/*/*/*.parquet')\n",
    "    parquet_files_proc = fs.glob(f's3://{bucket_curated_name}/{proc_master_parquet}*.parquet')\n",
    "\n",
    "    df_master_proc_master = pd.DataFrame()\n",
    "\n",
    "    for file in parquet_files_proc:\n",
    "    #     print(file)\n",
    "        df_proc = pd.read_parquet(f's3://{file}', engine='pyarrow')\n",
    "        df_master_proc_master = pd.concat([df_master_proc_master,df_proc],ignore_index=True)\n",
    "    df_master_proc_master = df_master_proc_master.sort_values(['start_dt'], ascending=[False])\n",
    "    df_master_proc_master = df_master_proc_master.drop_duplicates(subset='part_no', keep='first')\n",
    "    df_master_proc_master_subset = df_master_proc_master[['part_no', 'procurement_div_cd', 'procurement_operator_id', 'inventory_ctrl_class']].drop_duplicates()\n",
    "    df_master_proc_master_subset['procurement_div_cd'] = df_master_proc_master_subset['procurement_div_cd'].apply(lambda x:x[-3:])\n",
    "    df_master_proc_master_subset = df_master_proc_master_subset.rename(columns={'procurement_div_cd':'Vendor Code',\n",
    "                                                                                'part_no':'PART_NO',\n",
    "                                                                                'procurement_operator_id':'PROCUREMENT_OPERATOR_ID',\n",
    "                                                                                'inventory_ctrl_class': 'ICC_seg'\n",
    "                                                                       })\n",
    "    df_master_proc_master_subset['ICC_seg'] = df_master_proc_master_subset['ICC_seg'].apply(lambda x: x[0] if pd.notna(x) and len(x) > 1 else x)\n",
    "    return df_master_proc_master_subset\n",
    "\n",
    "def pic_parquet_read(bucket_curated_name, pic_master_parquet):\n",
    "    # Recursively list all Parquet files in the nested S3 path\n",
    "#     parquet_files_pic_master = fs.glob(f's3://{bucket_curated_name}/{pic_master_parquet}/*/*/*/*.parquet')\n",
    "    parquet_files_pic_master = fs.glob(f's3://{bucket_curated_name}/{pic_master_parquet}*.parquet')\n",
    "    \n",
    "\n",
    "    df_master_pic = pd.DataFrame()\n",
    "\n",
    "    for file in parquet_files_pic_master:\n",
    "    #     print(file)\n",
    "        df_pic = pd.read_parquet(f's3://{file}', engine='pyarrow')\n",
    "        df_master_pic = pd.concat([df_master_pic,df_pic],ignore_index=True)\n",
    "    \n",
    "    df_master_pic_subset = df_master_pic[['id', 'pic', 'pic_abb']].drop_duplicates().reset_index(drop=True)\n",
    "    df_master_pic_subset.columns = [col.upper() for col in df_master_pic_subset.columns]\n",
    "\n",
    "    return df_master_pic_subset\n",
    "\n",
    "def prod_master_read(bucket_curated_name, pg_master_path):\n",
    "    pg_master_filepath = \"s3://\"+fs.glob(f's3://{bucket_curated_name}/{pg_master_path}/*.csv')[0]\n",
    "    pg_master_df = read_file(pg_master_filepath)\n",
    "    pg_master_df = pg_master_df[['PART_NO', 'PRODUCT_GROUP']].drop_duplicates()\n",
    "    return pg_master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d61448e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:43.218892Z",
     "iopub.status.busy": "2024-11-05T11:34:43.218636Z",
     "iopub.status.idle": "2024-11-05T11:34:43.225575Z",
     "shell.execute_reply": "2024-11-05T11:34:43.225067Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_lastest_partition_path(bucket_name, prefix):\n",
    "    if prefix[-1] != \"/\": prefix += \"/\"\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    year_month_day = {}\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "            for obj in page.get('Contents', []):\n",
    "                key = obj['Key']\n",
    "                key = key[len(prefix):]\n",
    "                parts = key.split('/')\n",
    "#                 print(parts)\n",
    "                if len(parts) >= 3:\n",
    "                    if \"=\" in parts[0] and \"=\" in parts[1]:\n",
    "                        year = parts[0].split('=')[-1]\n",
    "                        month = parts[1].split('=')[-1]\n",
    "                        if \"=\" in parts[2]:\n",
    "                            day = parts[2].split('=')[-1]\n",
    "                        else: day = \"\"\n",
    "                            \n",
    "                        if len(day) > 2: continue\n",
    "\n",
    "                        if year not in year_month_day:\n",
    "                            year_month_day[year] = {}\n",
    "                        if month not in year_month_day[year]:\n",
    "                            year_month_day[year][month] = set()\n",
    "                        year_month_day[year][month].add(day)\n",
    "\n",
    "#     print(year_month_day.keys())\n",
    "    \n",
    "    most_recent_year = max(year_month_day.keys(), key=int)\n",
    "    most_recent_month = max(year_month_day[most_recent_year].keys(), key=int)\n",
    "    if year_month_day[most_recent_year][most_recent_month] != {\"\"}:\n",
    "        most_recent_day = max(year_month_day[most_recent_year][most_recent_month], key=int)\n",
    "        final_path = f\"{prefix}year={most_recent_year}/month={most_recent_month}/day={most_recent_day}/\"\n",
    "    else:\n",
    "        final_path = f\"{prefix}year={most_recent_year}/month={most_recent_month}/\"\n",
    "        \n",
    "    return final_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04135324",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:43.227570Z",
     "iopub.status.busy": "2024-11-05T11:34:43.227153Z",
     "iopub.status.idle": "2024-11-05T11:34:43.232781Z",
     "shell.execute_reply": "2024-11-05T11:34:43.232267Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_final_master_file(bucket_cur, gt_parquet_path, proc_parquet_path, pic_parquet_path, prod_path):\n",
    "    prefix_gtmaster = get_lastest_partition_path(bucket_cur, gt_parquet_path)\n",
    "    gt_data = gt_parquet_read(bucket_cur, prefix_gtmaster)\n",
    "    prefix_procmaster = get_lastest_partition_path(bucket_cur, proc_parquet_path)\n",
    "    proc_data = proc_parquet_read(bucket_cur, prefix_procmaster)\n",
    "    prefix_picmaster = get_lastest_partition_path(bucket_cur, pic_parquet_path)\n",
    "    pic_data = pic_parquet_read(bucket_cur, prefix_picmaster)\n",
    "    \n",
    "    pg_data = prod_master_read(bucket_cur, prod_path)\n",
    "    \n",
    "    pic_data['ID'] = pic_data['ID'].astype(str)\n",
    "    gt_data['PART_NO'] = gt_data['PART_NO'].astype(str)\n",
    "    proc_data['PART_NO'] = proc_data['PART_NO'].astype(str)\n",
    "    pg_data['PART_NO'] = pg_data['PART_NO'].astype(str)\n",
    "    \n",
    "    final_master_file_data = pd.merge(pd.merge(pd.merge(gt_data, proc_data, on='PART_NO', how='left'), \n",
    "                                 pic_data, left_on='PROCUREMENT_OPERATOR_ID', \n",
    "                                 right_on='ID', how='left').drop('ID', axis=1), \n",
    "                        pg_data, on='PART_NO', how='left')\n",
    "    \n",
    "    return final_master_file_data\n",
    "\n",
    "# def create_final_master_file(bucket_cur, gt_parquet_path, proc_parquet_path, pic_parquet_path, prod_path):\n",
    "#     gt_data = gt_parquet_read(bucket_cur, gt_parquet_path)\n",
    "#     proc_data = proc_parquet_read(bucket_cur, proc_parquet_path)\n",
    "#     pic_data = pic_parquet_read(bucket_cur, pic_parquet_path)\n",
    "    \n",
    "#     pg_data = prod_master_read(bucket_cur, prod_path)\n",
    "    \n",
    "#     pic_data['ID'] = pic_data['ID'].astype(str)\n",
    "#     gt_data['PART_NO'] = gt_data['PART_NO'].astype(str)\n",
    "#     proc_data['PART_NO'] = proc_data['PART_NO'].astype(str)\n",
    "#     pg_data['PART_NO'] = pg_data['PART_NO'].astype(str)\n",
    "    \n",
    "#     final_master_file_data = pd.merge(pd.merge(pd.merge(gt_data, proc_data, on='PART_NO', how='left'), \n",
    "#                                  pic_data, left_on='PROCUREMENT_OPERATOR_ID', \n",
    "#                                  right_on='ID', how='left').drop('ID', axis=1), \n",
    "#                         pg_data, on='PART_NO', how='left')\n",
    "    \n",
    "#     return final_master_file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15d24034-7fb9-4254-a56d-dcf06284b886",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:34:43.234455Z",
     "iopub.status.busy": "2024-11-05T11:34:43.234217Z",
     "iopub.status.idle": "2024-11-05T11:35:00.909221Z",
     "shell.execute_reply": "2024-11-05T11:35:00.908577Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_master_details = create_final_master_file(bucket_curated, prefix_gtmaster, prefix_proc, prefix_pic_master_path, product_master_path_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9582d49c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:00.911539Z",
     "iopub.status.busy": "2024-11-05T11:35:00.911243Z",
     "iopub.status.idle": "2024-11-05T11:35:01.215261Z",
     "shell.execute_reply": "2024-11-05T11:35:01.214691Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((631326, 8), 631326)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_master_details.shape, full_master_details['PART_NO'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27e0f156",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:01.217307Z",
     "iopub.status.busy": "2024-11-05T11:35:01.217014Z",
     "iopub.status.idle": "2024-11-05T11:35:01.225569Z",
     "shell.execute_reply": "2024-11-05T11:35:01.225040Z"
    }
   },
   "outputs": [],
   "source": [
    "def dashboard_builder(old_forecasting_data_bestfit, final_master_data):\n",
    "    old_forecasting_data_bestfit_set = old_forecasting_data_bestfit[['part_number', 'model_name','Actual',\n",
    "               'Forecasted','Accuracy_Pred', 'Confidence_interval', 'Threshold','date']]\n",
    "    old_forecasting_data_bestfit_set = old_forecasting_data_bestfit_set.rename(columns = {'part_number': 'Part Number', 'model_name': 'Model Name', \n",
    "                                                    'Accuracy_Pred': 'New Model Accuracy', \n",
    "                                                    'date': 'Date'\n",
    "                                                   })\n",
    "    old_forecasting_data_bestfit_set['Lag'] = 'Lag1'\n",
    "    old_forecasting_data_bestfit_set['Part Number'] = old_forecasting_data_bestfit_set['Part Number'].astype(str)\n",
    "    \n",
    "    old_forecasting_data_bestfit_set_master = old_forecasting_data_bestfit_set.merge(final_master_data, left_on='Part Number', \n",
    "                                                             right_on='PART_NO', \n",
    "                                                             how='left').drop(['PART_NO', 'PROCUREMENT_OPERATOR_ID'], \n",
    "                                                                              axis=1).rename(columns={'ICC_seg':'ICC',\n",
    "                                                                                                      'Vendor Code':'SUPPLIER_NAME',\n",
    "                                                                                                      'PIC_ABB':'PIC (Abb)'\n",
    "                                                                                                     })\n",
    "    \n",
    "    all_data_sorted_forecasted = old_forecasting_data_bestfit_set_master[['Part Number', 'Model Name', 'ICC','Date', \n",
    "                                                                          'Actual', 'Forecasted','New Model Accuracy', \n",
    "                                                                          'PRODUCT_GROUP', 'IMPORT_CD', 'Lag', 'SUPPLIER_NAME', \n",
    "                                                                          'PIC', 'PIC (Abb)', 'Threshold']]\n",
    "    \n",
    "    all_data_sorted_forecasted['Year'] = pd.to_datetime(all_data_sorted_forecasted['Date']).dt.year\n",
    "    all_data_sorted_forecasted['Month Name'] = pd.to_datetime(all_data_sorted_forecasted['Date']).dt.month_name()\n",
    "    all_data_sorted_forecasted['Month'] = pd.to_datetime(all_data_sorted_forecasted['Date']).dt.month\n",
    "    all_data_sorted_forecasted['sort'] = pd.to_datetime(all_data_sorted_forecasted['Date']).dt.year.astype(str) + pd.to_datetime(all_data_sorted_forecasted['Date']).dt.month.astype(str)\n",
    "    all_data_sorted_forecasted['months'] = pd.to_datetime(all_data_sorted_forecasted['Date']).dt.strftime('%b').astype(str)+pd.to_datetime(all_data_sorted_forecasted['Date']).dt.strftime('%y').astype(str)\n",
    "    #    rename columns\n",
    "    col_names_list = \"\"\"Part Number\n",
    "    Model Name\n",
    "    ICC\n",
    "    Date\n",
    "    Actual\n",
    "    Forecasted\n",
    "    Old Model Accuracy\n",
    "    New Model Accuracy\n",
    "    Product Group\n",
    "    Source Type\n",
    "    Lag\n",
    "    Supplier Name\n",
    "    Person in Charge\n",
    "    PIC (Abb)\n",
    "    Threshold\n",
    "    Year\n",
    "    Month Name\n",
    "    Month\n",
    "    sort\n",
    "    months\"\"\".split('\\n')\n",
    "    col_names_list = list(map(lambda x:x.strip(), col_names_list))\n",
    "    all_data_sorted_forecasted['Old Model Accuracy']=np.nan\n",
    "    all_data_sorted_forecasted = all_data_sorted_forecasted[['Part Number', 'Model Name', 'ICC', 'Date', 'Actual', 'Forecasted','Old Model Accuracy',\n",
    "           'New Model Accuracy', 'PRODUCT_GROUP', 'IMPORT_CD', 'Lag',\n",
    "           'SUPPLIER_NAME', 'PIC', 'PIC (Abb)', 'Threshold','Year', 'Month Name', 'Month',\n",
    "           'sort', 'months']]\n",
    "    all_data_sorted_forecasted.columns = col_names_list\n",
    "    all_data_sorted_forecasted['Forecast_flag'] = 'Future'\n",
    "    return all_data_sorted_forecasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e71a560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:01.227347Z",
     "iopub.status.busy": "2024-11-05T11:35:01.227095Z",
     "iopub.status.idle": "2024-11-05T11:35:01.656817Z",
     "shell.execute_reply": "2024-11-05T11:35:01.656208Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_final_data=dashboard_builder(final_data_compared_bestfit_1,full_master_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "617f6e0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:01.659094Z",
     "iopub.status.busy": "2024-11-05T11:35:01.658804Z",
     "iopub.status.idle": "2024-11-05T11:35:10.502713Z",
     "shell.execute_reply": "2024-11-05T11:35:10.502106Z"
    }
   },
   "outputs": [],
   "source": [
    "file_date_forecasting = pd.to_datetime(config.get_value('model_forecasting_date')).strftime(\"%Y-%m-%d\")\n",
    "filename_forecast_powerbi = f'{config.get_value(\"filename_forecast_powerbi\")}{file_date_forecasting}.xlsx'\n",
    "file_path_forecast_powerbi = f's3://{bucket_new}/{prefix_forecast_file}{filename_forecast_powerbi}'\n",
    "sorted_final_data.to_excel(file_path_forecast_powerbi, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71a3cee1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:10.504840Z",
     "iopub.status.busy": "2024-11-05T11:35:10.504555Z",
     "iopub.status.idle": "2024-11-05T11:35:10.509020Z",
     "shell.execute_reply": "2024-11-05T11:35:10.508505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Part Number', 'Model Name', 'ICC', 'Date', 'Actual', 'Forecasted',\n",
       "       'Old Model Accuracy', 'New Model Accuracy', 'Product Group',\n",
       "       'Source Type', 'Lag', 'Supplier Name', 'Person in Charge', 'PIC (Abb)',\n",
       "       'Threshold', 'Year', 'Month Name', 'Month', 'sort', 'months',\n",
       "       'Forecast_flag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_final_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbb2052c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T11:35:10.510684Z",
     "iopub.status.busy": "2024-11-05T11:35:10.510444Z",
     "iopub.status.idle": "2024-11-05T11:35:10.514220Z",
     "shell.execute_reply": "2024-11-05T11:35:10.513673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://s3-dx-tdem-ie-nonprod-curated/curated-bo/curated-bo-demand-forecast/selected-models-forecasted-output/forecast_selected_models_future_months_powerbi_2024-11-03.xlsx'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_forecast_powerbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4132dec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
